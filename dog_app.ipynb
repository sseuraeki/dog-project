{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "\n",
    "## Project: Write an Algorithm for a Dog Identification App \n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with **'(IMPLEMENTATION)'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section, and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! \n",
    "\n",
    "> **Note**: Once you have completed all of the code implementations, you need to finalize your work by exporting the iPython Notebook as an HTML document. Before exporting the notebook to html, all of the code cells need to have been run so that reviewers can see the final implementation and output. You can then export the notebook by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut.  Markdown cells can be edited by double-clicking the cell to enter edit mode.\n",
    "\n",
    "The rubric contains _optional_ \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. If you decide to pursue the \"Stand Out Suggestions\", you should include the code in this IPython notebook.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Why We're Here \n",
    "\n",
    "In this notebook, you will make the first steps towards developing an algorithm that could be used as part of a mobile or web app.  At the end of this project, your code will accept any user-supplied image as input.  If a dog is detected in the image, it will provide an estimate of the dog's breed.  If a human is detected, it will provide an estimate of the dog breed that is most resembling.  The image below displays potential sample output of your finished project (... but we expect that each student's algorithm will behave differently!). \n",
    "\n",
    "![Sample Dog Output](images/sample_dog_output.png)\n",
    "\n",
    "In this real-world setting, you will need to piece together a series of models to perform different tasks; for instance, the algorithm that detects humans in an image will be different from the CNN that infers dog breed.  There are many points of possible failure, and no perfect algorithm exists.  Your imperfect solution will nonetheless create a fun user experience!\n",
    "\n",
    "### The Road Ahead\n",
    "\n",
    "We break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.\n",
    "\n",
    "* [Step 0](#step0): Import Datasets\n",
    "* [Step 1](#step1): Detect Humans\n",
    "* [Step 2](#step2): Detect Dogs\n",
    "* [Step 3](#step3): Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "* [Step 4](#step4): Use a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 5](#step5): Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 6](#step6): Write your Algorithm\n",
    "* [Step 7](#step7): Test Your Algorithm\n",
    "\n",
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 0: Import Datasets\n",
    "\n",
    "### Import Dog Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of dog images.  We populate a few variables through the use of the `load_files` function from the scikit-learn library:\n",
    "- `train_files`, `valid_files`, `test_files` - numpy arrays containing file paths to images\n",
    "- `train_targets`, `valid_targets`, `test_targets` - numpy arrays containing onehot-encoded classification labels \n",
    "- `dog_names` - list of string-valued dog breed names for translating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133 total dog categories.\n",
      "There are 8351 total dog images.\n",
      "\n",
      "There are 6680 training dog images.\n",
      "There are 835 validation dog images.\n",
      "There are 836 test dog images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('dogImages/train')\n",
    "valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "test_files, test_targets = load_dataset('dogImages/test')\n",
    "\n",
    "# load list of dog names\n",
    "dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Human Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of human images, where the file paths are stored in the numpy array `human_files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13233 total human images.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(8675309)\n",
    "\n",
    "# load filenames in shuffled human dataset\n",
    "human_files = np.array(glob(\"lfw/*/*\"))\n",
    "random.shuffle(human_files)\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total human images.' % len(human_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step1'></a>\n",
    "## Step 1: Detect Humans\n",
    "\n",
    "We use OpenCV's implementation of [Haar feature-based cascade classifiers](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html) to detect human faces in images.  OpenCV provides many pre-trained face detectors, stored as XML files on [github](https://github.com/opencv/opencv/tree/master/data/haarcascades).  We have downloaded one of these detectors and stored it in the `haarcascades` directory.\n",
    "\n",
    "In the next code cell, we demonstrate how to use this detector to find human faces in a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of faces detected: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQsAAAD8CAYAAABgtYFHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1aaa1a2c3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2                \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline                               \n",
    "\n",
    "# extract pre-trained face detector\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "# load color (BGR) image\n",
    "img = cv2.imread(human_files[3])\n",
    "# convert BGR image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# find faces in image\n",
    "faces = face_cascade.detectMultiScale(gray)\n",
    "\n",
    "# print number of faces detected in the image\n",
    "print('Number of faces detected:', len(faces))\n",
    "\n",
    "# get bounding box for each detected face\n",
    "for (x,y,w,h) in faces:\n",
    "    # add bounding box to color image\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    \n",
    "# convert BGR image to RGB for plotting\n",
    "cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# display the image, along with bounding box\n",
    "plt.imshow(cv_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using any of the face detectors, it is standard procedure to convert the images to grayscale.  The `detectMultiScale` function executes the classifier stored in `face_cascade` and takes the grayscale image as a parameter.  \n",
    "\n",
    "In the above code, `faces` is a numpy array of detected faces, where each row corresponds to a detected face.  Each detected face is a 1D array with four entries that specifies the bounding box of the detected face.  The first two entries in the array (extracted in the above code as `x` and `y`) specify the horizontal and vertical positions of the top left corner of the bounding box.  The last two entries in the array (extracted here as `w` and `h`) specify the width and height of the box.\n",
    "\n",
    "### Write a Human Face Detector\n",
    "\n",
    "We can use this procedure to write a function that returns `True` if a human face is detected in an image and `False` otherwise.  This function, aptly named `face_detector`, takes a string-valued file path to an image as input and appears in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns \"True\" if face is detected in image stored at img_path\n",
    "def face_detector(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Assess the Human Face Detector\n",
    "\n",
    "__Question 1:__ Use the code cell below to test the performance of the `face_detector` function.  \n",
    "- What percentage of the first 100 images in `human_files` have a detected human face?  \n",
    "- What percentage of the first 100 images in `dog_files` have a detected human face? \n",
    "\n",
    "Ideally, we would like 100% of human images with a detected face and 0% of dog images with a detected face.  You will see that our algorithm falls short of this goal, but still gives acceptable performance.  We extract the file paths for the first 100 images from each of the datasets and store them in the numpy arrays `human_files_short` and `dog_files_short`.\n",
    "\n",
    "__Answer:__ 99% of the images in 'human_files' have a detected human face and 11 % of the images in 'dog_files' have a detected human face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human detected in human files:  99 %\n",
      "human detected in dog files:  11 %\n"
     ]
    }
   ],
   "source": [
    "human_files_short = human_files[:100]\n",
    "dog_files_short = train_files[:100]\n",
    "# Do NOT modify the code above this line.\n",
    "\n",
    "## TODO: Test the performance of the face_detector algorithm \n",
    "## on the images in human_files_short and dog_files_short.\n",
    "\n",
    "human_files_detected = 0\n",
    "for file in human_files_short:\n",
    "    if face_detector(file):\n",
    "        human_files_detected += 1\n",
    "\n",
    "dog_files_detected = 0\n",
    "for file in dog_files_short:\n",
    "    if face_detector(file):\n",
    "        dog_files_detected += 1\n",
    "\n",
    "print(\"human detected in human files: \", human_files_detected, \"%\")\n",
    "print(\"human detected in dog files: \", dog_files_detected, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2:__ This algorithmic choice necessitates that we communicate to the user that we accept human images only when they provide a clear view of a face (otherwise, we risk having unneccessarily frustrated users!). In your opinion, is this a reasonable expectation to pose on the user? If not, can you think of a way to detect humans in images that does not necessitate an image with a clearly presented face?\n",
    "\n",
    "__Answer:__ Yes, I think it's a reasonable expectation but it would be better if it scored 100% in detecting humans in human files. If someone provides a human image and the model fails to detect a human, that someone can get very frustrated. In order to solve this, I guess drawing a ROC curve and trying to adjust the model to have as high true positive(detect humans in human images) score as possible seems to be the way.\n",
    "\n",
    "We suggest the face detector from OpenCV as a potential way to detect human images in your algorithm, but you are free to explore other approaches, especially approaches that make use of deep learning :).  Please use the code cell below to design and test your own face detection algorithm.  If you decide to pursue this _optional_ task, report performance on each of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## (Optional) TODO: Report the performance of another  \n",
    "## face detection algorithm on the LFW dataset\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6680, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make dataset consist of 50% humans and 50% dogs\n",
    "\n",
    "dog_size = len(train_files)\n",
    "\n",
    "# randomly choose dog_size humans\n",
    "random_humans = np.random.choice(human_files, size=dog_size, replace=False)\n",
    "\n",
    "# add target data (human = 1)\n",
    "random_humans = np.hstack((random_humans.reshape(-1,1), np.ones(random_humans.shape).reshape(-1,1)))\n",
    "random_humans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6680, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 50% dogs\n",
    "dogs = np.hstack((train_files.reshape(-1,1), np.zeros(train_files.shape).reshape(-1,1)))\n",
    "dogs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13360, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full dataset\n",
    "option_dataset = np.vstack((random_humans, dogs))\n",
    "option_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset size:  10688\n",
      "trainset humans:  5358.0\n",
      "testset size:  2672\n",
      "testset humans:  1322.0\n"
     ]
    }
   ],
   "source": [
    "# shuffle and split into train/testset\n",
    "np.random.shuffle(option_dataset)\n",
    "split_size = int(len(option_dataset) * 0.8)\n",
    "option_train = option_dataset[:split_size]\n",
    "option_test = option_dataset[split_size:]\n",
    "\n",
    "print(\"trainset size: \", len(option_train))\n",
    "print(\"trainset humans: \", sum(option_train[:,1].astype(\"float\")))\n",
    "print(\"testset size: \", len(option_test))\n",
    "print(\"testset humans: \", sum(option_test[:,1].astype(\"float\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 250, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check input shape\n",
    "cv2.imread(option_train[2,0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX shape:  (10688, 64, 64, 3)\n",
      "trainY shape;  (10688, 1)\n"
     ]
    }
   ],
   "source": [
    "# convert and resize imgs\n",
    "\n",
    "option_trainX = cv2.imread(option_train[0,0])\n",
    "option_trainX = cv2.resize(option_trainX, (64,64)).reshape(-1,64,64,3)\n",
    "for image in option_train[1:,0]:\n",
    "    image = cv2.resize(cv2.imread(image), (64,64)).reshape(-1,64,64,3)\n",
    "    option_trainX = np.vstack((option_trainX, image))\n",
    "option_trainY = option_train[:,1].reshape(-1,1)\n",
    "\n",
    "option_testX = cv2.imread(option_test[0,0])\n",
    "option_testX = cv2.resize(option_testX, (64,64)).reshape(-1,64,64,3)\n",
    "for image in option_test[1:,0]:\n",
    "    image = cv2.resize(cv2.imread(image), (64,64)).reshape(-1,64,64,3)\n",
    "    option_testX = np.vstack((option_testX, image))\n",
    "option_testY = option_test[:,1].reshape(-1,1)\n",
    "\n",
    "print(\"trainX shape: \", option_trainX.shape)\n",
    "print(\"trainY shape; \", option_trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ],\n",
       "         ..., \n",
       "         [ 0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ],\n",
       "         ..., \n",
       "         [ 0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ],\n",
       "         ..., \n",
       "         [ 0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.71372549,  0.64705882,  0.6       ],\n",
       "         [ 0.58431373,  0.50980392,  0.46666667],\n",
       "         [ 0.55294118,  0.49019608,  0.44313725],\n",
       "         ..., \n",
       "         [ 0.77254902,  0.85490196,  0.80392157],\n",
       "         [ 0.83529412,  0.89411765,  0.84705882],\n",
       "         [ 0.83529412,  0.85490196,  0.81176471]],\n",
       "\n",
       "        [[ 0.70980392,  0.65490196,  0.60784314],\n",
       "         [ 0.69019608,  0.62352941,  0.58039216],\n",
       "         [ 0.56862745,  0.50196078,  0.45490196],\n",
       "         ..., \n",
       "         [ 0.77254902,  0.82745098,  0.78039216],\n",
       "         [ 0.83137255,  0.87058824,  0.81960784],\n",
       "         [ 0.89411765,  0.90588235,  0.8627451 ]],\n",
       "\n",
       "        [[ 0.68235294,  0.64313725,  0.59215686],\n",
       "         [ 0.76470588,  0.69803922,  0.64705882],\n",
       "         [ 0.67843137,  0.60784314,  0.55686275],\n",
       "         ..., \n",
       "         [ 0.90588235,  0.93333333,  0.89019608],\n",
       "         [ 0.8745098 ,  0.88627451,  0.84313725],\n",
       "         [ 0.95294118,  0.95294118,  0.90980392]]],\n",
       "\n",
       "\n",
       "       [[[ 0.2       ,  0.20392157,  0.2627451 ],\n",
       "         [ 0.19607843,  0.20392157,  0.27843137],\n",
       "         [ 0.20784314,  0.23137255,  0.29411765],\n",
       "         ..., \n",
       "         [ 0.46666667,  0.43921569,  0.64705882],\n",
       "         [ 0.42352941,  0.44313725,  0.62745098],\n",
       "         [ 0.30196078,  0.33333333,  0.50588235]],\n",
       "\n",
       "        [[ 0.25098039,  0.31372549,  0.31372549],\n",
       "         [ 0.19215686,  0.23921569,  0.29019608],\n",
       "         [ 0.23137255,  0.22352941,  0.3254902 ],\n",
       "         ..., \n",
       "         [ 0.41960784,  0.44313725,  0.63921569],\n",
       "         [ 0.4       ,  0.44313725,  0.60392157],\n",
       "         [ 0.28627451,  0.30588235,  0.49803922]],\n",
       "\n",
       "        [[ 0.40784314,  0.4       ,  0.41960784],\n",
       "         [ 0.39607843,  0.41568627,  0.41176471],\n",
       "         [ 0.37254902,  0.38431373,  0.36862745],\n",
       "         ..., \n",
       "         [ 0.38823529,  0.43529412,  0.58039216],\n",
       "         [ 0.39215686,  0.43529412,  0.65882353],\n",
       "         [ 0.37254902,  0.34509804,  0.54117647]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.55686275,  0.62745098,  0.69019608],\n",
       "         [ 0.63921569,  0.66666667,  0.70980392],\n",
       "         [ 0.6       ,  0.66666667,  0.68627451],\n",
       "         ..., \n",
       "         [ 0.68627451,  0.76470588,  0.78823529],\n",
       "         [ 0.68235294,  0.71764706,  0.75686275],\n",
       "         [ 0.71764706,  0.78431373,  0.81960784]],\n",
       "\n",
       "        [[ 0.63921569,  0.68627451,  0.68627451],\n",
       "         [ 0.61960784,  0.6627451 ,  0.70196078],\n",
       "         [ 0.63137255,  0.67843137,  0.72156863],\n",
       "         ..., \n",
       "         [ 0.69019608,  0.73333333,  0.76470588],\n",
       "         [ 0.72941176,  0.78431373,  0.8       ],\n",
       "         [ 0.6745098 ,  0.72156863,  0.77254902]],\n",
       "\n",
       "        [[ 0.64313725,  0.70980392,  0.7254902 ],\n",
       "         [ 0.60784314,  0.6627451 ,  0.69411765],\n",
       "         [ 0.57647059,  0.65098039,  0.69019608],\n",
       "         ..., \n",
       "         [ 0.70980392,  0.79607843,  0.80784314],\n",
       "         [ 0.69411765,  0.74509804,  0.77647059],\n",
       "         [ 0.6627451 ,  0.69411765,  0.71372549]]],\n",
       "\n",
       "\n",
       "       [[[ 0.16470588,  0.1372549 ,  0.05490196],\n",
       "         [ 0.16470588,  0.14901961,  0.05490196],\n",
       "         [ 0.19607843,  0.17647059,  0.06666667],\n",
       "         ..., \n",
       "         [ 0.33333333,  0.41960784,  0.4       ],\n",
       "         [ 0.3254902 ,  0.41176471,  0.39215686],\n",
       "         [ 0.29411765,  0.38039216,  0.36078431]],\n",
       "\n",
       "        [[ 0.16470588,  0.13333333,  0.05490196],\n",
       "         [ 0.18431373,  0.16078431,  0.06666667],\n",
       "         [ 0.21568627,  0.18823529,  0.08627451],\n",
       "         ..., \n",
       "         [ 0.31372549,  0.38431373,  0.36862745],\n",
       "         [ 0.2745098 ,  0.34117647,  0.3254902 ],\n",
       "         [ 0.19215686,  0.25882353,  0.23921569]],\n",
       "\n",
       "        [[ 0.21568627,  0.18431373,  0.07058824],\n",
       "         [ 0.24705882,  0.21568627,  0.09803922],\n",
       "         [ 0.23921569,  0.21176471,  0.08627451],\n",
       "         ..., \n",
       "         [ 0.24313725,  0.28627451,  0.2745098 ],\n",
       "         [ 0.15294118,  0.19607843,  0.18431373],\n",
       "         [ 0.08235294,  0.12156863,  0.10980392]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.42352941,  0.3254902 ,  0.24313725],\n",
       "         [ 0.35294118,  0.28627451,  0.21176471],\n",
       "         [ 0.38431373,  0.31764706,  0.2627451 ],\n",
       "         ..., \n",
       "         [ 0.09411765,  0.07058824,  0.05490196],\n",
       "         [ 0.09411765,  0.04313725,  0.09803922],\n",
       "         [ 0.05098039,  0.24313725,  0.36078431]],\n",
       "\n",
       "        [[ 0.4627451 ,  0.36862745,  0.28235294],\n",
       "         [ 0.58431373,  0.50980392,  0.44313725],\n",
       "         [ 0.31372549,  0.25882353,  0.20784314],\n",
       "         ..., \n",
       "         [ 0.0745098 ,  0.0627451 ,  0.04313725],\n",
       "         [ 0.0745098 ,  0.05098039,  0.10196078],\n",
       "         [ 0.02352941,  0.23921569,  0.34901961]],\n",
       "\n",
       "        [[ 0.36470588,  0.25098039,  0.17254902],\n",
       "         [ 0.38823529,  0.30588235,  0.24313725],\n",
       "         [ 0.21960784,  0.17647059,  0.1254902 ],\n",
       "         ..., \n",
       "         [ 0.07058824,  0.08235294,  0.05490196],\n",
       "         [ 0.04705882,  0.0627451 ,  0.10588235],\n",
       "         [ 0.03921569,  0.30588235,  0.40784314]]],\n",
       "\n",
       "\n",
       "       [[[ 0.98039216,  0.98039216,  0.98039216],\n",
       "         [ 0.99215686,  0.99215686,  0.99215686],\n",
       "         [ 0.99215686,  0.99215686,  0.99215686],\n",
       "         ..., \n",
       "         [ 0.99215686,  0.99215686,  0.99215686],\n",
       "         [ 0.99215686,  0.99215686,  0.99215686],\n",
       "         [ 0.98823529,  0.98823529,  0.98823529]],\n",
       "\n",
       "        [[ 0.99607843,  0.99607843,  0.99607843],\n",
       "         [ 1.        ,  1.        ,  1.        ],\n",
       "         [ 1.        ,  1.        ,  1.        ],\n",
       "         ..., \n",
       "         [ 1.        ,  1.        ,  1.        ],\n",
       "         [ 1.        ,  1.        ,  1.        ],\n",
       "         [ 1.        ,  1.        ,  1.        ]],\n",
       "\n",
       "        [[ 0.99607843,  0.99607843,  0.99607843],\n",
       "         [ 1.        ,  1.        ,  1.        ],\n",
       "         [ 1.        ,  1.        ,  1.        ],\n",
       "         ..., \n",
       "         [ 1.        ,  1.        ,  1.        ],\n",
       "         [ 1.        ,  1.        ,  1.        ],\n",
       "         [ 1.        ,  1.        ,  1.        ]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.81176471,  0.99607843,  0.99215686],\n",
       "         [ 0.75686275,  1.        ,  0.98823529],\n",
       "         [ 0.76078431,  0.98039216,  1.        ],\n",
       "         ..., \n",
       "         [ 0.56470588,  0.7372549 ,  0.85098039],\n",
       "         [ 0.6627451 ,  0.89803922,  0.98039216],\n",
       "         [ 0.7372549 ,  0.97254902,  0.98431373]],\n",
       "\n",
       "        [[ 0.8627451 ,  0.98039216,  0.99607843],\n",
       "         [ 0.96862745,  1.        ,  0.99215686],\n",
       "         [ 0.83921569,  0.99607843,  0.99607843],\n",
       "         ..., \n",
       "         [ 0.75686275,  0.98039216,  1.        ],\n",
       "         [ 0.71372549,  0.96862745,  1.        ],\n",
       "         [ 0.72941176,  0.93333333,  0.97254902]],\n",
       "\n",
       "        [[ 0.88627451,  0.96078431,  0.98039216],\n",
       "         [ 0.87843137,  1.        ,  0.98823529],\n",
       "         [ 0.8627451 ,  0.99215686,  0.98823529],\n",
       "         ..., \n",
       "         [ 0.78823529,  0.97647059,  0.98039216],\n",
       "         [ 0.73333333,  0.97647059,  1.        ],\n",
       "         [ 0.77647059,  0.97647059,  0.96862745]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ],\n",
       "         ..., \n",
       "         [ 0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ],\n",
       "         ..., \n",
       "         [ 0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ],\n",
       "         ..., \n",
       "         [ 0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.02352941,  0.04705882,  0.04705882],\n",
       "         [ 0.02352941,  0.04705882,  0.04313725],\n",
       "         [ 0.01960784,  0.04705882,  0.03529412],\n",
       "         ..., \n",
       "         [ 0.05490196,  0.08235294,  0.05882353],\n",
       "         [ 0.08235294,  0.11372549,  0.0745098 ],\n",
       "         [ 0.10196078,  0.12941176,  0.08235294]],\n",
       "\n",
       "        [[ 0.02352941,  0.04705882,  0.04705882],\n",
       "         [ 0.02745098,  0.04705882,  0.04705882],\n",
       "         [ 0.01960784,  0.04705882,  0.03529412],\n",
       "         ..., \n",
       "         [ 0.05098039,  0.0745098 ,  0.05882353],\n",
       "         [ 0.0745098 ,  0.10196078,  0.0745098 ],\n",
       "         [ 0.08627451,  0.11764706,  0.0745098 ]],\n",
       "\n",
       "        [[ 0.00784314,  0.02352941,  0.02352941],\n",
       "         [ 0.01568627,  0.03529412,  0.03137255],\n",
       "         [ 0.01568627,  0.03529412,  0.02745098],\n",
       "         ..., \n",
       "         [ 0.05490196,  0.0745098 ,  0.07058824],\n",
       "         [ 0.06666667,  0.09019608,  0.0745098 ],\n",
       "         [ 0.08235294,  0.10980392,  0.08235294]]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize data\n",
    "option_trainX = option_trainX / 255.\n",
    "option_testX = option_testX / 255.\n",
    "\n",
    "option_trainX[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 16)        784       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        8224      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        32832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               409700    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 451,641\n",
      "Trainable params: 451,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "option_model = Sequential()\n",
    "option_model.add(Conv2D(filters=16, kernel_size=4, padding='same', activation='relu', \n",
    "                        input_shape=(64, 64, 3)))\n",
    "option_model.add(MaxPooling2D(pool_size=2))\n",
    "option_model.add(Conv2D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
    "option_model.add(MaxPooling2D(pool_size=2))\n",
    "option_model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
    "option_model.add(MaxPooling2D(pool_size=2))\n",
    "option_model.add(Dropout(0.3))\n",
    "option_model.add(Flatten())\n",
    "option_model.add(Dense(100, activation='relu'))\n",
    "option_model.add(Dropout(0.4))\n",
    "option_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "option_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile the model\n",
    "option_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10688 samples, validate on 2672 samples\n",
      "Epoch 1/10\n",
      " - 85s - loss: 0.1614 - acc: 0.9336 - val_loss: 0.0317 - val_acc: 0.9906\n",
      "Epoch 2/10\n",
      " - 85s - loss: 0.0414 - acc: 0.9845 - val_loss: 0.0118 - val_acc: 0.9978\n",
      "Epoch 3/10\n",
      " - 85s - loss: 0.0215 - acc: 0.9923 - val_loss: 0.0090 - val_acc: 0.9970\n",
      "Epoch 4/10\n",
      " - 85s - loss: 0.0193 - acc: 0.9939 - val_loss: 0.0148 - val_acc: 0.9948\n",
      "Epoch 5/10\n",
      " - 84s - loss: 0.0176 - acc: 0.9943 - val_loss: 0.0095 - val_acc: 0.9963\n",
      "Epoch 6/10\n",
      " - 84s - loss: 0.0208 - acc: 0.9917 - val_loss: 0.0445 - val_acc: 0.9813\n",
      "Epoch 7/10\n",
      " - 86s - loss: 0.0080 - acc: 0.9970 - val_loss: 0.0074 - val_acc: 0.9978\n",
      "Epoch 8/10\n",
      " - 87s - loss: 0.0105 - acc: 0.9958 - val_loss: 0.0090 - val_acc: 0.9963\n",
      "Epoch 9/10\n",
      " - 84s - loss: 0.0100 - acc: 0.9969 - val_loss: 0.0105 - val_acc: 0.9963\n",
      "Epoch 10/10\n",
      " - 84s - loss: 0.0140 - acc: 0.9956 - val_loss: 0.0051 - val_acc: 0.9981\n"
     ]
    }
   ],
   "source": [
    "hist = option_model.fit(option_trainX, option_trainY, batch_size=32, epochs=10,\n",
    "          validation_data=(option_testX, option_testY), verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(option) human detected in human files:  100 %\n",
      "(option) human detected in dog files:  0 %\n"
     ]
    }
   ],
   "source": [
    "# option model performance\n",
    "\n",
    "# human_files_short and dog_files_short are already used for training\n",
    "# so select different data for testing\n",
    "option_human_files_short = human_files[-100:]\n",
    "option_dog_files_short = test_files[:100]\n",
    "\n",
    "human_files_detected = 0\n",
    "for file in option_human_files_short:\n",
    "    data = cv2.resize(cv2.imread(file), (64,64)).reshape(-1,64,64,3)\n",
    "    if option_model.predict(data):\n",
    "        human_files_detected += 1\n",
    "\n",
    "dog_files_detected = 0\n",
    "for file in option_dog_files_short:\n",
    "    data = cv2.resize(cv2.imread(file), (64,64)).reshape(-1,64,64,3)\n",
    "    if option_model.predict(data):\n",
    "        dog_files_detected += 1\n",
    "\n",
    "print(\"(option) human detected in human files: \", human_files_detected, \"%\")\n",
    "print(\"(option) human detected in dog files: \", dog_files_detected, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above option model distinguishes humans from dogs and dogs from humans much better than the given default human face detector.\n",
    "\n",
    "But this does not mean the model can distinguish humans from any image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step2'></a>\n",
    "## Step 2: Detect Dogs\n",
    "\n",
    "In this section, we use a pre-trained [ResNet-50](http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006) model to detect dogs in images.  Our first line of code downloads the ResNet-50 model, along with weights that have been trained on [ImageNet](http://www.image-net.org/), a very large, very popular dataset used for image classification and other vision tasks.  ImageNet contains over 10 million URLs, each linking to an image containing an object from one of [1000 categories](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).  Given an image, this pre-trained ResNet-50 model returns a prediction (derived from the available categories in ImageNet) for the object that is contained in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "# define ResNet50 model\n",
    "ResNet50_model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data\n",
    "\n",
    "When using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape\n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n",
    "$$\n",
    "\n",
    "where `nb_samples` corresponds to the total number of images (or samples), and `rows`, `columns`, and `channels` correspond to the number of rows, columns, and channels for each image, respectively.  \n",
    "\n",
    "The `path_to_tensor` function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN.  The function first loads the image and resizes it to a square image that is $224 \\times 224$ pixels.  Next, the image is converted to an array, which is then resized to a 4D tensor.  In this case, since we are working with color images, each image has three channels.  Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n",
    "\n",
    "$$\n",
    "(1, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "The `paths_to_tensor` function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape \n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "Here, `nb_samples` is the number of samples, or number of images, in the supplied array of image paths.  It is best to think of `nb_samples` as the number of 3D tensors (where each 3D tensor corresponds to a different image) in your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions with ResNet-50\n",
    "\n",
    "Getting the 4D tensor ready for ResNet-50, and for any other pre-trained model in Keras, requires some additional processing.  First, the RGB image is converted to BGR by reordering the channels.  All pre-trained models have the additional normalization step that the mean pixel (expressed in RGB as $[103.939, 116.779, 123.68]$ and calculated from all pixels in all images in ImageNet) must be subtracted from every pixel in each image.  This is implemented in the imported function `preprocess_input`.  If you're curious, you can check the code for `preprocess_input` [here](https://github.com/fchollet/keras/blob/master/keras/applications/imagenet_utils.py).\n",
    "\n",
    "Now that we have a way to format our image for supplying to ResNet-50, we are now ready to use the model to extract the predictions.  This is accomplished with the `predict` method, which returns an array whose $i$-th entry is the model's predicted probability that the image belongs to the $i$-th ImageNet category.  This is implemented in the `ResNet50_predict_labels` function below.\n",
    "\n",
    "By taking the argmax of the predicted probability vector, we obtain an integer corresponding to the model's predicted object class, which we can identify with an object category through the use of this [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "def ResNet50_predict_labels(img_path):\n",
    "    # returns prediction vector for image located at img_path\n",
    "    img = preprocess_input(path_to_tensor(img_path))\n",
    "    return np.argmax(ResNet50_model.predict(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Dog Detector\n",
    "\n",
    "While looking at the [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a), you will notice that the categories corresponding to dogs appear in an uninterrupted sequence and correspond to dictionary keys 151-268, inclusive, to include all categories from `'Chihuahua'` to `'Mexican hairless'`.  Thus, in order to check to see if an image is predicted to contain a dog by the pre-trained ResNet-50 model, we need only check if the `ResNet50_predict_labels` function above returns a value between 151 and 268 (inclusive).\n",
    "\n",
    "We use these ideas to complete the `dog_detector` function below, which returns `True` if a dog is detected in an image (and `False` if not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### returns \"True\" if a dog is detected in the image stored at img_path\n",
    "def dog_detector(img_path):\n",
    "    prediction = ResNet50_predict_labels(img_path)\n",
    "    return ((prediction <= 268) & (prediction >= 151)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Assess the Dog Detector\n",
    "\n",
    "__Question 3:__ Use the code cell below to test the performance of your `dog_detector` function.  \n",
    "- What percentage of the images in `human_files_short` have a detected dog?  \n",
    "- What percentage of the images in `dog_files_short` have a detected dog?\n",
    "\n",
    "__Answer:__ 1% of the images in 'human_files_short' have a detected dog and 100% for the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Test the performance of the dog_detector function\n",
    "### on the images in human_files_short and dog_files_short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs detected in human files:  1 %\n",
      "dogs detected in dog files:  100 %\n"
     ]
    }
   ],
   "source": [
    "detected_in_humans = 0\n",
    "for file in human_files_short:\n",
    "    if dog_detector(file):\n",
    "        detected_in_humans += 1\n",
    "\n",
    "detected_in_dogs = 0\n",
    "for file in dog_files_short:\n",
    "    if dog_detector(file):\n",
    "        detected_in_dogs += 1\n",
    "\n",
    "print(\"dogs detected in human files: \", detected_in_humans, \"%\")\n",
    "print(\"dogs detected in dog files: \", detected_in_dogs, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "## Step 3: Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "\n",
    "Now that we have functions for detecting humans and dogs in images, we need a way to predict breed from images.  In this step, you will create a CNN that classifies dog breeds.  You must create your CNN _from scratch_ (so, you can't use transfer learning _yet_!), and you must attain a test accuracy of at least 1%.  In Step 5 of this notebook, you will have the opportunity to use transfer learning to create a CNN that attains greatly improved accuracy.\n",
    "\n",
    "Be careful with adding too many trainable layers!  More parameters means longer training, which means you are more likely to need a GPU to accelerate the training process.  Thankfully, Keras provides a handy estimate of the time that each epoch is likely to take; you can extrapolate this estimate to figure out how long it will take for your algorithm to train. \n",
    "\n",
    "We mention that the task of assigning breed to dogs from images is considered exceptionally challenging.  To see why, consider that *even a human* would have great difficulty in distinguishing between a Brittany and a Welsh Springer Spaniel.  \n",
    "\n",
    "Brittany | Welsh Springer Spaniel\n",
    "- | - \n",
    "<img src=\"images/Brittany_02625.jpg\" width=\"100\"> | <img src=\"images/Welsh_springer_spaniel_08203.jpg\" width=\"200\">\n",
    "\n",
    "It is not difficult to find other dog breed pairs with minimal inter-class variation (for instance, Curly-Coated Retrievers and American Water Spaniels).  \n",
    "\n",
    "Curly-Coated Retriever | American Water Spaniel\n",
    "- | -\n",
    "<img src=\"images/Curly-coated_retriever_03896.jpg\" width=\"200\"> | <img src=\"images/American_water_spaniel_00648.jpg\" width=\"200\">\n",
    "\n",
    "\n",
    "Likewise, recall that labradors come in yellow, chocolate, and black.  Your vision-based algorithm will have to conquer this high intra-class variation to determine how to classify all of these different shades as the same breed.  \n",
    "\n",
    "Yellow Labrador | Chocolate Labrador | Black Labrador\n",
    "- | -\n",
    "<img src=\"images/Labrador_retriever_06457.jpg\" width=\"150\"> | <img src=\"images/Labrador_retriever_06455.jpg\" width=\"240\"> | <img src=\"images/Labrador_retriever_06449.jpg\" width=\"220\">\n",
    "\n",
    "We also mention that random chance presents an exceptionally low bar: setting aside the fact that the classes are slightly imabalanced, a random guess will provide a correct answer roughly 1 in 133 times, which corresponds to an accuracy of less than 1%.  \n",
    "\n",
    "Remember that the practice is far ahead of the theory in deep learning.  Experiment with many different architectures, and trust your intuition.  And, of course, have fun! \n",
    "\n",
    "### Pre-process the Data\n",
    "\n",
    "We rescale the images by dividing every pixel in every image by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 6680/6680 [01:13<00:00, 91.30it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 835/835 [00:09<00:00, 86.64it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 836/836 [00:09<00:00, 89.39it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        model.summary()\n",
    "\n",
    "We have imported some Python modules to get you started, but feel free to import as many modules as you need.  If you end up getting stuck, here's a hint that specifies a model that trains relatively fast on CPU and attains >1% test accuracy in 5 epochs:\n",
    "\n",
    "![Sample CNN](images/sample_cnn.png)\n",
    "           \n",
    "__Question 4:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  If you chose to use the hinted architecture above, describe why you think that CNN architecture should work well for the image classification task.\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to try out the hinted architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hinted model's first layer is a conv2d layer with (None, 223, 223, 16) shape.\n",
    "\n",
    "Since the input shape is train_tensors.shape = (None, 224, 224, 3), the kernel_size should be 2 with 'valid' padding and strides size of 1.\n",
    "\n",
    "ceil((224 - 2 +1) / 1) = 223\n",
    "\n",
    "This layer should scan the image with a kernel(2x2 size) moving by 1 pixel(stride) and extract features with 16 filters.\n",
    "\n",
    "Using many layers might suffer from vanishing gradient problem. To overcome this issue, the activation function is set to 'ReLU'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "hint_model = Sequential()\n",
    "\n",
    "hint_model.add(Conv2D(filters=16, kernel_size=2, strides=1, padding='valid', activation='relu', input_shape=(224, 224, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to reduce overfitting problem, maxpooling layer comes in extracting only the maximum value from each pooling window.\n",
    "\n",
    "To get the shape of (None, 111, 111, 16) as in the hinted model, a maxpooling layer with pool size of 2 and strides of 2 will do.\n",
    "\n",
    "ceil((223 - 2 + 1) / 2) = 111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hint_model.add(MaxPooling2D(pool_size=2, strides=2, padding='valid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the extracted features might not be enough to classify correctly. So the model adds one more conv2d layer, extracting features from pooled features.\n",
    "\n",
    "To get the shape of (None, 110, 110, 32) as in the hinted model, the same conv layer(as the first one) with 32 filters will do.\n",
    "\n",
    "ceil((111 - 2 + 1) / 1) = 110\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hint_model.add(Conv2D(filters=32, kernel_size=2, strides=1, padding='valid', activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, to prevent overfitting, the model adds a maxpooling layer.\n",
    "\n",
    "To get the shape of (None, 55, 55, 32) as in the hinted model, the same maxpooling layer will do.\n",
    "\n",
    "ceil((110 - 2 + 1) / 2) = 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hint_model.add(MaxPooling2D(pool_size=2, strides=2, padding='valid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the classification accuracy, the model adds another conv layer with more filters.\n",
    "\n",
    "To get the shape of (None, 54, 54, 64) as in the hinted model, the same conv layer with 64 filters will do.\n",
    "\n",
    "ceil((55 - 2 + 1) / 1) = 54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hint_model.add(Conv2D(filters=64, kernel_size=2, strides=1, padding='valid', activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, a maxpooling layer to prevent overfitting.\n",
    "\n",
    "To get the shape of (None, 27, 27, 64), the same maxpooling layer will do.\n",
    "\n",
    "ceil((54 - 2 + 1) / 2) = 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hint_model.add(MaxPooling2D(pool_size=2, strides=2, padding='valid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply softmax layer to get the final output, the values so far must be fully-connected into a 2d shape array.\n",
    "\n",
    "But rather than, just flattening the values, the hinted model average-pool the values to prevent overfitting.\n",
    "\n",
    "Pooling averages from each filter will get the shape of (None, 64) as in the hinted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hint_model.add(GlobalAveragePooling2D())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the softmax layer comes in to produce the output.\n",
    "\n",
    "Since there are 133 classes to classify, the softmax layer should have 133 output nodes.\n",
    "\n",
    "Each node produces a signoid value and then all those values are assigned with a softmax value.\n",
    "\n",
    "The value with the largest softmax value will be selected as the prediction. (this is done in the \"Test the Model\" section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hint_model.add(Dense(133, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the model I made has the same architecture as the hinted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 223, 223, 16)      208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 111, 111, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 110, 110, 32)      2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 55, 55, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 54, 54, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 27, 27, 64)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 133)               8645      \n",
      "=================================================================\n",
      "Total params: 19,189\n",
      "Trainable params: 19,189\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hint_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hint_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.\n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4040/6680 [=================>............] - ETA: 8:22 - loss: 4.8691 - acc: 0.0000e+0 - ETA: 6:05 - loss: 4.8592 - acc: 0.0000e+0 - ETA: 5:21 - loss: 4.8602 - acc: 0.0000e+0 - ETA: 4:58 - loss: 4.8763 - acc: 0.0000e+0 - ETA: 4:48 - loss: 4.8767 - acc: 0.0000e+0 - ETA: 4:41 - loss: 4.8915 - acc: 0.0000e+0 - ETA: 4:36 - loss: 4.8947 - acc: 0.0000e+0 - ETA: 4:30 - loss: 4.8986 - acc: 0.0063    - ETA: 4:25 - loss: 4.8968 - acc: 0.005 - ETA: 4:21 - loss: 4.8930 - acc: 0.005 - ETA: 4:17 - loss: 4.8850 - acc: 0.004 - ETA: 4:13 - loss: 4.8866 - acc: 0.004 - ETA: 4:11 - loss: 4.8897 - acc: 0.003 - ETA: 4:10 - loss: 4.8948 - acc: 0.003 - ETA: 4:07 - loss: 4.8939 - acc: 0.003 - ETA: 4:05 - loss: 4.8906 - acc: 0.003 - ETA: 4:03 - loss: 4.8913 - acc: 0.002 - ETA: 4:01 - loss: 4.8921 - acc: 0.005 - ETA: 4:00 - loss: 4.8926 - acc: 0.005 - ETA: 3:58 - loss: 4.8921 - acc: 0.005 - ETA: 3:57 - loss: 4.8904 - acc: 0.004 - ETA: 3:55 - loss: 4.8895 - acc: 0.004 - ETA: 3:54 - loss: 4.8907 - acc: 0.004 - ETA: 3:52 - loss: 4.8894 - acc: 0.004 - ETA: 3:51 - loss: 4.8905 - acc: 0.004 - ETA: 3:50 - loss: 4.8895 - acc: 0.003 - ETA: 3:48 - loss: 4.8874 - acc: 0.003 - ETA: 3:47 - loss: 4.8869 - acc: 0.003 - ETA: 3:46 - loss: 4.8874 - acc: 0.003 - ETA: 3:46 - loss: 4.8880 - acc: 0.003 - ETA: 3:45 - loss: 4.8900 - acc: 0.003 - ETA: 3:44 - loss: 4.8897 - acc: 0.004 - ETA: 3:42 - loss: 4.8900 - acc: 0.004 - ETA: 3:41 - loss: 4.8895 - acc: 0.004 - ETA: 3:40 - loss: 4.8905 - acc: 0.004 - ETA: 3:39 - loss: 4.8905 - acc: 0.004 - ETA: 3:38 - loss: 4.8901 - acc: 0.005 - ETA: 3:38 - loss: 4.8899 - acc: 0.005 - ETA: 3:37 - loss: 4.8903 - acc: 0.005 - ETA: 3:36 - loss: 4.8899 - acc: 0.005 - ETA: 3:35 - loss: 4.8892 - acc: 0.004 - ETA: 3:34 - loss: 4.8886 - acc: 0.006 - ETA: 3:33 - loss: 4.8871 - acc: 0.005 - ETA: 3:32 - loss: 4.8882 - acc: 0.005 - ETA: 3:31 - loss: 4.8877 - acc: 0.005 - ETA: 3:30 - loss: 4.8891 - acc: 0.005 - ETA: 3:29 - loss: 4.8893 - acc: 0.005 - ETA: 3:28 - loss: 4.8892 - acc: 0.005 - ETA: 3:28 - loss: 4.8896 - acc: 0.005 - ETA: 3:27 - loss: 4.8899 - acc: 0.005 - ETA: 3:26 - loss: 4.8907 - acc: 0.004 - ETA: 3:25 - loss: 4.8905 - acc: 0.004 - ETA: 3:25 - loss: 4.8904 - acc: 0.004 - ETA: 3:24 - loss: 4.8905 - acc: 0.005 - ETA: 3:23 - loss: 4.8911 - acc: 0.005 - ETA: 3:23 - loss: 4.8912 - acc: 0.006 - ETA: 3:23 - loss: 4.8908 - acc: 0.006 - ETA: 3:22 - loss: 4.8909 - acc: 0.006 - ETA: 3:22 - loss: 4.8902 - acc: 0.006 - ETA: 3:21 - loss: 4.8906 - acc: 0.006 - ETA: 3:20 - loss: 4.8899 - acc: 0.007 - ETA: 3:19 - loss: 4.8900 - acc: 0.007 - ETA: 3:18 - loss: 4.8906 - acc: 0.007 - ETA: 3:18 - loss: 4.8908 - acc: 0.007 - ETA: 3:17 - loss: 4.8905 - acc: 0.007 - ETA: 3:16 - loss: 4.8901 - acc: 0.007 - ETA: 3:15 - loss: 4.8898 - acc: 0.007 - ETA: 3:14 - loss: 4.8899 - acc: 0.007 - ETA: 3:13 - loss: 4.8895 - acc: 0.007 - ETA: 3:12 - loss: 4.8896 - acc: 0.007 - ETA: 3:12 - loss: 4.8894 - acc: 0.007 - ETA: 3:11 - loss: 4.8888 - acc: 0.006 - ETA: 3:10 - loss: 4.8892 - acc: 0.006 - ETA: 3:09 - loss: 4.8892 - acc: 0.006 - ETA: 3:08 - loss: 4.8899 - acc: 0.007 - ETA: 3:07 - loss: 4.8899 - acc: 0.007 - ETA: 3:06 - loss: 4.8896 - acc: 0.007 - ETA: 3:06 - loss: 4.8899 - acc: 0.007 - ETA: 3:05 - loss: 4.8899 - acc: 0.007 - ETA: 3:04 - loss: 4.8902 - acc: 0.006 - ETA: 3:03 - loss: 4.8904 - acc: 0.006 - ETA: 3:02 - loss: 4.8903 - acc: 0.006 - ETA: 3:01 - loss: 4.8904 - acc: 0.006 - ETA: 3:00 - loss: 4.8903 - acc: 0.006 - ETA: 3:00 - loss: 4.8900 - acc: 0.006 - ETA: 2:59 - loss: 4.8903 - acc: 0.006 - ETA: 2:58 - loss: 4.8902 - acc: 0.006 - ETA: 2:57 - loss: 4.8903 - acc: 0.006 - ETA: 2:57 - loss: 4.8904 - acc: 0.006 - ETA: 2:56 - loss: 4.8903 - acc: 0.006 - ETA: 2:55 - loss: 4.8904 - acc: 0.006 - ETA: 2:54 - loss: 4.8903 - acc: 0.006 - ETA: 2:53 - loss: 4.8900 - acc: 0.005 - ETA: 2:53 - loss: 4.8898 - acc: 0.005 - ETA: 2:52 - loss: 4.8897 - acc: 0.005 - ETA: 2:51 - loss: 4.8899 - acc: 0.005 - ETA: 2:50 - loss: 4.8900 - acc: 0.005 - ETA: 2:50 - loss: 4.8901 - acc: 0.005 - ETA: 2:49 - loss: 4.8900 - acc: 0.005 - ETA: 2:48 - loss: 4.8903 - acc: 0.005 - ETA: 2:47 - loss: 4.8904 - acc: 0.005 - ETA: 2:46 - loss: 4.8903 - acc: 0.005 - ETA: 2:46 - loss: 4.8902 - acc: 0.005 - ETA: 2:45 - loss: 4.8904 - acc: 0.005 - ETA: 2:44 - loss: 4.8902 - acc: 0.005 - ETA: 2:43 - loss: 4.8907 - acc: 0.005 - ETA: 2:42 - loss: 4.8906 - acc: 0.005 - ETA: 2:42 - loss: 4.8905 - acc: 0.005 - ETA: 2:41 - loss: 4.8901 - acc: 0.006 - ETA: 2:40 - loss: 4.8901 - acc: 0.006 - ETA: 2:39 - loss: 4.8903 - acc: 0.006 - ETA: 2:39 - loss: 4.8903 - acc: 0.006 - ETA: 2:38 - loss: 4.8903 - acc: 0.006 - ETA: 2:37 - loss: 4.8904 - acc: 0.006 - ETA: 2:36 - loss: 4.8905 - acc: 0.006 - ETA: 2:35 - loss: 4.8904 - acc: 0.006 - ETA: 2:35 - loss: 4.8906 - acc: 0.006 - ETA: 2:34 - loss: 4.8906 - acc: 0.006 - ETA: 2:33 - loss: 4.8904 - acc: 0.006 - ETA: 2:33 - loss: 4.8902 - acc: 0.006 - ETA: 2:32 - loss: 4.8903 - acc: 0.006 - ETA: 2:31 - loss: 4.8904 - acc: 0.006 - ETA: 2:30 - loss: 4.8903 - acc: 0.006 - ETA: 2:30 - loss: 4.8901 - acc: 0.006 - ETA: 2:29 - loss: 4.8903 - acc: 0.006 - ETA: 2:28 - loss: 4.8903 - acc: 0.006 - ETA: 2:27 - loss: 4.8900 - acc: 0.006 - ETA: 2:27 - loss: 4.8899 - acc: 0.006 - ETA: 2:26 - loss: 4.8899 - acc: 0.006 - ETA: 2:25 - loss: 4.8899 - acc: 0.006 - ETA: 2:24 - loss: 4.8900 - acc: 0.006 - ETA: 2:24 - loss: 4.8900 - acc: 0.006 - ETA: 2:23 - loss: 4.8898 - acc: 0.006 - ETA: 2:22 - loss: 4.8894 - acc: 0.006 - ETA: 2:21 - loss: 4.8895 - acc: 0.006 - ETA: 2:21 - loss: 4.8895 - acc: 0.006 - ETA: 2:20 - loss: 4.8897 - acc: 0.006 - ETA: 2:19 - loss: 4.8898 - acc: 0.006 - ETA: 2:19 - loss: 4.8896 - acc: 0.006 - ETA: 2:18 - loss: 4.8896 - acc: 0.006 - ETA: 2:17 - loss: 4.8897 - acc: 0.006 - ETA: 2:16 - loss: 4.8896 - acc: 0.006 - ETA: 2:16 - loss: 4.8896 - acc: 0.006 - ETA: 2:15 - loss: 4.8895 - acc: 0.006 - ETA: 2:14 - loss: 4.8893 - acc: 0.006 - ETA: 2:13 - loss: 4.8892 - acc: 0.006 - ETA: 2:13 - loss: 4.8892 - acc: 0.006 - ETA: 2:12 - loss: 4.8889 - acc: 0.006 - ETA: 2:11 - loss: 4.8887 - acc: 0.007 - ETA: 2:10 - loss: 4.8887 - acc: 0.007 - ETA: 2:10 - loss: 4.8887 - acc: 0.007 - ETA: 2:09 - loss: 4.8887 - acc: 0.007 - ETA: 2:08 - loss: 4.8885 - acc: 0.007 - ETA: 2:08 - loss: 4.8883 - acc: 0.007 - ETA: 2:07 - loss: 4.8880 - acc: 0.007 - ETA: 2:06 - loss: 4.8879 - acc: 0.007 - ETA: 2:05 - loss: 4.8877 - acc: 0.007 - ETA: 2:05 - loss: 4.8878 - acc: 0.007 - ETA: 2:04 - loss: 4.8877 - acc: 0.007 - ETA: 2:03 - loss: 4.8875 - acc: 0.007 - ETA: 2:02 - loss: 4.8873 - acc: 0.007 - ETA: 2:02 - loss: 4.8874 - acc: 0.007 - ETA: 2:01 - loss: 4.8875 - acc: 0.007 - ETA: 2:00 - loss: 4.8876 - acc: 0.007 - ETA: 2:00 - loss: 4.8874 - acc: 0.007 - ETA: 1:59 - loss: 4.8876 - acc: 0.007 - ETA: 1:58 - loss: 4.8874 - acc: 0.007 - ETA: 1:57 - loss: 4.8870 - acc: 0.007 - ETA: 1:57 - loss: 4.8869 - acc: 0.007 - ETA: 1:56 - loss: 4.8868 - acc: 0.007 - ETA: 1:55 - loss: 4.8868 - acc: 0.007 - ETA: 1:54 - loss: 4.8866 - acc: 0.007 - ETA: 1:54 - loss: 4.8869 - acc: 0.007 - ETA: 1:53 - loss: 4.8871 - acc: 0.007 - ETA: 1:52 - loss: 4.8869 - acc: 0.007 - ETA: 1:52 - loss: 4.8869 - acc: 0.007 - ETA: 1:51 - loss: 4.8865 - acc: 0.007 - ETA: 1:50 - loss: 4.8869 - acc: 0.007 - ETA: 1:49 - loss: 4.8873 - acc: 0.007 - ETA: 1:49 - loss: 4.8873 - acc: 0.006 - ETA: 1:48 - loss: 4.8873 - acc: 0.006 - ETA: 1:47 - loss: 4.8872 - acc: 0.006 - ETA: 1:46 - loss: 4.8873 - acc: 0.006 - ETA: 1:46 - loss: 4.8872 - acc: 0.006 - ETA: 1:45 - loss: 4.8874 - acc: 0.006 - ETA: 1:44 - loss: 4.8873 - acc: 0.006 - ETA: 1:44 - loss: 4.8873 - acc: 0.006 - ETA: 1:43 - loss: 4.8872 - acc: 0.006 - ETA: 1:42 - loss: 4.8876 - acc: 0.006 - ETA: 1:41 - loss: 4.8876 - acc: 0.006 - ETA: 1:41 - loss: 4.8876 - acc: 0.006 - ETA: 1:40 - loss: 4.8877 - acc: 0.006 - ETA: 1:39 - loss: 4.8874 - acc: 0.006 - ETA: 1:39 - loss: 4.8872 - acc: 0.006 - ETA: 1:38 - loss: 4.8874 - acc: 0.006 - ETA: 1:37 - loss: 4.8873 - acc: 0.006 - ETA: 1:36 - loss: 4.8873 - acc: 0.006 - ETA: 1:36 - loss: 4.8870 - acc: 0.006 - ETA: 1:35 - loss: 4.8869 - acc: 0.006 - ETA: 1:34 - loss: 4.8865 - acc: 0.006 - ETA: 1:33 - loss: 4.8864 - acc: 0.006 - ETA: 1:33 - loss: 4.8864 - acc: 0.0067\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 1:32 - loss: 4.8863 - acc: 0.006 - ETA: 1:31 - loss: 4.8865 - acc: 0.006 - ETA: 1:31 - loss: 4.8865 - acc: 0.006 - ETA: 1:30 - loss: 4.8865 - acc: 0.006 - ETA: 1:29 - loss: 4.8865 - acc: 0.007 - ETA: 1:28 - loss: 4.8863 - acc: 0.007 - ETA: 1:28 - loss: 4.8863 - acc: 0.006 - ETA: 1:27 - loss: 4.8862 - acc: 0.006 - ETA: 1:26 - loss: 4.8861 - acc: 0.006 - ETA: 1:26 - loss: 4.8861 - acc: 0.006 - ETA: 1:25 - loss: 4.8863 - acc: 0.006 - ETA: 1:24 - loss: 4.8864 - acc: 0.006 - ETA: 1:23 - loss: 4.8864 - acc: 0.006 - ETA: 1:23 - loss: 4.8865 - acc: 0.006 - ETA: 1:22 - loss: 4.8865 - acc: 0.006 - ETA: 1:21 - loss: 4.8865 - acc: 0.006 - ETA: 1:21 - loss: 4.8866 - acc: 0.006 - ETA: 1:20 - loss: 4.8869 - acc: 0.006 - ETA: 1:19 - loss: 4.8870 - acc: 0.006 - ETA: 1:18 - loss: 4.8870 - acc: 0.006 - ETA: 1:18 - loss: 4.8869 - acc: 0.006 - ETA: 1:17 - loss: 4.8870 - acc: 0.006 - ETA: 1:16 - loss: 4.8869 - acc: 0.006 - ETA: 1:16 - loss: 4.8868 - acc: 0.006 - ETA: 1:15 - loss: 4.8866 - acc: 0.006 - ETA: 1:14 - loss: 4.8867 - acc: 0.006 - ETA: 1:13 - loss: 4.8869 - acc: 0.006 - ETA: 1:13 - loss: 4.8868 - acc: 0.006 - ETA: 1:12 - loss: 4.8867 - acc: 0.006 - ETA: 1:11 - loss: 4.8866 - acc: 0.006 - ETA: 1:11 - loss: 4.8867 - acc: 0.006 - ETA: 1:10 - loss: 4.8865 - acc: 0.006 - ETA: 1:09 - loss: 4.8865 - acc: 0.006 - ETA: 1:08 - loss: 4.8864 - acc: 0.006 - ETA: 1:08 - loss: 4.8864 - acc: 0.006 - ETA: 1:07 - loss: 4.8865 - acc: 0.006 - ETA: 1:06 - loss: 4.8864 - acc: 0.006 - ETA: 1:06 - loss: 4.8862 - acc: 0.006 - ETA: 1:05 - loss: 4.8863 - acc: 0.006 - ETA: 1:04 - loss: 4.8865 - acc: 0.006 - ETA: 1:03 - loss: 4.8867 - acc: 0.006 - ETA: 1:03 - loss: 4.8864 - acc: 0.006 - ETA: 1:02 - loss: 4.8864 - acc: 0.006 - ETA: 1:01 - loss: 4.8863 - acc: 0.006 - ETA: 1:01 - loss: 4.8862 - acc: 0.006 - ETA: 1:00 - loss: 4.8863 - acc: 0.006 - ETA: 59s - loss: 4.8860 - acc: 0.006 - ETA: 59s - loss: 4.8860 - acc: 0.00 - ETA: 58s - loss: 4.8858 - acc: 0.00 - ETA: 57s - loss: 4.8859 - acc: 0.00 - ETA: 56s - loss: 4.8862 - acc: 0.00 - ETA: 56s - loss: 4.8860 - acc: 0.00 - ETA: 55s - loss: 4.8860 - acc: 0.00 - ETA: 54s - loss: 4.8860 - acc: 0.00 - ETA: 54s - loss: 4.8859 - acc: 0.00 - ETA: 53s - loss: 4.8858 - acc: 0.00 - ETA: 52s - loss: 4.8859 - acc: 0.00 - ETA: 51s - loss: 4.8859 - acc: 0.00 - ETA: 51s - loss: 4.8858 - acc: 0.00 - ETA: 50s - loss: 4.8858 - acc: 0.00 - ETA: 49s - loss: 4.8859 - acc: 0.00 - ETA: 49s - loss: 4.8860 - acc: 0.00 - ETA: 48s - loss: 4.8858 - acc: 0.00 - ETA: 47s - loss: 4.8857 - acc: 0.00 - ETA: 47s - loss: 4.8855 - acc: 0.00 - ETA: 46s - loss: 4.8853 - acc: 0.00 - ETA: 45s - loss: 4.8853 - acc: 0.00 - ETA: 44s - loss: 4.8851 - acc: 0.00 - ETA: 44s - loss: 4.8853 - acc: 0.00 - ETA: 43s - loss: 4.8851 - acc: 0.00 - ETA: 42s - loss: 4.8853 - acc: 0.00 - ETA: 42s - loss: 4.8850 - acc: 0.00 - ETA: 41s - loss: 4.8851 - acc: 0.00 - ETA: 40s - loss: 4.8854 - acc: 0.00 - ETA: 39s - loss: 4.8853 - acc: 0.00 - ETA: 39s - loss: 4.8853 - acc: 0.00 - ETA: 38s - loss: 4.8853 - acc: 0.00 - ETA: 37s - loss: 4.8853 - acc: 0.00 - ETA: 37s - loss: 4.8852 - acc: 0.00 - ETA: 36s - loss: 4.8851 - acc: 0.00 - ETA: 35s - loss: 4.8850 - acc: 0.00 - ETA: 35s - loss: 4.8848 - acc: 0.00 - ETA: 34s - loss: 4.8847 - acc: 0.00 - ETA: 33s - loss: 4.8846 - acc: 0.00 - ETA: 32s - loss: 4.8845 - acc: 0.00 - ETA: 32s - loss: 4.8844 - acc: 0.00 - ETA: 31s - loss: 4.8845 - acc: 0.00 - ETA: 30s - loss: 4.8846 - acc: 0.00 - ETA: 30s - loss: 4.8848 - acc: 0.00 - ETA: 29s - loss: 4.8845 - acc: 0.00 - ETA: 28s - loss: 4.8846 - acc: 0.00 - ETA: 28s - loss: 4.8845 - acc: 0.00 - ETA: 27s - loss: 4.8845 - acc: 0.00 - ETA: 26s - loss: 4.8843 - acc: 0.00 - ETA: 25s - loss: 4.8843 - acc: 0.00 - ETA: 25s - loss: 4.8843 - acc: 0.00 - ETA: 24s - loss: 4.8844 - acc: 0.00 - ETA: 23s - loss: 4.8844 - acc: 0.00 - ETA: 23s - loss: 4.8844 - acc: 0.00 - ETA: 22s - loss: 4.8842 - acc: 0.00 - ETA: 21s - loss: 4.8841 - acc: 0.00 - ETA: 21s - loss: 4.8842 - acc: 0.00 - ETA: 20s - loss: 4.8840 - acc: 0.00 - ETA: 19s - loss: 4.8839 - acc: 0.00 - ETA: 18s - loss: 4.8839 - acc: 0.00 - ETA: 18s - loss: 4.8838 - acc: 0.00 - ETA: 17s - loss: 4.8839 - acc: 0.00 - ETA: 16s - loss: 4.8839 - acc: 0.00 - ETA: 16s - loss: 4.8840 - acc: 0.00 - ETA: 15s - loss: 4.8841 - acc: 0.00 - ETA: 14s - loss: 4.8843 - acc: 0.00 - ETA: 14s - loss: 4.8843 - acc: 0.00 - ETA: 13s - loss: 4.8843 - acc: 0.00 - ETA: 12s - loss: 4.8843 - acc: 0.00 - ETA: 11s - loss: 4.8844 - acc: 0.00 - ETA: 11s - loss: 4.8844 - acc: 0.00 - ETA: 10s - loss: 4.8843 - acc: 0.00 - ETA: 9s - loss: 4.8845 - acc: 0.0070 - ETA: 9s - loss: 4.8845 - acc: 0.007 - ETA: 8s - loss: 4.8844 - acc: 0.007 - ETA: 7s - loss: 4.8845 - acc: 0.007 - ETA: 7s - loss: 4.8845 - acc: 0.006 - ETA: 6s - loss: 4.8846 - acc: 0.006 - ETA: 5s - loss: 4.8845 - acc: 0.006 - ETA: 4s - loss: 4.8845 - acc: 0.007 - ETA: 4s - loss: 4.8845 - acc: 0.007 - ETA: 3s - loss: 4.8845 - acc: 0.007 - ETA: 2s - loss: 4.8845 - acc: 0.007 - ETA: 2s - loss: 4.8844 - acc: 0.006 - ETA: 1s - loss: 4.8846 - acc: 0.006 - ETA: 0s - loss: 4.8847 - acc: 0.0069Epoch 00001: val_loss improved from inf to 4.87200, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 244s 37ms/step - loss: 4.8846 - acc: 0.0070 - val_loss: 4.8720 - val_acc: 0.0144\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4060/6680 [=================>............] - ETA: 3:48 - loss: 4.8779 - acc: 0.0000e+0 - ETA: 3:45 - loss: 4.8707 - acc: 0.0000e+0 - ETA: 3:46 - loss: 4.8733 - acc: 0.0000e+0 - ETA: 3:45 - loss: 4.8766 - acc: 0.0000e+0 - ETA: 3:42 - loss: 4.8688 - acc: 0.0000e+0 - ETA: 3:42 - loss: 4.8775 - acc: 0.0000e+0 - ETA: 3:41 - loss: 4.8703 - acc: 0.0000e+0 - ETA: 3:40 - loss: 4.8678 - acc: 0.0000e+0 - ETA: 3:39 - loss: 4.8635 - acc: 0.0000e+0 - ETA: 3:38 - loss: 4.8625 - acc: 0.0100    - ETA: 3:38 - loss: 4.8557 - acc: 0.009 - ETA: 3:37 - loss: 4.8633 - acc: 0.008 - ETA: 3:36 - loss: 4.8638 - acc: 0.007 - ETA: 3:35 - loss: 4.8667 - acc: 0.007 - ETA: 3:35 - loss: 4.8629 - acc: 0.010 - ETA: 3:34 - loss: 4.8643 - acc: 0.009 - ETA: 3:33 - loss: 4.8648 - acc: 0.011 - ETA: 3:32 - loss: 4.8641 - acc: 0.013 - ETA: 3:32 - loss: 4.8611 - acc: 0.013 - ETA: 3:31 - loss: 4.8660 - acc: 0.012 - ETA: 3:31 - loss: 4.8670 - acc: 0.011 - ETA: 3:30 - loss: 4.8686 - acc: 0.011 - ETA: 3:29 - loss: 4.8662 - acc: 0.010 - ETA: 3:28 - loss: 4.8654 - acc: 0.010 - ETA: 3:28 - loss: 4.8641 - acc: 0.012 - ETA: 3:27 - loss: 4.8630 - acc: 0.011 - ETA: 3:26 - loss: 4.8653 - acc: 0.011 - ETA: 3:26 - loss: 4.8660 - acc: 0.010 - ETA: 3:25 - loss: 4.8658 - acc: 0.010 - ETA: 3:25 - loss: 4.8657 - acc: 0.013 - ETA: 3:24 - loss: 4.8653 - acc: 0.012 - ETA: 3:24 - loss: 4.8676 - acc: 0.012 - ETA: 3:23 - loss: 4.8656 - acc: 0.012 - ETA: 3:22 - loss: 4.8660 - acc: 0.011 - ETA: 3:22 - loss: 4.8687 - acc: 0.011 - ETA: 3:21 - loss: 4.8675 - acc: 0.012 - ETA: 3:20 - loss: 4.8663 - acc: 0.012 - ETA: 3:20 - loss: 4.8671 - acc: 0.011 - ETA: 3:19 - loss: 4.8677 - acc: 0.011 - ETA: 3:18 - loss: 4.8679 - acc: 0.011 - ETA: 3:18 - loss: 4.8673 - acc: 0.011 - ETA: 3:17 - loss: 4.8655 - acc: 0.011 - ETA: 3:16 - loss: 4.8657 - acc: 0.011 - ETA: 3:16 - loss: 4.8648 - acc: 0.011 - ETA: 3:15 - loss: 4.8654 - acc: 0.011 - ETA: 3:15 - loss: 4.8663 - acc: 0.010 - ETA: 3:14 - loss: 4.8665 - acc: 0.010 - ETA: 3:13 - loss: 4.8668 - acc: 0.010 - ETA: 3:12 - loss: 4.8665 - acc: 0.011 - ETA: 3:12 - loss: 4.8671 - acc: 0.011 - ETA: 3:11 - loss: 4.8677 - acc: 0.010 - ETA: 3:10 - loss: 4.8673 - acc: 0.010 - ETA: 3:10 - loss: 4.8677 - acc: 0.010 - ETA: 3:09 - loss: 4.8678 - acc: 0.010 - ETA: 3:09 - loss: 4.8690 - acc: 0.010 - ETA: 3:08 - loss: 4.8688 - acc: 0.010 - ETA: 3:07 - loss: 4.8695 - acc: 0.010 - ETA: 3:07 - loss: 4.8699 - acc: 0.010 - ETA: 3:06 - loss: 4.8694 - acc: 0.011 - ETA: 3:06 - loss: 4.8700 - acc: 0.011 - ETA: 3:05 - loss: 4.8691 - acc: 0.011 - ETA: 3:04 - loss: 4.8688 - acc: 0.011 - ETA: 3:04 - loss: 4.8683 - acc: 0.011 - ETA: 3:03 - loss: 4.8691 - acc: 0.010 - ETA: 3:02 - loss: 4.8693 - acc: 0.010 - ETA: 3:02 - loss: 4.8693 - acc: 0.010 - ETA: 3:01 - loss: 4.8691 - acc: 0.010 - ETA: 3:00 - loss: 4.8694 - acc: 0.010 - ETA: 2:59 - loss: 4.8705 - acc: 0.010 - ETA: 2:59 - loss: 4.8695 - acc: 0.010 - ETA: 2:58 - loss: 4.8694 - acc: 0.010 - ETA: 2:57 - loss: 4.8696 - acc: 0.010 - ETA: 2:57 - loss: 4.8699 - acc: 0.011 - ETA: 2:56 - loss: 4.8689 - acc: 0.011 - ETA: 2:55 - loss: 4.8694 - acc: 0.011 - ETA: 2:55 - loss: 4.8697 - acc: 0.011 - ETA: 2:54 - loss: 4.8701 - acc: 0.011 - ETA: 2:53 - loss: 4.8697 - acc: 0.011 - ETA: 2:53 - loss: 4.8701 - acc: 0.011 - ETA: 2:52 - loss: 4.8697 - acc: 0.011 - ETA: 2:51 - loss: 4.8700 - acc: 0.011 - ETA: 2:51 - loss: 4.8703 - acc: 0.011 - ETA: 2:50 - loss: 4.8702 - acc: 0.010 - ETA: 2:49 - loss: 4.8700 - acc: 0.010 - ETA: 2:49 - loss: 4.8704 - acc: 0.010 - ETA: 2:48 - loss: 4.8707 - acc: 0.010 - ETA: 2:47 - loss: 4.8701 - acc: 0.010 - ETA: 2:47 - loss: 4.8694 - acc: 0.011 - ETA: 2:46 - loss: 4.8691 - acc: 0.011 - ETA: 2:45 - loss: 4.8693 - acc: 0.011 - ETA: 2:45 - loss: 4.8688 - acc: 0.011 - ETA: 2:44 - loss: 4.8684 - acc: 0.011 - ETA: 2:43 - loss: 4.8678 - acc: 0.011 - ETA: 2:43 - loss: 4.8677 - acc: 0.011 - ETA: 2:42 - loss: 4.8676 - acc: 0.012 - ETA: 2:41 - loss: 4.8671 - acc: 0.012 - ETA: 2:41 - loss: 4.8672 - acc: 0.011 - ETA: 2:40 - loss: 4.8674 - acc: 0.011 - ETA: 2:39 - loss: 4.8674 - acc: 0.011 - ETA: 2:38 - loss: 4.8673 - acc: 0.012 - ETA: 2:38 - loss: 4.8679 - acc: 0.011 - ETA: 2:37 - loss: 4.8677 - acc: 0.011 - ETA: 2:36 - loss: 4.8680 - acc: 0.011 - ETA: 2:36 - loss: 4.8675 - acc: 0.011 - ETA: 2:35 - loss: 4.8677 - acc: 0.011 - ETA: 2:34 - loss: 4.8676 - acc: 0.011 - ETA: 2:34 - loss: 4.8676 - acc: 0.011 - ETA: 2:33 - loss: 4.8668 - acc: 0.011 - ETA: 2:32 - loss: 4.8673 - acc: 0.011 - ETA: 2:32 - loss: 4.8668 - acc: 0.011 - ETA: 2:31 - loss: 4.8667 - acc: 0.011 - ETA: 2:30 - loss: 4.8666 - acc: 0.011 - ETA: 2:30 - loss: 4.8668 - acc: 0.011 - ETA: 2:29 - loss: 4.8675 - acc: 0.011 - ETA: 2:28 - loss: 4.8671 - acc: 0.011 - ETA: 2:28 - loss: 4.8669 - acc: 0.011 - ETA: 2:27 - loss: 4.8672 - acc: 0.011 - ETA: 2:26 - loss: 4.8670 - acc: 0.011 - ETA: 2:25 - loss: 4.8675 - acc: 0.011 - ETA: 2:25 - loss: 4.8678 - acc: 0.011 - ETA: 2:24 - loss: 4.8688 - acc: 0.011 - ETA: 2:23 - loss: 4.8689 - acc: 0.011 - ETA: 2:23 - loss: 4.8687 - acc: 0.011 - ETA: 2:22 - loss: 4.8683 - acc: 0.011 - ETA: 2:21 - loss: 4.8681 - acc: 0.011 - ETA: 2:21 - loss: 4.8680 - acc: 0.011 - ETA: 2:20 - loss: 4.8677 - acc: 0.011 - ETA: 2:19 - loss: 4.8675 - acc: 0.011 - ETA: 2:19 - loss: 4.8676 - acc: 0.011 - ETA: 2:18 - loss: 4.8673 - acc: 0.011 - ETA: 2:17 - loss: 4.8687 - acc: 0.011 - ETA: 2:17 - loss: 4.8685 - acc: 0.011 - ETA: 2:16 - loss: 4.8682 - acc: 0.010 - ETA: 2:15 - loss: 4.8674 - acc: 0.011 - ETA: 2:14 - loss: 4.8670 - acc: 0.011 - ETA: 2:14 - loss: 4.8664 - acc: 0.011 - ETA: 2:13 - loss: 4.8662 - acc: 0.010 - ETA: 2:12 - loss: 4.8668 - acc: 0.010 - ETA: 2:12 - loss: 4.8665 - acc: 0.010 - ETA: 2:11 - loss: 4.8666 - acc: 0.011 - ETA: 2:10 - loss: 4.8663 - acc: 0.011 - ETA: 2:10 - loss: 4.8667 - acc: 0.011 - ETA: 2:09 - loss: 4.8665 - acc: 0.011 - ETA: 2:08 - loss: 4.8663 - acc: 0.011 - ETA: 2:08 - loss: 4.8660 - acc: 0.011 - ETA: 2:07 - loss: 4.8656 - acc: 0.011 - ETA: 2:06 - loss: 4.8655 - acc: 0.011 - ETA: 2:06 - loss: 4.8654 - acc: 0.011 - ETA: 2:05 - loss: 4.8654 - acc: 0.011 - ETA: 2:04 - loss: 4.8647 - acc: 0.011 - ETA: 2:04 - loss: 4.8647 - acc: 0.011 - ETA: 2:03 - loss: 4.8650 - acc: 0.011 - ETA: 2:02 - loss: 4.8642 - acc: 0.011 - ETA: 2:02 - loss: 4.8647 - acc: 0.011 - ETA: 2:01 - loss: 4.8652 - acc: 0.011 - ETA: 2:00 - loss: 4.8648 - acc: 0.011 - ETA: 2:00 - loss: 4.8647 - acc: 0.011 - ETA: 1:59 - loss: 4.8647 - acc: 0.011 - ETA: 1:58 - loss: 4.8639 - acc: 0.011 - ETA: 1:58 - loss: 4.8639 - acc: 0.011 - ETA: 1:57 - loss: 4.8642 - acc: 0.011 - ETA: 1:56 - loss: 4.8643 - acc: 0.011 - ETA: 1:56 - loss: 4.8644 - acc: 0.012 - ETA: 1:55 - loss: 4.8641 - acc: 0.012 - ETA: 1:54 - loss: 4.8636 - acc: 0.012 - ETA: 1:54 - loss: 4.8635 - acc: 0.012 - ETA: 1:53 - loss: 4.8637 - acc: 0.012 - ETA: 1:52 - loss: 4.8636 - acc: 0.012 - ETA: 1:51 - loss: 4.8640 - acc: 0.012 - ETA: 1:51 - loss: 4.8639 - acc: 0.012 - ETA: 1:50 - loss: 4.8638 - acc: 0.012 - ETA: 1:49 - loss: 4.8635 - acc: 0.012 - ETA: 1:49 - loss: 4.8635 - acc: 0.012 - ETA: 1:48 - loss: 4.8635 - acc: 0.012 - ETA: 1:47 - loss: 4.8635 - acc: 0.012 - ETA: 1:47 - loss: 4.8634 - acc: 0.011 - ETA: 1:46 - loss: 4.8631 - acc: 0.012 - ETA: 1:45 - loss: 4.8630 - acc: 0.012 - ETA: 1:45 - loss: 4.8632 - acc: 0.012 - ETA: 1:44 - loss: 4.8631 - acc: 0.012 - ETA: 1:43 - loss: 4.8639 - acc: 0.012 - ETA: 1:43 - loss: 4.8640 - acc: 0.012 - ETA: 1:42 - loss: 4.8637 - acc: 0.012 - ETA: 1:41 - loss: 4.8635 - acc: 0.012 - ETA: 1:41 - loss: 4.8638 - acc: 0.012 - ETA: 1:40 - loss: 4.8639 - acc: 0.012 - ETA: 1:39 - loss: 4.8639 - acc: 0.012 - ETA: 1:39 - loss: 4.8639 - acc: 0.012 - ETA: 1:38 - loss: 4.8639 - acc: 0.012 - ETA: 1:37 - loss: 4.8639 - acc: 0.012 - ETA: 1:37 - loss: 4.8641 - acc: 0.012 - ETA: 1:36 - loss: 4.8643 - acc: 0.012 - ETA: 1:35 - loss: 4.8643 - acc: 0.012 - ETA: 1:35 - loss: 4.8642 - acc: 0.012 - ETA: 1:34 - loss: 4.8637 - acc: 0.012 - ETA: 1:33 - loss: 4.8640 - acc: 0.012 - ETA: 1:32 - loss: 4.8646 - acc: 0.012 - ETA: 1:32 - loss: 4.8645 - acc: 0.012 - ETA: 1:31 - loss: 4.8644 - acc: 0.012 - ETA: 1:30 - loss: 4.8643 - acc: 0.012 - ETA: 1:30 - loss: 4.8643 - acc: 0.012 - ETA: 1:29 - loss: 4.8646 - acc: 0.012 - ETA: 1:28 - loss: 4.8641 - acc: 0.0128"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 1:28 - loss: 4.8640 - acc: 0.012 - ETA: 1:27 - loss: 4.8640 - acc: 0.012 - ETA: 1:26 - loss: 4.8642 - acc: 0.012 - ETA: 1:26 - loss: 4.8645 - acc: 0.012 - ETA: 1:25 - loss: 4.8645 - acc: 0.012 - ETA: 1:24 - loss: 4.8647 - acc: 0.012 - ETA: 1:24 - loss: 4.8644 - acc: 0.012 - ETA: 1:23 - loss: 4.8646 - acc: 0.012 - ETA: 1:22 - loss: 4.8648 - acc: 0.012 - ETA: 1:22 - loss: 4.8646 - acc: 0.012 - ETA: 1:21 - loss: 4.8644 - acc: 0.012 - ETA: 1:20 - loss: 4.8642 - acc: 0.012 - ETA: 1:20 - loss: 4.8642 - acc: 0.012 - ETA: 1:19 - loss: 4.8641 - acc: 0.012 - ETA: 1:18 - loss: 4.8641 - acc: 0.012 - ETA: 1:18 - loss: 4.8642 - acc: 0.012 - ETA: 1:17 - loss: 4.8642 - acc: 0.012 - ETA: 1:16 - loss: 4.8642 - acc: 0.012 - ETA: 1:16 - loss: 4.8640 - acc: 0.012 - ETA: 1:15 - loss: 4.8642 - acc: 0.012 - ETA: 1:14 - loss: 4.8643 - acc: 0.012 - ETA: 1:14 - loss: 4.8641 - acc: 0.012 - ETA: 1:13 - loss: 4.8640 - acc: 0.012 - ETA: 1:12 - loss: 4.8639 - acc: 0.012 - ETA: 1:12 - loss: 4.8639 - acc: 0.012 - ETA: 1:11 - loss: 4.8637 - acc: 0.013 - ETA: 1:10 - loss: 4.8635 - acc: 0.013 - ETA: 1:10 - loss: 4.8633 - acc: 0.013 - ETA: 1:09 - loss: 4.8627 - acc: 0.013 - ETA: 1:08 - loss: 4.8628 - acc: 0.013 - ETA: 1:08 - loss: 4.8629 - acc: 0.013 - ETA: 1:07 - loss: 4.8633 - acc: 0.013 - ETA: 1:06 - loss: 4.8633 - acc: 0.013 - ETA: 1:06 - loss: 4.8631 - acc: 0.013 - ETA: 1:05 - loss: 4.8633 - acc: 0.013 - ETA: 1:04 - loss: 4.8638 - acc: 0.013 - ETA: 1:04 - loss: 4.8638 - acc: 0.013 - ETA: 1:03 - loss: 4.8639 - acc: 0.013 - ETA: 1:02 - loss: 4.8640 - acc: 0.013 - ETA: 1:02 - loss: 4.8642 - acc: 0.013 - ETA: 1:01 - loss: 4.8641 - acc: 0.013 - ETA: 1:00 - loss: 4.8643 - acc: 0.013 - ETA: 1:00 - loss: 4.8642 - acc: 0.013 - ETA: 59s - loss: 4.8643 - acc: 0.013 - ETA: 58s - loss: 4.8641 - acc: 0.01 - ETA: 57s - loss: 4.8639 - acc: 0.01 - ETA: 57s - loss: 4.8639 - acc: 0.01 - ETA: 56s - loss: 4.8639 - acc: 0.01 - ETA: 55s - loss: 4.8640 - acc: 0.01 - ETA: 55s - loss: 4.8639 - acc: 0.01 - ETA: 54s - loss: 4.8639 - acc: 0.01 - ETA: 53s - loss: 4.8640 - acc: 0.01 - ETA: 53s - loss: 4.8639 - acc: 0.01 - ETA: 52s - loss: 4.8638 - acc: 0.01 - ETA: 51s - loss: 4.8639 - acc: 0.01 - ETA: 51s - loss: 4.8635 - acc: 0.01 - ETA: 50s - loss: 4.8634 - acc: 0.01 - ETA: 49s - loss: 4.8641 - acc: 0.01 - ETA: 49s - loss: 4.8640 - acc: 0.01 - ETA: 48s - loss: 4.8638 - acc: 0.01 - ETA: 47s - loss: 4.8638 - acc: 0.01 - ETA: 47s - loss: 4.8636 - acc: 0.01 - ETA: 46s - loss: 4.8635 - acc: 0.01 - ETA: 45s - loss: 4.8634 - acc: 0.01 - ETA: 45s - loss: 4.8632 - acc: 0.01 - ETA: 44s - loss: 4.8629 - acc: 0.01 - ETA: 43s - loss: 4.8631 - acc: 0.01 - ETA: 42s - loss: 4.8631 - acc: 0.01 - ETA: 42s - loss: 4.8630 - acc: 0.01 - ETA: 41s - loss: 4.8629 - acc: 0.01 - ETA: 40s - loss: 4.8628 - acc: 0.01 - ETA: 40s - loss: 4.8633 - acc: 0.01 - ETA: 39s - loss: 4.8633 - acc: 0.01 - ETA: 38s - loss: 4.8633 - acc: 0.01 - ETA: 38s - loss: 4.8632 - acc: 0.01 - ETA: 37s - loss: 4.8630 - acc: 0.01 - ETA: 36s - loss: 4.8632 - acc: 0.01 - ETA: 36s - loss: 4.8629 - acc: 0.01 - ETA: 35s - loss: 4.8630 - acc: 0.01 - ETA: 34s - loss: 4.8631 - acc: 0.01 - ETA: 34s - loss: 4.8630 - acc: 0.01 - ETA: 33s - loss: 4.8631 - acc: 0.01 - ETA: 32s - loss: 4.8629 - acc: 0.01 - ETA: 32s - loss: 4.8628 - acc: 0.01 - ETA: 31s - loss: 4.8632 - acc: 0.01 - ETA: 30s - loss: 4.8631 - acc: 0.01 - ETA: 30s - loss: 4.8631 - acc: 0.01 - ETA: 29s - loss: 4.8634 - acc: 0.01 - ETA: 28s - loss: 4.8634 - acc: 0.01 - ETA: 27s - loss: 4.8634 - acc: 0.01 - ETA: 27s - loss: 4.8636 - acc: 0.01 - ETA: 26s - loss: 4.8635 - acc: 0.01 - ETA: 25s - loss: 4.8635 - acc: 0.01 - ETA: 25s - loss: 4.8634 - acc: 0.01 - ETA: 24s - loss: 4.8635 - acc: 0.01 - ETA: 23s - loss: 4.8633 - acc: 0.01 - ETA: 23s - loss: 4.8632 - acc: 0.01 - ETA: 22s - loss: 4.8630 - acc: 0.01 - ETA: 21s - loss: 4.8632 - acc: 0.01 - ETA: 21s - loss: 4.8631 - acc: 0.01 - ETA: 20s - loss: 4.8629 - acc: 0.01 - ETA: 19s - loss: 4.8627 - acc: 0.01 - ETA: 19s - loss: 4.8628 - acc: 0.01 - ETA: 18s - loss: 4.8634 - acc: 0.01 - ETA: 17s - loss: 4.8630 - acc: 0.01 - ETA: 17s - loss: 4.8630 - acc: 0.01 - ETA: 16s - loss: 4.8629 - acc: 0.01 - ETA: 15s - loss: 4.8626 - acc: 0.01 - ETA: 14s - loss: 4.8625 - acc: 0.01 - ETA: 14s - loss: 4.8623 - acc: 0.01 - ETA: 13s - loss: 4.8623 - acc: 0.01 - ETA: 12s - loss: 4.8623 - acc: 0.01 - ETA: 12s - loss: 4.8622 - acc: 0.01 - ETA: 11s - loss: 4.8620 - acc: 0.01 - ETA: 10s - loss: 4.8618 - acc: 0.01 - ETA: 10s - loss: 4.8618 - acc: 0.01 - ETA: 9s - loss: 4.8619 - acc: 0.0134 - ETA: 8s - loss: 4.8617 - acc: 0.013 - ETA: 8s - loss: 4.8619 - acc: 0.013 - ETA: 7s - loss: 4.8620 - acc: 0.013 - ETA: 6s - loss: 4.8619 - acc: 0.013 - ETA: 6s - loss: 4.8621 - acc: 0.013 - ETA: 5s - loss: 4.8621 - acc: 0.013 - ETA: 4s - loss: 4.8623 - acc: 0.013 - ETA: 4s - loss: 4.8621 - acc: 0.013 - ETA: 3s - loss: 4.8621 - acc: 0.013 - ETA: 2s - loss: 4.8621 - acc: 0.013 - ETA: 2s - loss: 4.8618 - acc: 0.013 - ETA: 1s - loss: 4.8617 - acc: 0.013 - ETA: 0s - loss: 4.8618 - acc: 0.0129Epoch 00002: val_loss improved from 4.87200 to 4.84306, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 238s 36ms/step - loss: 4.8619 - acc: 0.0130 - val_loss: 4.8431 - val_acc: 0.0192\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/6680 [=================>............] - ETA: 3:41 - loss: 4.8806 - acc: 0.0000e+0 - ETA: 3:46 - loss: 4.8384 - acc: 0.0000e+0 - ETA: 3:45 - loss: 4.8048 - acc: 0.0333    - ETA: 3:43 - loss: 4.8122 - acc: 0.025 - ETA: 3:44 - loss: 4.8106 - acc: 0.020 - ETA: 3:43 - loss: 4.8238 - acc: 0.025 - ETA: 3:42 - loss: 4.8171 - acc: 0.021 - ETA: 3:42 - loss: 4.8137 - acc: 0.018 - ETA: 3:42 - loss: 4.8050 - acc: 0.022 - ETA: 3:41 - loss: 4.8101 - acc: 0.020 - ETA: 3:40 - loss: 4.8181 - acc: 0.018 - ETA: 3:39 - loss: 4.8223 - acc: 0.020 - ETA: 3:38 - loss: 4.8316 - acc: 0.019 - ETA: 3:37 - loss: 4.8303 - acc: 0.017 - ETA: 3:36 - loss: 4.8226 - acc: 0.016 - ETA: 3:36 - loss: 4.8185 - acc: 0.018 - ETA: 3:35 - loss: 4.8194 - acc: 0.017 - ETA: 3:34 - loss: 4.8219 - acc: 0.016 - ETA: 3:34 - loss: 4.8236 - acc: 0.018 - ETA: 3:33 - loss: 4.8193 - acc: 0.020 - ETA: 3:32 - loss: 4.8276 - acc: 0.019 - ETA: 3:31 - loss: 4.8286 - acc: 0.018 - ETA: 3:31 - loss: 4.8283 - acc: 0.017 - ETA: 3:30 - loss: 4.8262 - acc: 0.016 - ETA: 3:29 - loss: 4.8254 - acc: 0.016 - ETA: 3:28 - loss: 4.8275 - acc: 0.015 - ETA: 3:28 - loss: 4.8277 - acc: 0.014 - ETA: 3:27 - loss: 4.8280 - acc: 0.014 - ETA: 3:26 - loss: 4.8264 - acc: 0.015 - ETA: 3:26 - loss: 4.8222 - acc: 0.016 - ETA: 3:25 - loss: 4.8234 - acc: 0.016 - ETA: 3:24 - loss: 4.8268 - acc: 0.015 - ETA: 3:23 - loss: 4.8250 - acc: 0.016 - ETA: 3:23 - loss: 4.8250 - acc: 0.016 - ETA: 3:22 - loss: 4.8289 - acc: 0.015 - ETA: 3:22 - loss: 4.8324 - acc: 0.015 - ETA: 3:21 - loss: 4.8337 - acc: 0.014 - ETA: 3:20 - loss: 4.8324 - acc: 0.014 - ETA: 3:20 - loss: 4.8319 - acc: 0.014 - ETA: 3:19 - loss: 4.8313 - acc: 0.015 - ETA: 3:18 - loss: 4.8322 - acc: 0.014 - ETA: 3:18 - loss: 4.8313 - acc: 0.014 - ETA: 3:17 - loss: 4.8297 - acc: 0.015 - ETA: 3:16 - loss: 4.8315 - acc: 0.014 - ETA: 3:16 - loss: 4.8303 - acc: 0.015 - ETA: 3:15 - loss: 4.8318 - acc: 0.015 - ETA: 3:14 - loss: 4.8324 - acc: 0.014 - ETA: 3:14 - loss: 4.8328 - acc: 0.014 - ETA: 3:14 - loss: 4.8326 - acc: 0.014 - ETA: 3:14 - loss: 4.8349 - acc: 0.014 - ETA: 3:13 - loss: 4.8349 - acc: 0.014 - ETA: 3:13 - loss: 4.8333 - acc: 0.015 - ETA: 3:13 - loss: 4.8328 - acc: 0.015 - ETA: 3:13 - loss: 4.8327 - acc: 0.014 - ETA: 3:12 - loss: 4.8333 - acc: 0.014 - ETA: 3:12 - loss: 4.8325 - acc: 0.015 - ETA: 3:11 - loss: 4.8330 - acc: 0.014 - ETA: 3:10 - loss: 4.8326 - acc: 0.014 - ETA: 3:10 - loss: 4.8341 - acc: 0.014 - ETA: 3:09 - loss: 4.8345 - acc: 0.014 - ETA: 3:08 - loss: 4.8350 - acc: 0.014 - ETA: 3:08 - loss: 4.8358 - acc: 0.014 - ETA: 3:07 - loss: 4.8381 - acc: 0.014 - ETA: 3:07 - loss: 4.8377 - acc: 0.014 - ETA: 3:06 - loss: 4.8367 - acc: 0.015 - ETA: 3:05 - loss: 4.8370 - acc: 0.015 - ETA: 3:05 - loss: 4.8356 - acc: 0.014 - ETA: 3:05 - loss: 4.8354 - acc: 0.015 - ETA: 3:05 - loss: 4.8347 - acc: 0.015 - ETA: 3:04 - loss: 4.8339 - acc: 0.015 - ETA: 3:03 - loss: 4.8343 - acc: 0.014 - ETA: 3:03 - loss: 4.8326 - acc: 0.014 - ETA: 3:02 - loss: 4.8343 - acc: 0.014 - ETA: 3:01 - loss: 4.8324 - acc: 0.014 - ETA: 3:00 - loss: 4.8329 - acc: 0.014 - ETA: 3:00 - loss: 4.8316 - acc: 0.013 - ETA: 3:00 - loss: 4.8300 - acc: 0.013 - ETA: 3:00 - loss: 4.8299 - acc: 0.013 - ETA: 3:00 - loss: 4.8305 - acc: 0.013 - ETA: 3:00 - loss: 4.8314 - acc: 0.013 - ETA: 3:00 - loss: 4.8316 - acc: 0.013 - ETA: 2:59 - loss: 4.8304 - acc: 0.013 - ETA: 2:58 - loss: 4.8303 - acc: 0.013 - ETA: 2:58 - loss: 4.8290 - acc: 0.014 - ETA: 2:57 - loss: 4.8291 - acc: 0.014 - ETA: 2:57 - loss: 4.8284 - acc: 0.014 - ETA: 2:57 - loss: 4.8280 - acc: 0.013 - ETA: 2:56 - loss: 4.8278 - acc: 0.013 - ETA: 2:56 - loss: 4.8269 - acc: 0.013 - ETA: 2:55 - loss: 4.8280 - acc: 0.013 - ETA: 2:54 - loss: 4.8274 - acc: 0.013 - ETA: 2:53 - loss: 4.8283 - acc: 0.013 - ETA: 2:52 - loss: 4.8282 - acc: 0.013 - ETA: 2:51 - loss: 4.8274 - acc: 0.013 - ETA: 2:50 - loss: 4.8274 - acc: 0.013 - ETA: 2:50 - loss: 4.8281 - acc: 0.013 - ETA: 2:49 - loss: 4.8269 - acc: 0.013 - ETA: 2:48 - loss: 4.8273 - acc: 0.013 - ETA: 2:47 - loss: 4.8283 - acc: 0.013 - ETA: 2:46 - loss: 4.8288 - acc: 0.013 - ETA: 2:46 - loss: 4.8279 - acc: 0.013 - ETA: 2:45 - loss: 4.8283 - acc: 0.013 - ETA: 2:44 - loss: 4.8287 - acc: 0.013 - ETA: 2:43 - loss: 4.8281 - acc: 0.013 - ETA: 2:43 - loss: 4.8280 - acc: 0.012 - ETA: 2:42 - loss: 4.8297 - acc: 0.012 - ETA: 2:42 - loss: 4.8293 - acc: 0.012 - ETA: 2:41 - loss: 4.8281 - acc: 0.012 - ETA: 2:40 - loss: 4.8304 - acc: 0.012 - ETA: 2:39 - loss: 4.8321 - acc: 0.012 - ETA: 2:38 - loss: 4.8315 - acc: 0.012 - ETA: 2:38 - loss: 4.8307 - acc: 0.012 - ETA: 2:37 - loss: 4.8320 - acc: 0.011 - ETA: 2:36 - loss: 4.8323 - acc: 0.011 - ETA: 2:35 - loss: 4.8321 - acc: 0.012 - ETA: 2:34 - loss: 4.8321 - acc: 0.012 - ETA: 2:34 - loss: 4.8326 - acc: 0.012 - ETA: 2:33 - loss: 4.8318 - acc: 0.012 - ETA: 2:32 - loss: 4.8320 - acc: 0.012 - ETA: 2:31 - loss: 4.8316 - acc: 0.012 - ETA: 2:30 - loss: 4.8321 - acc: 0.012 - ETA: 2:30 - loss: 4.8321 - acc: 0.013 - ETA: 2:29 - loss: 4.8321 - acc: 0.013 - ETA: 2:28 - loss: 4.8325 - acc: 0.012 - ETA: 2:27 - loss: 4.8319 - acc: 0.012 - ETA: 2:27 - loss: 4.8309 - acc: 0.013 - ETA: 2:26 - loss: 4.8310 - acc: 0.013 - ETA: 2:25 - loss: 4.8307 - acc: 0.012 - ETA: 2:24 - loss: 4.8308 - acc: 0.012 - ETA: 2:23 - loss: 4.8310 - acc: 0.012 - ETA: 2:23 - loss: 4.8314 - acc: 0.013 - ETA: 2:22 - loss: 4.8316 - acc: 0.012 - ETA: 2:21 - loss: 4.8314 - acc: 0.012 - ETA: 2:20 - loss: 4.8301 - acc: 0.013 - ETA: 2:20 - loss: 4.8295 - acc: 0.013 - ETA: 2:19 - loss: 4.8291 - acc: 0.012 - ETA: 2:18 - loss: 4.8301 - acc: 0.012 - ETA: 2:18 - loss: 4.8305 - acc: 0.012 - ETA: 2:17 - loss: 4.8311 - acc: 0.012 - ETA: 2:17 - loss: 4.8309 - acc: 0.012 - ETA: 2:16 - loss: 4.8303 - acc: 0.012 - ETA: 2:15 - loss: 4.8306 - acc: 0.012 - ETA: 2:15 - loss: 4.8309 - acc: 0.012 - ETA: 2:14 - loss: 4.8309 - acc: 0.013 - ETA: 2:13 - loss: 4.8303 - acc: 0.013 - ETA: 2:13 - loss: 4.8297 - acc: 0.013 - ETA: 2:12 - loss: 4.8293 - acc: 0.013 - ETA: 2:12 - loss: 4.8287 - acc: 0.013 - ETA: 2:11 - loss: 4.8285 - acc: 0.013 - ETA: 2:11 - loss: 4.8292 - acc: 0.013 - ETA: 2:10 - loss: 4.8289 - acc: 0.012 - ETA: 2:09 - loss: 4.8287 - acc: 0.012 - ETA: 2:09 - loss: 4.8289 - acc: 0.012 - ETA: 2:08 - loss: 4.8286 - acc: 0.012 - ETA: 2:07 - loss: 4.8284 - acc: 0.012 - ETA: 2:06 - loss: 4.8294 - acc: 0.012 - ETA: 2:06 - loss: 4.8292 - acc: 0.013 - ETA: 2:05 - loss: 4.8296 - acc: 0.013 - ETA: 2:04 - loss: 4.8296 - acc: 0.013 - ETA: 2:03 - loss: 4.8295 - acc: 0.013 - ETA: 2:03 - loss: 4.8289 - acc: 0.013 - ETA: 2:02 - loss: 4.8289 - acc: 0.013 - ETA: 2:01 - loss: 4.8288 - acc: 0.013 - ETA: 2:00 - loss: 4.8287 - acc: 0.013 - ETA: 1:59 - loss: 4.8300 - acc: 0.013 - ETA: 1:59 - loss: 4.8301 - acc: 0.013 - ETA: 1:58 - loss: 4.8301 - acc: 0.012 - ETA: 1:57 - loss: 4.8305 - acc: 0.012 - ETA: 1:56 - loss: 4.8301 - acc: 0.013 - ETA: 1:56 - loss: 4.8303 - acc: 0.012 - ETA: 1:55 - loss: 4.8309 - acc: 0.012 - ETA: 1:54 - loss: 4.8314 - acc: 0.012 - ETA: 1:53 - loss: 4.8313 - acc: 0.013 - ETA: 1:53 - loss: 4.8314 - acc: 0.012 - ETA: 1:52 - loss: 4.8316 - acc: 0.013 - ETA: 1:51 - loss: 4.8320 - acc: 0.013 - ETA: 1:50 - loss: 4.8315 - acc: 0.013 - ETA: 1:50 - loss: 4.8320 - acc: 0.012 - ETA: 1:49 - loss: 4.8318 - acc: 0.012 - ETA: 1:48 - loss: 4.8313 - acc: 0.012 - ETA: 1:48 - loss: 4.8305 - acc: 0.013 - ETA: 1:47 - loss: 4.8307 - acc: 0.013 - ETA: 1:46 - loss: 4.8305 - acc: 0.013 - ETA: 1:45 - loss: 4.8306 - acc: 0.013 - ETA: 1:45 - loss: 4.8302 - acc: 0.013 - ETA: 1:44 - loss: 4.8301 - acc: 0.012 - ETA: 1:43 - loss: 4.8307 - acc: 0.012 - ETA: 1:43 - loss: 4.8304 - acc: 0.013 - ETA: 1:42 - loss: 4.8297 - acc: 0.013 - ETA: 1:41 - loss: 4.8287 - acc: 0.012 - ETA: 1:40 - loss: 4.8282 - acc: 0.013 - ETA: 1:40 - loss: 4.8281 - acc: 0.013 - ETA: 1:39 - loss: 4.8283 - acc: 0.013 - ETA: 1:38 - loss: 4.8292 - acc: 0.013 - ETA: 1:38 - loss: 4.8289 - acc: 0.013 - ETA: 1:37 - loss: 4.8288 - acc: 0.013 - ETA: 1:36 - loss: 4.8281 - acc: 0.012 - ETA: 1:35 - loss: 4.8281 - acc: 0.012 - ETA: 1:35 - loss: 4.8286 - acc: 0.012 - ETA: 1:34 - loss: 4.8281 - acc: 0.012 - ETA: 1:33 - loss: 4.8279 - acc: 0.012 - ETA: 1:32 - loss: 4.8282 - acc: 0.012 - ETA: 1:32 - loss: 4.8281 - acc: 0.012 - ETA: 1:31 - loss: 4.8278 - acc: 0.0130"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 1:30 - loss: 4.8276 - acc: 0.013 - ETA: 1:30 - loss: 4.8277 - acc: 0.013 - ETA: 1:29 - loss: 4.8275 - acc: 0.013 - ETA: 1:28 - loss: 4.8268 - acc: 0.013 - ETA: 1:27 - loss: 4.8270 - acc: 0.013 - ETA: 1:27 - loss: 4.8261 - acc: 0.013 - ETA: 1:26 - loss: 4.8260 - acc: 0.013 - ETA: 1:25 - loss: 4.8262 - acc: 0.013 - ETA: 1:25 - loss: 4.8264 - acc: 0.013 - ETA: 1:24 - loss: 4.8267 - acc: 0.013 - ETA: 1:23 - loss: 4.8264 - acc: 0.013 - ETA: 1:22 - loss: 4.8262 - acc: 0.013 - ETA: 1:22 - loss: 4.8262 - acc: 0.012 - ETA: 1:21 - loss: 4.8261 - acc: 0.012 - ETA: 1:20 - loss: 4.8258 - acc: 0.012 - ETA: 1:19 - loss: 4.8254 - acc: 0.012 - ETA: 1:19 - loss: 4.8260 - acc: 0.012 - ETA: 1:18 - loss: 4.8254 - acc: 0.012 - ETA: 1:17 - loss: 4.8254 - acc: 0.012 - ETA: 1:17 - loss: 4.8255 - acc: 0.012 - ETA: 1:16 - loss: 4.8256 - acc: 0.012 - ETA: 1:15 - loss: 4.8253 - acc: 0.012 - ETA: 1:14 - loss: 4.8257 - acc: 0.012 - ETA: 1:14 - loss: 4.8259 - acc: 0.012 - ETA: 1:13 - loss: 4.8261 - acc: 0.012 - ETA: 1:12 - loss: 4.8263 - acc: 0.012 - ETA: 1:12 - loss: 4.8262 - acc: 0.013 - ETA: 1:11 - loss: 4.8262 - acc: 0.013 - ETA: 1:10 - loss: 4.8262 - acc: 0.013 - ETA: 1:09 - loss: 4.8262 - acc: 0.013 - ETA: 1:09 - loss: 4.8260 - acc: 0.014 - ETA: 1:08 - loss: 4.8265 - acc: 0.014 - ETA: 1:07 - loss: 4.8265 - acc: 0.013 - ETA: 1:07 - loss: 4.8269 - acc: 0.013 - ETA: 1:06 - loss: 4.8271 - acc: 0.013 - ETA: 1:05 - loss: 4.8275 - acc: 0.013 - ETA: 1:05 - loss: 4.8272 - acc: 0.013 - ETA: 1:04 - loss: 4.8271 - acc: 0.013 - ETA: 1:03 - loss: 4.8269 - acc: 0.014 - ETA: 1:02 - loss: 4.8272 - acc: 0.013 - ETA: 1:02 - loss: 4.8272 - acc: 0.013 - ETA: 1:01 - loss: 4.8269 - acc: 0.014 - ETA: 1:00 - loss: 4.8259 - acc: 0.014 - ETA: 1:00 - loss: 4.8261 - acc: 0.014 - ETA: 59s - loss: 4.8260 - acc: 0.014 - ETA: 58s - loss: 4.8258 - acc: 0.01 - ETA: 58s - loss: 4.8261 - acc: 0.01 - ETA: 57s - loss: 4.8259 - acc: 0.01 - ETA: 56s - loss: 4.8257 - acc: 0.01 - ETA: 55s - loss: 4.8262 - acc: 0.01 - ETA: 55s - loss: 4.8264 - acc: 0.01 - ETA: 54s - loss: 4.8260 - acc: 0.01 - ETA: 53s - loss: 4.8259 - acc: 0.01 - ETA: 53s - loss: 4.8253 - acc: 0.01 - ETA: 52s - loss: 4.8247 - acc: 0.01 - ETA: 51s - loss: 4.8241 - acc: 0.01 - ETA: 50s - loss: 4.8239 - acc: 0.01 - ETA: 50s - loss: 4.8237 - acc: 0.01 - ETA: 49s - loss: 4.8233 - acc: 0.01 - ETA: 48s - loss: 4.8243 - acc: 0.01 - ETA: 48s - loss: 4.8240 - acc: 0.01 - ETA: 47s - loss: 4.8239 - acc: 0.01 - ETA: 46s - loss: 4.8233 - acc: 0.01 - ETA: 45s - loss: 4.8231 - acc: 0.01 - ETA: 45s - loss: 4.8234 - acc: 0.01 - ETA: 44s - loss: 4.8239 - acc: 0.01 - ETA: 43s - loss: 4.8243 - acc: 0.01 - ETA: 43s - loss: 4.8240 - acc: 0.01 - ETA: 42s - loss: 4.8238 - acc: 0.01 - ETA: 41s - loss: 4.8237 - acc: 0.01 - ETA: 41s - loss: 4.8238 - acc: 0.01 - ETA: 40s - loss: 4.8238 - acc: 0.01 - ETA: 39s - loss: 4.8233 - acc: 0.01 - ETA: 38s - loss: 4.8229 - acc: 0.01 - ETA: 38s - loss: 4.8233 - acc: 0.01 - ETA: 37s - loss: 4.8233 - acc: 0.01 - ETA: 36s - loss: 4.8231 - acc: 0.01 - ETA: 36s - loss: 4.8226 - acc: 0.01 - ETA: 35s - loss: 4.8217 - acc: 0.01 - ETA: 34s - loss: 4.8219 - acc: 0.01 - ETA: 34s - loss: 4.8223 - acc: 0.01 - ETA: 33s - loss: 4.8221 - acc: 0.01 - ETA: 32s - loss: 4.8220 - acc: 0.01 - ETA: 32s - loss: 4.8219 - acc: 0.01 - ETA: 31s - loss: 4.8221 - acc: 0.01 - ETA: 30s - loss: 4.8219 - acc: 0.01 - ETA: 29s - loss: 4.8220 - acc: 0.01 - ETA: 29s - loss: 4.8227 - acc: 0.01 - ETA: 28s - loss: 4.8230 - acc: 0.01 - ETA: 27s - loss: 4.8234 - acc: 0.01 - ETA: 27s - loss: 4.8232 - acc: 0.01 - ETA: 26s - loss: 4.8231 - acc: 0.01 - ETA: 25s - loss: 4.8231 - acc: 0.01 - ETA: 25s - loss: 4.8229 - acc: 0.01 - ETA: 24s - loss: 4.8228 - acc: 0.01 - ETA: 23s - loss: 4.8223 - acc: 0.01 - ETA: 22s - loss: 4.8226 - acc: 0.01 - ETA: 22s - loss: 4.8224 - acc: 0.01 - ETA: 21s - loss: 4.8222 - acc: 0.01 - ETA: 20s - loss: 4.8221 - acc: 0.01 - ETA: 20s - loss: 4.8221 - acc: 0.01 - ETA: 19s - loss: 4.8221 - acc: 0.01 - ETA: 18s - loss: 4.8220 - acc: 0.01 - ETA: 18s - loss: 4.8217 - acc: 0.01 - ETA: 17s - loss: 4.8215 - acc: 0.01 - ETA: 16s - loss: 4.8213 - acc: 0.01 - ETA: 15s - loss: 4.8212 - acc: 0.01 - ETA: 15s - loss: 4.8213 - acc: 0.01 - ETA: 14s - loss: 4.8214 - acc: 0.01 - ETA: 13s - loss: 4.8213 - acc: 0.01 - ETA: 13s - loss: 4.8212 - acc: 0.01 - ETA: 12s - loss: 4.8211 - acc: 0.01 - ETA: 11s - loss: 4.8212 - acc: 0.01 - ETA: 11s - loss: 4.8214 - acc: 0.01 - ETA: 10s - loss: 4.8218 - acc: 0.01 - ETA: 9s - loss: 4.8213 - acc: 0.0141 - ETA: 9s - loss: 4.8208 - acc: 0.014 - ETA: 8s - loss: 4.8209 - acc: 0.014 - ETA: 7s - loss: 4.8211 - acc: 0.014 - ETA: 6s - loss: 4.8207 - acc: 0.014 - ETA: 6s - loss: 4.8207 - acc: 0.014 - ETA: 5s - loss: 4.8208 - acc: 0.014 - ETA: 4s - loss: 4.8205 - acc: 0.013 - ETA: 4s - loss: 4.8203 - acc: 0.014 - ETA: 3s - loss: 4.8204 - acc: 0.014 - ETA: 2s - loss: 4.8202 - acc: 0.014 - ETA: 2s - loss: 4.8201 - acc: 0.014 - ETA: 1s - loss: 4.8198 - acc: 0.014 - ETA: 0s - loss: 4.8206 - acc: 0.0141Epoch 00003: val_loss improved from 4.84306 to 4.79927, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 243s 36ms/step - loss: 4.8201 - acc: 0.0142 - val_loss: 4.7993 - val_acc: 0.0251\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/6680 [=================>............] - ETA: 3:30 - loss: 4.8478 - acc: 0.0000e+0 - ETA: 3:35 - loss: 4.8674 - acc: 0.0000e+0 - ETA: 3:38 - loss: 4.8721 - acc: 0.0000e+0 - ETA: 3:37 - loss: 4.8431 - acc: 0.0000e+0 - ETA: 3:36 - loss: 4.8348 - acc: 0.0000e+0 - ETA: 3:34 - loss: 4.8236 - acc: 0.0000e+0 - ETA: 3:34 - loss: 4.8221 - acc: 0.0071    - ETA: 3:33 - loss: 4.8023 - acc: 0.018 - ETA: 3:33 - loss: 4.7948 - acc: 0.022 - ETA: 3:32 - loss: 4.8012 - acc: 0.025 - ETA: 3:32 - loss: 4.7935 - acc: 0.022 - ETA: 3:32 - loss: 4.7860 - acc: 0.020 - ETA: 3:31 - loss: 4.7886 - acc: 0.023 - ETA: 3:30 - loss: 4.7882 - acc: 0.021 - ETA: 3:30 - loss: 4.7901 - acc: 0.020 - ETA: 3:29 - loss: 4.8025 - acc: 0.018 - ETA: 3:28 - loss: 4.7998 - acc: 0.020 - ETA: 3:27 - loss: 4.7970 - acc: 0.019 - ETA: 3:27 - loss: 4.7955 - acc: 0.021 - ETA: 3:26 - loss: 4.7896 - acc: 0.022 - ETA: 3:25 - loss: 4.7824 - acc: 0.026 - ETA: 3:25 - loss: 4.7860 - acc: 0.025 - ETA: 3:24 - loss: 4.7803 - acc: 0.023 - ETA: 3:24 - loss: 4.7828 - acc: 0.022 - ETA: 3:24 - loss: 4.7800 - acc: 0.022 - ETA: 3:24 - loss: 4.7764 - acc: 0.023 - ETA: 3:23 - loss: 4.7763 - acc: 0.022 - ETA: 3:22 - loss: 4.7699 - acc: 0.021 - ETA: 3:23 - loss: 4.7726 - acc: 0.020 - ETA: 3:24 - loss: 4.7704 - acc: 0.020 - ETA: 3:24 - loss: 4.7763 - acc: 0.019 - ETA: 3:24 - loss: 4.7770 - acc: 0.020 - ETA: 3:23 - loss: 4.7748 - acc: 0.019 - ETA: 3:22 - loss: 4.7716 - acc: 0.019 - ETA: 3:21 - loss: 4.7725 - acc: 0.020 - ETA: 3:21 - loss: 4.7736 - acc: 0.019 - ETA: 3:20 - loss: 4.7725 - acc: 0.021 - ETA: 3:20 - loss: 4.7724 - acc: 0.021 - ETA: 3:19 - loss: 4.7687 - acc: 0.021 - ETA: 3:19 - loss: 4.7788 - acc: 0.021 - ETA: 3:18 - loss: 4.7789 - acc: 0.020 - ETA: 3:17 - loss: 4.7781 - acc: 0.020 - ETA: 3:17 - loss: 4.7762 - acc: 0.020 - ETA: 3:16 - loss: 4.7785 - acc: 0.020 - ETA: 3:15 - loss: 4.7767 - acc: 0.020 - ETA: 3:14 - loss: 4.7792 - acc: 0.019 - ETA: 3:14 - loss: 4.7800 - acc: 0.019 - ETA: 3:14 - loss: 4.7840 - acc: 0.018 - ETA: 3:13 - loss: 4.7833 - acc: 0.020 - ETA: 3:13 - loss: 4.7826 - acc: 0.020 - ETA: 3:12 - loss: 4.7803 - acc: 0.020 - ETA: 3:12 - loss: 4.7802 - acc: 0.020 - ETA: 3:11 - loss: 4.7806 - acc: 0.019 - ETA: 3:10 - loss: 4.7842 - acc: 0.019 - ETA: 3:09 - loss: 4.7831 - acc: 0.019 - ETA: 3:09 - loss: 4.7847 - acc: 0.018 - ETA: 3:08 - loss: 4.7838 - acc: 0.018 - ETA: 3:07 - loss: 4.7836 - acc: 0.018 - ETA: 3:07 - loss: 4.7863 - acc: 0.018 - ETA: 3:06 - loss: 4.7886 - acc: 0.018 - ETA: 3:06 - loss: 4.7892 - acc: 0.018 - ETA: 3:05 - loss: 4.7896 - acc: 0.017 - ETA: 3:04 - loss: 4.7877 - acc: 0.018 - ETA: 3:04 - loss: 4.7895 - acc: 0.018 - ETA: 3:03 - loss: 4.7888 - acc: 0.017 - ETA: 3:02 - loss: 4.7885 - acc: 0.017 - ETA: 3:01 - loss: 4.7874 - acc: 0.017 - ETA: 3:00 - loss: 4.7888 - acc: 0.016 - ETA: 3:00 - loss: 4.7888 - acc: 0.016 - ETA: 2:59 - loss: 4.7890 - acc: 0.016 - ETA: 2:59 - loss: 4.7905 - acc: 0.017 - ETA: 2:58 - loss: 4.7892 - acc: 0.018 - ETA: 2:57 - loss: 4.7888 - acc: 0.019 - ETA: 2:56 - loss: 4.7877 - acc: 0.019 - ETA: 2:56 - loss: 4.7887 - acc: 0.019 - ETA: 2:55 - loss: 4.7900 - acc: 0.019 - ETA: 2:55 - loss: 4.7897 - acc: 0.018 - ETA: 2:54 - loss: 4.7898 - acc: 0.018 - ETA: 2:53 - loss: 4.7921 - acc: 0.018 - ETA: 2:53 - loss: 4.7924 - acc: 0.018 - ETA: 2:52 - loss: 4.7905 - acc: 0.018 - ETA: 2:51 - loss: 4.7900 - acc: 0.018 - ETA: 2:50 - loss: 4.7910 - acc: 0.018 - ETA: 2:50 - loss: 4.7905 - acc: 0.017 - ETA: 2:49 - loss: 4.7907 - acc: 0.017 - ETA: 2:48 - loss: 4.7905 - acc: 0.017 - ETA: 2:48 - loss: 4.7897 - acc: 0.017 - ETA: 2:47 - loss: 4.7885 - acc: 0.018 - ETA: 2:46 - loss: 4.7897 - acc: 0.018 - ETA: 2:45 - loss: 4.7893 - acc: 0.017 - ETA: 2:45 - loss: 4.7887 - acc: 0.018 - ETA: 2:44 - loss: 4.7867 - acc: 0.018 - ETA: 2:43 - loss: 4.7878 - acc: 0.018 - ETA: 2:42 - loss: 4.7862 - acc: 0.019 - ETA: 2:42 - loss: 4.7863 - acc: 0.018 - ETA: 2:41 - loss: 4.7883 - acc: 0.018 - ETA: 2:40 - loss: 4.7875 - acc: 0.018 - ETA: 2:40 - loss: 4.7876 - acc: 0.018 - ETA: 2:39 - loss: 4.7875 - acc: 0.018 - ETA: 2:38 - loss: 4.7854 - acc: 0.019 - ETA: 2:38 - loss: 4.7841 - acc: 0.019 - ETA: 2:37 - loss: 4.7848 - acc: 0.020 - ETA: 2:36 - loss: 4.7831 - acc: 0.019 - ETA: 2:36 - loss: 4.7823 - acc: 0.020 - ETA: 2:35 - loss: 4.7835 - acc: 0.020 - ETA: 2:34 - loss: 4.7841 - acc: 0.019 - ETA: 2:33 - loss: 4.7848 - acc: 0.019 - ETA: 2:33 - loss: 4.7845 - acc: 0.020 - ETA: 2:32 - loss: 4.7852 - acc: 0.020 - ETA: 2:31 - loss: 4.7845 - acc: 0.020 - ETA: 2:30 - loss: 4.7850 - acc: 0.019 - ETA: 2:30 - loss: 4.7850 - acc: 0.019 - ETA: 2:29 - loss: 4.7843 - acc: 0.019 - ETA: 2:28 - loss: 4.7849 - acc: 0.019 - ETA: 2:28 - loss: 4.7860 - acc: 0.019 - ETA: 2:27 - loss: 4.7866 - acc: 0.019 - ETA: 2:26 - loss: 4.7859 - acc: 0.019 - ETA: 2:26 - loss: 4.7854 - acc: 0.019 - ETA: 2:25 - loss: 4.7849 - acc: 0.019 - ETA: 2:24 - loss: 4.7851 - acc: 0.019 - ETA: 2:24 - loss: 4.7845 - acc: 0.019 - ETA: 2:23 - loss: 4.7845 - acc: 0.019 - ETA: 2:22 - loss: 4.7856 - acc: 0.019 - ETA: 2:21 - loss: 4.7848 - acc: 0.019 - ETA: 2:21 - loss: 4.7862 - acc: 0.018 - ETA: 2:20 - loss: 4.7861 - acc: 0.018 - ETA: 2:19 - loss: 4.7862 - acc: 0.018 - ETA: 2:19 - loss: 4.7852 - acc: 0.019 - ETA: 2:18 - loss: 4.7854 - acc: 0.019 - ETA: 2:17 - loss: 4.7849 - acc: 0.019 - ETA: 2:17 - loss: 4.7843 - acc: 0.019 - ETA: 2:16 - loss: 4.7833 - acc: 0.019 - ETA: 2:15 - loss: 4.7838 - acc: 0.019 - ETA: 2:15 - loss: 4.7845 - acc: 0.019 - ETA: 2:14 - loss: 4.7842 - acc: 0.019 - ETA: 2:13 - loss: 4.7839 - acc: 0.019 - ETA: 2:12 - loss: 4.7830 - acc: 0.019 - ETA: 2:12 - loss: 4.7835 - acc: 0.019 - ETA: 2:11 - loss: 4.7818 - acc: 0.019 - ETA: 2:11 - loss: 4.7820 - acc: 0.019 - ETA: 2:10 - loss: 4.7826 - acc: 0.019 - ETA: 2:09 - loss: 4.7822 - acc: 0.019 - ETA: 2:08 - loss: 4.7820 - acc: 0.019 - ETA: 2:08 - loss: 4.7827 - acc: 0.019 - ETA: 2:07 - loss: 4.7832 - acc: 0.019 - ETA: 2:06 - loss: 4.7834 - acc: 0.019 - ETA: 2:06 - loss: 4.7829 - acc: 0.019 - ETA: 2:05 - loss: 4.7834 - acc: 0.018 - ETA: 2:04 - loss: 4.7832 - acc: 0.019 - ETA: 2:04 - loss: 4.7829 - acc: 0.019 - ETA: 2:03 - loss: 4.7830 - acc: 0.019 - ETA: 2:02 - loss: 4.7830 - acc: 0.019 - ETA: 2:02 - loss: 4.7830 - acc: 0.019 - ETA: 2:01 - loss: 4.7825 - acc: 0.019 - ETA: 2:00 - loss: 4.7831 - acc: 0.019 - ETA: 2:00 - loss: 4.7833 - acc: 0.019 - ETA: 1:59 - loss: 4.7837 - acc: 0.019 - ETA: 1:58 - loss: 4.7828 - acc: 0.019 - ETA: 1:58 - loss: 4.7823 - acc: 0.019 - ETA: 1:57 - loss: 4.7813 - acc: 0.019 - ETA: 1:56 - loss: 4.7803 - acc: 0.019 - ETA: 1:56 - loss: 4.7809 - acc: 0.019 - ETA: 1:55 - loss: 4.7809 - acc: 0.019 - ETA: 1:54 - loss: 4.7806 - acc: 0.019 - ETA: 1:54 - loss: 4.7807 - acc: 0.019 - ETA: 1:53 - loss: 4.7810 - acc: 0.019 - ETA: 1:52 - loss: 4.7809 - acc: 0.019 - ETA: 1:52 - loss: 4.7803 - acc: 0.019 - ETA: 1:51 - loss: 4.7820 - acc: 0.019 - ETA: 1:50 - loss: 4.7832 - acc: 0.019 - ETA: 1:49 - loss: 4.7835 - acc: 0.019 - ETA: 1:49 - loss: 4.7844 - acc: 0.019 - ETA: 1:48 - loss: 4.7843 - acc: 0.019 - ETA: 1:47 - loss: 4.7837 - acc: 0.019 - ETA: 1:47 - loss: 4.7835 - acc: 0.019 - ETA: 1:46 - loss: 4.7829 - acc: 0.019 - ETA: 1:45 - loss: 4.7841 - acc: 0.018 - ETA: 1:45 - loss: 4.7838 - acc: 0.018 - ETA: 1:44 - loss: 4.7835 - acc: 0.018 - ETA: 1:43 - loss: 4.7825 - acc: 0.018 - ETA: 1:43 - loss: 4.7819 - acc: 0.018 - ETA: 1:42 - loss: 4.7797 - acc: 0.019 - ETA: 1:41 - loss: 4.7798 - acc: 0.018 - ETA: 1:41 - loss: 4.7804 - acc: 0.018 - ETA: 1:40 - loss: 4.7808 - acc: 0.018 - ETA: 1:39 - loss: 4.7819 - acc: 0.018 - ETA: 1:39 - loss: 4.7806 - acc: 0.018 - ETA: 1:38 - loss: 4.7811 - acc: 0.018 - ETA: 1:37 - loss: 4.7811 - acc: 0.018 - ETA: 1:37 - loss: 4.7810 - acc: 0.018 - ETA: 1:36 - loss: 4.7815 - acc: 0.018 - ETA: 1:35 - loss: 4.7816 - acc: 0.018 - ETA: 1:35 - loss: 4.7812 - acc: 0.018 - ETA: 1:34 - loss: 4.7811 - acc: 0.018 - ETA: 1:33 - loss: 4.7807 - acc: 0.017 - ETA: 1:33 - loss: 4.7804 - acc: 0.018 - ETA: 1:32 - loss: 4.7804 - acc: 0.018 - ETA: 1:31 - loss: 4.7801 - acc: 0.018 - ETA: 1:31 - loss: 4.7796 - acc: 0.018 - ETA: 1:30 - loss: 4.7800 - acc: 0.018 - ETA: 1:29 - loss: 4.7798 - acc: 0.018 - ETA: 1:29 - loss: 4.7801 - acc: 0.018 - ETA: 1:28 - loss: 4.7800 - acc: 0.018 - ETA: 1:27 - loss: 4.7802 - acc: 0.0184"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 1:27 - loss: 4.7798 - acc: 0.018 - ETA: 1:26 - loss: 4.7795 - acc: 0.018 - ETA: 1:25 - loss: 4.7795 - acc: 0.018 - ETA: 1:24 - loss: 4.7792 - acc: 0.018 - ETA: 1:24 - loss: 4.7790 - acc: 0.018 - ETA: 1:23 - loss: 4.7789 - acc: 0.018 - ETA: 1:22 - loss: 4.7790 - acc: 0.018 - ETA: 1:22 - loss: 4.7797 - acc: 0.018 - ETA: 1:21 - loss: 4.7791 - acc: 0.018 - ETA: 1:20 - loss: 4.7783 - acc: 0.018 - ETA: 1:20 - loss: 4.7791 - acc: 0.018 - ETA: 1:19 - loss: 4.7787 - acc: 0.018 - ETA: 1:18 - loss: 4.7788 - acc: 0.018 - ETA: 1:18 - loss: 4.7784 - acc: 0.018 - ETA: 1:17 - loss: 4.7783 - acc: 0.018 - ETA: 1:16 - loss: 4.7788 - acc: 0.018 - ETA: 1:16 - loss: 4.7790 - acc: 0.018 - ETA: 1:15 - loss: 4.7795 - acc: 0.018 - ETA: 1:14 - loss: 4.7788 - acc: 0.018 - ETA: 1:14 - loss: 4.7785 - acc: 0.018 - ETA: 1:13 - loss: 4.7782 - acc: 0.018 - ETA: 1:12 - loss: 4.7781 - acc: 0.018 - ETA: 1:12 - loss: 4.7774 - acc: 0.018 - ETA: 1:11 - loss: 4.7783 - acc: 0.018 - ETA: 1:10 - loss: 4.7783 - acc: 0.017 - ETA: 1:10 - loss: 4.7781 - acc: 0.017 - ETA: 1:09 - loss: 4.7781 - acc: 0.018 - ETA: 1:08 - loss: 4.7781 - acc: 0.017 - ETA: 1:08 - loss: 4.7778 - acc: 0.018 - ETA: 1:07 - loss: 4.7778 - acc: 0.017 - ETA: 1:06 - loss: 4.7785 - acc: 0.017 - ETA: 1:06 - loss: 4.7777 - acc: 0.017 - ETA: 1:05 - loss: 4.7779 - acc: 0.017 - ETA: 1:04 - loss: 4.7776 - acc: 0.017 - ETA: 1:04 - loss: 4.7774 - acc: 0.017 - ETA: 1:03 - loss: 4.7774 - acc: 0.017 - ETA: 1:02 - loss: 4.7769 - acc: 0.017 - ETA: 1:02 - loss: 4.7764 - acc: 0.017 - ETA: 1:01 - loss: 4.7762 - acc: 0.017 - ETA: 1:00 - loss: 4.7757 - acc: 0.017 - ETA: 1:00 - loss: 4.7753 - acc: 0.017 - ETA: 59s - loss: 4.7752 - acc: 0.017 - ETA: 58s - loss: 4.7753 - acc: 0.01 - ETA: 58s - loss: 4.7758 - acc: 0.01 - ETA: 57s - loss: 4.7761 - acc: 0.01 - ETA: 56s - loss: 4.7760 - acc: 0.01 - ETA: 56s - loss: 4.7760 - acc: 0.01 - ETA: 55s - loss: 4.7756 - acc: 0.01 - ETA: 54s - loss: 4.7761 - acc: 0.01 - ETA: 54s - loss: 4.7754 - acc: 0.01 - ETA: 53s - loss: 4.7744 - acc: 0.01 - ETA: 52s - loss: 4.7741 - acc: 0.01 - ETA: 51s - loss: 4.7733 - acc: 0.01 - ETA: 51s - loss: 4.7749 - acc: 0.01 - ETA: 50s - loss: 4.7746 - acc: 0.01 - ETA: 49s - loss: 4.7750 - acc: 0.01 - ETA: 49s - loss: 4.7749 - acc: 0.01 - ETA: 48s - loss: 4.7747 - acc: 0.01 - ETA: 47s - loss: 4.7741 - acc: 0.01 - ETA: 47s - loss: 4.7744 - acc: 0.01 - ETA: 46s - loss: 4.7741 - acc: 0.01 - ETA: 45s - loss: 4.7739 - acc: 0.01 - ETA: 45s - loss: 4.7731 - acc: 0.01 - ETA: 44s - loss: 4.7722 - acc: 0.01 - ETA: 43s - loss: 4.7727 - acc: 0.01 - ETA: 43s - loss: 4.7731 - acc: 0.01 - ETA: 42s - loss: 4.7723 - acc: 0.01 - ETA: 41s - loss: 4.7724 - acc: 0.01 - ETA: 41s - loss: 4.7726 - acc: 0.01 - ETA: 40s - loss: 4.7727 - acc: 0.01 - ETA: 39s - loss: 4.7726 - acc: 0.01 - ETA: 39s - loss: 4.7727 - acc: 0.01 - ETA: 38s - loss: 4.7723 - acc: 0.01 - ETA: 37s - loss: 4.7727 - acc: 0.01 - ETA: 37s - loss: 4.7727 - acc: 0.01 - ETA: 36s - loss: 4.7727 - acc: 0.01 - ETA: 35s - loss: 4.7730 - acc: 0.01 - ETA: 35s - loss: 4.7733 - acc: 0.01 - ETA: 34s - loss: 4.7731 - acc: 0.01 - ETA: 33s - loss: 4.7747 - acc: 0.01 - ETA: 33s - loss: 4.7747 - acc: 0.01 - ETA: 32s - loss: 4.7746 - acc: 0.01 - ETA: 31s - loss: 4.7745 - acc: 0.01 - ETA: 31s - loss: 4.7746 - acc: 0.01 - ETA: 30s - loss: 4.7742 - acc: 0.01 - ETA: 29s - loss: 4.7743 - acc: 0.01 - ETA: 29s - loss: 4.7735 - acc: 0.01 - ETA: 28s - loss: 4.7735 - acc: 0.01 - ETA: 27s - loss: 4.7741 - acc: 0.01 - ETA: 26s - loss: 4.7735 - acc: 0.01 - ETA: 26s - loss: 4.7733 - acc: 0.01 - ETA: 25s - loss: 4.7737 - acc: 0.01 - ETA: 24s - loss: 4.7733 - acc: 0.01 - ETA: 24s - loss: 4.7735 - acc: 0.01 - ETA: 23s - loss: 4.7727 - acc: 0.01 - ETA: 22s - loss: 4.7727 - acc: 0.01 - ETA: 22s - loss: 4.7723 - acc: 0.01 - ETA: 21s - loss: 4.7725 - acc: 0.01 - ETA: 20s - loss: 4.7729 - acc: 0.01 - ETA: 20s - loss: 4.7726 - acc: 0.01 - ETA: 19s - loss: 4.7724 - acc: 0.01 - ETA: 18s - loss: 4.7726 - acc: 0.01 - ETA: 18s - loss: 4.7725 - acc: 0.01 - ETA: 17s - loss: 4.7729 - acc: 0.01 - ETA: 16s - loss: 4.7734 - acc: 0.01 - ETA: 16s - loss: 4.7729 - acc: 0.01 - ETA: 15s - loss: 4.7727 - acc: 0.01 - ETA: 14s - loss: 4.7725 - acc: 0.01 - ETA: 14s - loss: 4.7722 - acc: 0.01 - ETA: 13s - loss: 4.7722 - acc: 0.01 - ETA: 12s - loss: 4.7722 - acc: 0.01 - ETA: 12s - loss: 4.7721 - acc: 0.01 - ETA: 11s - loss: 4.7712 - acc: 0.01 - ETA: 10s - loss: 4.7712 - acc: 0.01 - ETA: 10s - loss: 4.7713 - acc: 0.01 - ETA: 9s - loss: 4.7709 - acc: 0.0191 - ETA: 8s - loss: 4.7707 - acc: 0.019 - ETA: 8s - loss: 4.7702 - acc: 0.019 - ETA: 7s - loss: 4.7709 - acc: 0.019 - ETA: 6s - loss: 4.7717 - acc: 0.019 - ETA: 6s - loss: 4.7713 - acc: 0.019 - ETA: 5s - loss: 4.7713 - acc: 0.019 - ETA: 4s - loss: 4.7713 - acc: 0.019 - ETA: 4s - loss: 4.7703 - acc: 0.019 - ETA: 3s - loss: 4.7706 - acc: 0.019 - ETA: 2s - loss: 4.7713 - acc: 0.019 - ETA: 2s - loss: 4.7715 - acc: 0.019 - ETA: 1s - loss: 4.7709 - acc: 0.019 - ETA: 0s - loss: 4.7707 - acc: 0.0192Epoch 00004: val_loss improved from 4.79927 to 4.78660, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 238s 36ms/step - loss: 4.7706 - acc: 0.0192 - val_loss: 4.7866 - val_acc: 0.0204\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/6680 [=================>............] - ETA: 3:40 - loss: 4.8157 - acc: 0.050 - ETA: 3:40 - loss: 4.7424 - acc: 0.050 - ETA: 3:39 - loss: 4.7431 - acc: 0.033 - ETA: 3:38 - loss: 4.7633 - acc: 0.025 - ETA: 3:38 - loss: 4.7558 - acc: 0.020 - ETA: 3:37 - loss: 4.7376 - acc: 0.025 - ETA: 3:35 - loss: 4.7454 - acc: 0.021 - ETA: 3:34 - loss: 4.7338 - acc: 0.025 - ETA: 3:33 - loss: 4.7362 - acc: 0.033 - ETA: 3:32 - loss: 4.7347 - acc: 0.035 - ETA: 3:32 - loss: 4.7235 - acc: 0.031 - ETA: 3:31 - loss: 4.7087 - acc: 0.033 - ETA: 3:30 - loss: 4.7103 - acc: 0.034 - ETA: 3:29 - loss: 4.7112 - acc: 0.035 - ETA: 3:29 - loss: 4.7017 - acc: 0.040 - ETA: 3:28 - loss: 4.7017 - acc: 0.043 - ETA: 3:28 - loss: 4.7097 - acc: 0.041 - ETA: 3:28 - loss: 4.7251 - acc: 0.038 - ETA: 3:28 - loss: 4.7312 - acc: 0.036 - ETA: 3:27 - loss: 4.7346 - acc: 0.035 - ETA: 3:26 - loss: 4.7340 - acc: 0.035 - ETA: 3:26 - loss: 4.7278 - acc: 0.034 - ETA: 3:26 - loss: 4.7194 - acc: 0.034 - ETA: 3:26 - loss: 4.7330 - acc: 0.033 - ETA: 3:26 - loss: 4.7340 - acc: 0.032 - ETA: 3:25 - loss: 4.7294 - acc: 0.032 - ETA: 3:24 - loss: 4.7282 - acc: 0.031 - ETA: 3:23 - loss: 4.7267 - acc: 0.030 - ETA: 3:22 - loss: 4.7209 - acc: 0.029 - ETA: 3:22 - loss: 4.7239 - acc: 0.030 - ETA: 3:21 - loss: 4.7271 - acc: 0.030 - ETA: 3:21 - loss: 4.7223 - acc: 0.031 - ETA: 3:21 - loss: 4.7181 - acc: 0.031 - ETA: 3:20 - loss: 4.7167 - acc: 0.030 - ETA: 3:19 - loss: 4.7245 - acc: 0.030 - ETA: 3:19 - loss: 4.7188 - acc: 0.029 - ETA: 3:18 - loss: 4.7176 - acc: 0.032 - ETA: 3:17 - loss: 4.7179 - acc: 0.031 - ETA: 3:16 - loss: 4.7162 - acc: 0.030 - ETA: 3:16 - loss: 4.7170 - acc: 0.030 - ETA: 3:15 - loss: 4.7146 - acc: 0.029 - ETA: 3:14 - loss: 4.7151 - acc: 0.028 - ETA: 3:13 - loss: 4.7141 - acc: 0.027 - ETA: 3:13 - loss: 4.7191 - acc: 0.028 - ETA: 3:12 - loss: 4.7204 - acc: 0.027 - ETA: 3:11 - loss: 4.7179 - acc: 0.027 - ETA: 3:10 - loss: 4.7212 - acc: 0.026 - ETA: 3:10 - loss: 4.7184 - acc: 0.026 - ETA: 3:09 - loss: 4.7256 - acc: 0.025 - ETA: 3:08 - loss: 4.7272 - acc: 0.026 - ETA: 3:08 - loss: 4.7277 - acc: 0.026 - ETA: 3:07 - loss: 4.7272 - acc: 0.026 - ETA: 3:07 - loss: 4.7270 - acc: 0.025 - ETA: 3:06 - loss: 4.7312 - acc: 0.025 - ETA: 3:05 - loss: 4.7288 - acc: 0.025 - ETA: 3:05 - loss: 4.7300 - acc: 0.025 - ETA: 3:04 - loss: 4.7317 - acc: 0.024 - ETA: 3:04 - loss: 4.7291 - acc: 0.025 - ETA: 3:03 - loss: 4.7278 - acc: 0.025 - ETA: 3:02 - loss: 4.7296 - acc: 0.025 - ETA: 3:01 - loss: 4.7315 - acc: 0.025 - ETA: 3:01 - loss: 4.7332 - acc: 0.026 - ETA: 3:00 - loss: 4.7325 - acc: 0.027 - ETA: 2:59 - loss: 4.7312 - acc: 0.026 - ETA: 2:59 - loss: 4.7320 - acc: 0.026 - ETA: 3:00 - loss: 4.7348 - acc: 0.026 - ETA: 3:00 - loss: 4.7350 - acc: 0.026 - ETA: 2:59 - loss: 4.7358 - acc: 0.025 - ETA: 2:58 - loss: 4.7353 - acc: 0.025 - ETA: 2:58 - loss: 4.7338 - acc: 0.025 - ETA: 2:57 - loss: 4.7327 - acc: 0.025 - ETA: 2:57 - loss: 4.7346 - acc: 0.025 - ETA: 2:56 - loss: 4.7330 - acc: 0.025 - ETA: 2:56 - loss: 4.7325 - acc: 0.026 - ETA: 2:55 - loss: 4.7342 - acc: 0.026 - ETA: 2:55 - loss: 4.7328 - acc: 0.027 - ETA: 2:54 - loss: 4.7351 - acc: 0.026 - ETA: 2:54 - loss: 4.7342 - acc: 0.026 - ETA: 2:54 - loss: 4.7342 - acc: 0.026 - ETA: 2:53 - loss: 4.7354 - acc: 0.026 - ETA: 2:52 - loss: 4.7343 - acc: 0.026 - ETA: 2:52 - loss: 4.7340 - acc: 0.026 - ETA: 2:51 - loss: 4.7348 - acc: 0.025 - ETA: 2:50 - loss: 4.7346 - acc: 0.025 - ETA: 2:49 - loss: 4.7357 - acc: 0.025 - ETA: 2:49 - loss: 4.7362 - acc: 0.025 - ETA: 2:48 - loss: 4.7353 - acc: 0.025 - ETA: 2:47 - loss: 4.7329 - acc: 0.027 - ETA: 2:47 - loss: 4.7341 - acc: 0.027 - ETA: 2:46 - loss: 4.7359 - acc: 0.027 - ETA: 2:45 - loss: 4.7372 - acc: 0.026 - ETA: 2:44 - loss: 4.7378 - acc: 0.026 - ETA: 2:44 - loss: 4.7365 - acc: 0.026 - ETA: 2:43 - loss: 4.7359 - acc: 0.026 - ETA: 2:42 - loss: 4.7384 - acc: 0.025 - ETA: 2:41 - loss: 4.7374 - acc: 0.025 - ETA: 2:41 - loss: 4.7363 - acc: 0.025 - ETA: 2:40 - loss: 4.7362 - acc: 0.025 - ETA: 2:39 - loss: 4.7375 - acc: 0.025 - ETA: 2:39 - loss: 4.7403 - acc: 0.025 - ETA: 2:38 - loss: 4.7389 - acc: 0.026 - ETA: 2:38 - loss: 4.7377 - acc: 0.026 - ETA: 2:37 - loss: 4.7366 - acc: 0.026 - ETA: 2:37 - loss: 4.7367 - acc: 0.026 - ETA: 2:36 - loss: 4.7358 - acc: 0.026 - ETA: 2:35 - loss: 4.7366 - acc: 0.025 - ETA: 2:35 - loss: 4.7364 - acc: 0.026 - ETA: 2:34 - loss: 4.7337 - acc: 0.026 - ETA: 2:34 - loss: 4.7333 - acc: 0.026 - ETA: 2:33 - loss: 4.7325 - acc: 0.025 - ETA: 2:32 - loss: 4.7326 - acc: 0.025 - ETA: 2:32 - loss: 4.7335 - acc: 0.025 - ETA: 2:31 - loss: 4.7343 - acc: 0.025 - ETA: 2:30 - loss: 4.7339 - acc: 0.025 - ETA: 2:30 - loss: 4.7336 - acc: 0.024 - ETA: 2:29 - loss: 4.7346 - acc: 0.024 - ETA: 2:28 - loss: 4.7351 - acc: 0.024 - ETA: 2:28 - loss: 4.7334 - acc: 0.025 - ETA: 2:27 - loss: 4.7341 - acc: 0.024 - ETA: 2:26 - loss: 4.7342 - acc: 0.024 - ETA: 2:26 - loss: 4.7325 - acc: 0.025 - ETA: 2:25 - loss: 4.7314 - acc: 0.025 - ETA: 2:24 - loss: 4.7317 - acc: 0.024 - ETA: 2:23 - loss: 4.7318 - acc: 0.024 - ETA: 2:23 - loss: 4.7326 - acc: 0.024 - ETA: 2:22 - loss: 4.7320 - acc: 0.025 - ETA: 2:21 - loss: 4.7331 - acc: 0.024 - ETA: 2:20 - loss: 4.7328 - acc: 0.024 - ETA: 2:20 - loss: 4.7307 - acc: 0.024 - ETA: 2:19 - loss: 4.7308 - acc: 0.024 - ETA: 2:18 - loss: 4.7300 - acc: 0.024 - ETA: 2:18 - loss: 4.7306 - acc: 0.024 - ETA: 2:17 - loss: 4.7304 - acc: 0.024 - ETA: 2:16 - loss: 4.7325 - acc: 0.023 - ETA: 2:16 - loss: 4.7317 - acc: 0.023 - ETA: 2:15 - loss: 4.7312 - acc: 0.023 - ETA: 2:14 - loss: 4.7327 - acc: 0.023 - ETA: 2:13 - loss: 4.7336 - acc: 0.023 - ETA: 2:13 - loss: 4.7352 - acc: 0.023 - ETA: 2:12 - loss: 4.7355 - acc: 0.022 - ETA: 2:11 - loss: 4.7359 - acc: 0.022 - ETA: 2:11 - loss: 4.7358 - acc: 0.022 - ETA: 2:10 - loss: 4.7351 - acc: 0.023 - ETA: 2:09 - loss: 4.7345 - acc: 0.022 - ETA: 2:09 - loss: 4.7346 - acc: 0.022 - ETA: 2:08 - loss: 4.7343 - acc: 0.022 - ETA: 2:07 - loss: 4.7338 - acc: 0.022 - ETA: 2:07 - loss: 4.7330 - acc: 0.022 - ETA: 2:06 - loss: 4.7329 - acc: 0.022 - ETA: 2:06 - loss: 4.7347 - acc: 0.022 - ETA: 2:05 - loss: 4.7345 - acc: 0.022 - ETA: 2:04 - loss: 4.7346 - acc: 0.022 - ETA: 2:03 - loss: 4.7349 - acc: 0.021 - ETA: 2:03 - loss: 4.7355 - acc: 0.021 - ETA: 2:02 - loss: 4.7355 - acc: 0.021 - ETA: 2:01 - loss: 4.7364 - acc: 0.021 - ETA: 2:01 - loss: 4.7365 - acc: 0.021 - ETA: 2:00 - loss: 4.7371 - acc: 0.021 - ETA: 1:59 - loss: 4.7359 - acc: 0.021 - ETA: 1:59 - loss: 4.7352 - acc: 0.022 - ETA: 1:58 - loss: 4.7360 - acc: 0.022 - ETA: 1:57 - loss: 4.7365 - acc: 0.021 - ETA: 1:56 - loss: 4.7358 - acc: 0.021 - ETA: 1:56 - loss: 4.7347 - acc: 0.021 - ETA: 1:55 - loss: 4.7346 - acc: 0.021 - ETA: 1:54 - loss: 4.7334 - acc: 0.021 - ETA: 1:54 - loss: 4.7340 - acc: 0.021 - ETA: 1:53 - loss: 4.7341 - acc: 0.021 - ETA: 1:52 - loss: 4.7350 - acc: 0.021 - ETA: 1:52 - loss: 4.7349 - acc: 0.021 - ETA: 1:51 - loss: 4.7343 - acc: 0.021 - ETA: 1:50 - loss: 4.7338 - acc: 0.021 - ETA: 1:50 - loss: 4.7336 - acc: 0.021 - ETA: 1:49 - loss: 4.7339 - acc: 0.021 - ETA: 1:48 - loss: 4.7339 - acc: 0.021 - ETA: 1:47 - loss: 4.7345 - acc: 0.021 - ETA: 1:47 - loss: 4.7349 - acc: 0.021 - ETA: 1:46 - loss: 4.7346 - acc: 0.021 - ETA: 1:45 - loss: 4.7346 - acc: 0.021 - ETA: 1:45 - loss: 4.7351 - acc: 0.021 - ETA: 1:44 - loss: 4.7353 - acc: 0.021 - ETA: 1:43 - loss: 4.7351 - acc: 0.022 - ETA: 1:43 - loss: 4.7348 - acc: 0.021 - ETA: 1:42 - loss: 4.7364 - acc: 0.021 - ETA: 1:41 - loss: 4.7371 - acc: 0.021 - ETA: 1:41 - loss: 4.7377 - acc: 0.021 - ETA: 1:40 - loss: 4.7373 - acc: 0.021 - ETA: 1:39 - loss: 4.7376 - acc: 0.021 - ETA: 1:38 - loss: 4.7379 - acc: 0.021 - ETA: 1:38 - loss: 4.7383 - acc: 0.021 - ETA: 1:37 - loss: 4.7385 - acc: 0.021 - ETA: 1:36 - loss: 4.7385 - acc: 0.021 - ETA: 1:36 - loss: 4.7394 - acc: 0.021 - ETA: 1:35 - loss: 4.7412 - acc: 0.021 - ETA: 1:34 - loss: 4.7413 - acc: 0.021 - ETA: 1:34 - loss: 4.7405 - acc: 0.021 - ETA: 1:33 - loss: 4.7408 - acc: 0.021 - ETA: 1:32 - loss: 4.7408 - acc: 0.021 - ETA: 1:32 - loss: 4.7407 - acc: 0.021 - ETA: 1:31 - loss: 4.7403 - acc: 0.021 - ETA: 1:30 - loss: 4.7411 - acc: 0.021 - ETA: 1:29 - loss: 4.7412 - acc: 0.021 - ETA: 1:29 - loss: 4.7410 - acc: 0.021 - ETA: 1:28 - loss: 4.7412 - acc: 0.0213"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 1:27 - loss: 4.7409 - acc: 0.021 - ETA: 1:27 - loss: 4.7411 - acc: 0.021 - ETA: 1:26 - loss: 4.7411 - acc: 0.021 - ETA: 1:25 - loss: 4.7418 - acc: 0.021 - ETA: 1:25 - loss: 4.7419 - acc: 0.021 - ETA: 1:24 - loss: 4.7419 - acc: 0.021 - ETA: 1:23 - loss: 4.7411 - acc: 0.021 - ETA: 1:23 - loss: 4.7410 - acc: 0.021 - ETA: 1:22 - loss: 4.7406 - acc: 0.021 - ETA: 1:21 - loss: 4.7416 - acc: 0.021 - ETA: 1:21 - loss: 4.7423 - acc: 0.021 - ETA: 1:20 - loss: 4.7422 - acc: 0.021 - ETA: 1:19 - loss: 4.7434 - acc: 0.021 - ETA: 1:19 - loss: 4.7430 - acc: 0.020 - ETA: 1:18 - loss: 4.7433 - acc: 0.020 - ETA: 1:17 - loss: 4.7444 - acc: 0.020 - ETA: 1:16 - loss: 4.7445 - acc: 0.020 - ETA: 1:16 - loss: 4.7448 - acc: 0.020 - ETA: 1:15 - loss: 4.7457 - acc: 0.020 - ETA: 1:14 - loss: 4.7461 - acc: 0.021 - ETA: 1:14 - loss: 4.7457 - acc: 0.020 - ETA: 1:13 - loss: 4.7458 - acc: 0.021 - ETA: 1:12 - loss: 4.7451 - acc: 0.020 - ETA: 1:12 - loss: 4.7449 - acc: 0.020 - ETA: 1:11 - loss: 4.7454 - acc: 0.020 - ETA: 1:10 - loss: 4.7457 - acc: 0.020 - ETA: 1:10 - loss: 4.7456 - acc: 0.020 - ETA: 1:09 - loss: 4.7453 - acc: 0.020 - ETA: 1:08 - loss: 4.7449 - acc: 0.020 - ETA: 1:08 - loss: 4.7449 - acc: 0.020 - ETA: 1:07 - loss: 4.7448 - acc: 0.020 - ETA: 1:06 - loss: 4.7447 - acc: 0.021 - ETA: 1:05 - loss: 4.7447 - acc: 0.021 - ETA: 1:05 - loss: 4.7446 - acc: 0.021 - ETA: 1:04 - loss: 4.7447 - acc: 0.021 - ETA: 1:03 - loss: 4.7441 - acc: 0.021 - ETA: 1:03 - loss: 4.7438 - acc: 0.021 - ETA: 1:02 - loss: 4.7431 - acc: 0.021 - ETA: 1:01 - loss: 4.7430 - acc: 0.021 - ETA: 1:01 - loss: 4.7432 - acc: 0.021 - ETA: 1:00 - loss: 4.7424 - acc: 0.021 - ETA: 59s - loss: 4.7425 - acc: 0.021 - ETA: 59s - loss: 4.7414 - acc: 0.02 - ETA: 58s - loss: 4.7409 - acc: 0.02 - ETA: 57s - loss: 4.7407 - acc: 0.02 - ETA: 57s - loss: 4.7400 - acc: 0.02 - ETA: 56s - loss: 4.7398 - acc: 0.02 - ETA: 55s - loss: 4.7400 - acc: 0.02 - ETA: 55s - loss: 4.7396 - acc: 0.02 - ETA: 54s - loss: 4.7396 - acc: 0.02 - ETA: 53s - loss: 4.7394 - acc: 0.02 - ETA: 52s - loss: 4.7398 - acc: 0.02 - ETA: 52s - loss: 4.7397 - acc: 0.02 - ETA: 51s - loss: 4.7390 - acc: 0.02 - ETA: 50s - loss: 4.7384 - acc: 0.02 - ETA: 50s - loss: 4.7385 - acc: 0.02 - ETA: 49s - loss: 4.7382 - acc: 0.02 - ETA: 48s - loss: 4.7374 - acc: 0.02 - ETA: 48s - loss: 4.7369 - acc: 0.02 - ETA: 47s - loss: 4.7374 - acc: 0.02 - ETA: 46s - loss: 4.7375 - acc: 0.02 - ETA: 46s - loss: 4.7374 - acc: 0.02 - ETA: 45s - loss: 4.7374 - acc: 0.02 - ETA: 44s - loss: 4.7369 - acc: 0.02 - ETA: 44s - loss: 4.7357 - acc: 0.02 - ETA: 43s - loss: 4.7361 - acc: 0.02 - ETA: 42s - loss: 4.7366 - acc: 0.02 - ETA: 42s - loss: 4.7368 - acc: 0.02 - ETA: 41s - loss: 4.7363 - acc: 0.02 - ETA: 40s - loss: 4.7359 - acc: 0.02 - ETA: 40s - loss: 4.7359 - acc: 0.02 - ETA: 39s - loss: 4.7361 - acc: 0.02 - ETA: 38s - loss: 4.7366 - acc: 0.02 - ETA: 38s - loss: 4.7372 - acc: 0.02 - ETA: 37s - loss: 4.7372 - acc: 0.02 - ETA: 36s - loss: 4.7370 - acc: 0.02 - ETA: 35s - loss: 4.7368 - acc: 0.02 - ETA: 35s - loss: 4.7366 - acc: 0.02 - ETA: 34s - loss: 4.7369 - acc: 0.02 - ETA: 33s - loss: 4.7372 - acc: 0.02 - ETA: 33s - loss: 4.7371 - acc: 0.02 - ETA: 32s - loss: 4.7371 - acc: 0.02 - ETA: 31s - loss: 4.7370 - acc: 0.02 - ETA: 31s - loss: 4.7372 - acc: 0.02 - ETA: 30s - loss: 4.7375 - acc: 0.02 - ETA: 29s - loss: 4.7376 - acc: 0.02 - ETA: 29s - loss: 4.7375 - acc: 0.02 - ETA: 28s - loss: 4.7374 - acc: 0.02 - ETA: 27s - loss: 4.7374 - acc: 0.02 - ETA: 27s - loss: 4.7367 - acc: 0.02 - ETA: 26s - loss: 4.7371 - acc: 0.02 - ETA: 25s - loss: 4.7363 - acc: 0.02 - ETA: 25s - loss: 4.7358 - acc: 0.02 - ETA: 24s - loss: 4.7361 - acc: 0.02 - ETA: 23s - loss: 4.7358 - acc: 0.02 - ETA: 23s - loss: 4.7360 - acc: 0.02 - ETA: 22s - loss: 4.7364 - acc: 0.02 - ETA: 21s - loss: 4.7365 - acc: 0.02 - ETA: 21s - loss: 4.7372 - acc: 0.02 - ETA: 20s - loss: 4.7372 - acc: 0.02 - ETA: 19s - loss: 4.7370 - acc: 0.02 - ETA: 19s - loss: 4.7369 - acc: 0.02 - ETA: 18s - loss: 4.7368 - acc: 0.02 - ETA: 17s - loss: 4.7362 - acc: 0.02 - ETA: 16s - loss: 4.7368 - acc: 0.02 - ETA: 16s - loss: 4.7359 - acc: 0.02 - ETA: 15s - loss: 4.7362 - acc: 0.02 - ETA: 14s - loss: 4.7358 - acc: 0.02 - ETA: 14s - loss: 4.7361 - acc: 0.02 - ETA: 13s - loss: 4.7361 - acc: 0.02 - ETA: 12s - loss: 4.7363 - acc: 0.02 - ETA: 12s - loss: 4.7363 - acc: 0.02 - ETA: 11s - loss: 4.7369 - acc: 0.02 - ETA: 10s - loss: 4.7369 - acc: 0.02 - ETA: 10s - loss: 4.7368 - acc: 0.02 - ETA: 9s - loss: 4.7364 - acc: 0.0222 - ETA: 8s - loss: 4.7362 - acc: 0.022 - ETA: 8s - loss: 4.7353 - acc: 0.022 - ETA: 7s - loss: 4.7346 - acc: 0.022 - ETA: 6s - loss: 4.7342 - acc: 0.022 - ETA: 6s - loss: 4.7348 - acc: 0.022 - ETA: 5s - loss: 4.7350 - acc: 0.022 - ETA: 4s - loss: 4.7354 - acc: 0.022 - ETA: 4s - loss: 4.7355 - acc: 0.022 - ETA: 3s - loss: 4.7354 - acc: 0.022 - ETA: 2s - loss: 4.7356 - acc: 0.022 - ETA: 2s - loss: 4.7361 - acc: 0.022 - ETA: 1s - loss: 4.7358 - acc: 0.022 - ETA: 0s - loss: 4.7364 - acc: 0.0224Epoch 00005: val_loss improved from 4.78660 to 4.75539, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 239s 36ms/step - loss: 4.7359 - acc: 0.0226 - val_loss: 4.7554 - val_acc: 0.0323\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/6680 [=================>............] - ETA: 3:46 - loss: 4.6964 - acc: 0.0000e+0 - ETA: 3:49 - loss: 4.6789 - acc: 0.0250    - ETA: 3:57 - loss: 4.6776 - acc: 0.050 - ETA: 3:57 - loss: 4.7270 - acc: 0.050 - ETA: 3:52 - loss: 4.7775 - acc: 0.040 - ETA: 3:53 - loss: 4.7363 - acc: 0.041 - ETA: 3:49 - loss: 4.7300 - acc: 0.035 - ETA: 3:46 - loss: 4.7237 - acc: 0.037 - ETA: 3:46 - loss: 4.7134 - acc: 0.033 - ETA: 3:44 - loss: 4.7214 - acc: 0.030 - ETA: 3:43 - loss: 4.7088 - acc: 0.031 - ETA: 3:42 - loss: 4.6955 - acc: 0.029 - ETA: 3:41 - loss: 4.7213 - acc: 0.026 - ETA: 3:40 - loss: 4.7307 - acc: 0.025 - ETA: 3:42 - loss: 4.7253 - acc: 0.026 - ETA: 3:46 - loss: 4.7239 - acc: 0.025 - ETA: 3:48 - loss: 4.7175 - acc: 0.023 - ETA: 3:46 - loss: 4.7222 - acc: 0.022 - ETA: 3:45 - loss: 4.7153 - acc: 0.021 - ETA: 3:46 - loss: 4.7220 - acc: 0.020 - ETA: 3:47 - loss: 4.7190 - acc: 0.021 - ETA: 3:47 - loss: 4.7158 - acc: 0.025 - ETA: 3:46 - loss: 4.7130 - acc: 0.026 - ETA: 3:46 - loss: 4.7091 - acc: 0.025 - ETA: 3:45 - loss: 4.7040 - acc: 0.024 - ETA: 3:44 - loss: 4.7051 - acc: 0.025 - ETA: 3:44 - loss: 4.7050 - acc: 0.024 - ETA: 3:42 - loss: 4.7017 - acc: 0.023 - ETA: 3:41 - loss: 4.6996 - acc: 0.022 - ETA: 3:39 - loss: 4.6978 - acc: 0.023 - ETA: 3:38 - loss: 4.6955 - acc: 0.027 - ETA: 3:37 - loss: 4.6955 - acc: 0.026 - ETA: 3:36 - loss: 4.6910 - acc: 0.025 - ETA: 3:35 - loss: 4.6859 - acc: 0.026 - ETA: 3:33 - loss: 4.6817 - acc: 0.025 - ETA: 3:32 - loss: 4.6820 - acc: 0.025 - ETA: 3:31 - loss: 4.6811 - acc: 0.025 - ETA: 3:30 - loss: 4.6826 - acc: 0.026 - ETA: 3:29 - loss: 4.6765 - acc: 0.029 - ETA: 3:28 - loss: 4.6796 - acc: 0.030 - ETA: 3:27 - loss: 4.6776 - acc: 0.029 - ETA: 3:27 - loss: 4.6803 - acc: 0.028 - ETA: 3:28 - loss: 4.6842 - acc: 0.027 - ETA: 3:28 - loss: 4.6818 - acc: 0.027 - ETA: 3:27 - loss: 4.6850 - acc: 0.026 - ETA: 3:26 - loss: 4.6859 - acc: 0.026 - ETA: 3:25 - loss: 4.6819 - acc: 0.027 - ETA: 3:24 - loss: 4.6809 - acc: 0.028 - ETA: 3:24 - loss: 4.6802 - acc: 0.027 - ETA: 3:24 - loss: 4.6784 - acc: 0.027 - ETA: 3:24 - loss: 4.6775 - acc: 0.027 - ETA: 3:23 - loss: 4.6777 - acc: 0.028 - ETA: 3:22 - loss: 4.6780 - acc: 0.030 - ETA: 3:21 - loss: 4.6786 - acc: 0.030 - ETA: 3:20 - loss: 4.6793 - acc: 0.030 - ETA: 3:19 - loss: 4.6796 - acc: 0.031 - ETA: 3:18 - loss: 4.6831 - acc: 0.030 - ETA: 3:17 - loss: 4.6825 - acc: 0.031 - ETA: 3:16 - loss: 4.6818 - acc: 0.031 - ETA: 3:15 - loss: 4.6866 - acc: 0.030 - ETA: 3:15 - loss: 4.6881 - acc: 0.030 - ETA: 3:14 - loss: 4.6904 - acc: 0.029 - ETA: 3:13 - loss: 4.6898 - acc: 0.029 - ETA: 3:13 - loss: 4.6902 - acc: 0.028 - ETA: 3:13 - loss: 4.6915 - acc: 0.028 - ETA: 3:13 - loss: 4.6877 - acc: 0.028 - ETA: 3:12 - loss: 4.6883 - acc: 0.029 - ETA: 3:12 - loss: 4.6905 - acc: 0.028 - ETA: 3:11 - loss: 4.6921 - acc: 0.028 - ETA: 3:10 - loss: 4.6915 - acc: 0.029 - ETA: 3:09 - loss: 4.6882 - acc: 0.029 - ETA: 3:09 - loss: 4.6891 - acc: 0.029 - ETA: 3:08 - loss: 4.6918 - acc: 0.030 - ETA: 3:07 - loss: 4.6924 - acc: 0.031 - ETA: 3:06 - loss: 4.6916 - acc: 0.032 - ETA: 3:05 - loss: 4.6926 - acc: 0.032 - ETA: 3:04 - loss: 4.6931 - acc: 0.031 - ETA: 3:03 - loss: 4.6964 - acc: 0.031 - ETA: 3:03 - loss: 4.6977 - acc: 0.031 - ETA: 3:02 - loss: 4.6943 - acc: 0.031 - ETA: 3:01 - loss: 4.6940 - acc: 0.032 - ETA: 3:00 - loss: 4.6982 - acc: 0.031 - ETA: 2:59 - loss: 4.6982 - acc: 0.031 - ETA: 2:58 - loss: 4.6994 - acc: 0.031 - ETA: 2:57 - loss: 4.7019 - acc: 0.030 - ETA: 2:57 - loss: 4.7015 - acc: 0.031 - ETA: 2:56 - loss: 4.7002 - acc: 0.031 - ETA: 2:55 - loss: 4.7015 - acc: 0.030 - ETA: 2:54 - loss: 4.7014 - acc: 0.030 - ETA: 2:53 - loss: 4.7017 - acc: 0.030 - ETA: 2:53 - loss: 4.7019 - acc: 0.029 - ETA: 2:52 - loss: 4.7019 - acc: 0.029 - ETA: 2:51 - loss: 4.7032 - acc: 0.029 - ETA: 2:50 - loss: 4.7028 - acc: 0.029 - ETA: 2:49 - loss: 4.7025 - acc: 0.028 - ETA: 2:48 - loss: 4.7019 - acc: 0.028 - ETA: 2:48 - loss: 4.7022 - acc: 0.028 - ETA: 2:47 - loss: 4.7040 - acc: 0.028 - ETA: 2:46 - loss: 4.7025 - acc: 0.028 - ETA: 2:45 - loss: 4.7042 - acc: 0.028 - ETA: 2:44 - loss: 4.7023 - acc: 0.028 - ETA: 2:44 - loss: 4.7035 - acc: 0.028 - ETA: 2:43 - loss: 4.7035 - acc: 0.028 - ETA: 2:42 - loss: 4.7043 - acc: 0.028 - ETA: 2:41 - loss: 4.7039 - acc: 0.028 - ETA: 2:40 - loss: 4.7029 - acc: 0.027 - ETA: 2:40 - loss: 4.7032 - acc: 0.027 - ETA: 2:39 - loss: 4.7031 - acc: 0.027 - ETA: 2:38 - loss: 4.7040 - acc: 0.028 - ETA: 2:37 - loss: 4.7044 - acc: 0.027 - ETA: 2:36 - loss: 4.7062 - acc: 0.027 - ETA: 2:36 - loss: 4.7053 - acc: 0.027 - ETA: 2:35 - loss: 4.7066 - acc: 0.027 - ETA: 2:34 - loss: 4.7085 - acc: 0.027 - ETA: 2:33 - loss: 4.7081 - acc: 0.027 - ETA: 2:33 - loss: 4.7092 - acc: 0.026 - ETA: 2:32 - loss: 4.7096 - acc: 0.026 - ETA: 2:31 - loss: 4.7098 - acc: 0.026 - ETA: 2:31 - loss: 4.7101 - acc: 0.026 - ETA: 2:30 - loss: 4.7116 - acc: 0.026 - ETA: 2:29 - loss: 4.7118 - acc: 0.026 - ETA: 2:28 - loss: 4.7132 - acc: 0.026 - ETA: 2:27 - loss: 4.7123 - acc: 0.026 - ETA: 2:27 - loss: 4.7140 - acc: 0.026 - ETA: 2:26 - loss: 4.7131 - acc: 0.026 - ETA: 2:25 - loss: 4.7135 - acc: 0.026 - ETA: 2:24 - loss: 4.7133 - acc: 0.026 - ETA: 2:24 - loss: 4.7122 - acc: 0.026 - ETA: 2:23 - loss: 4.7123 - acc: 0.026 - ETA: 2:22 - loss: 4.7121 - acc: 0.025 - ETA: 2:21 - loss: 4.7126 - acc: 0.025 - ETA: 2:21 - loss: 4.7124 - acc: 0.025 - ETA: 2:20 - loss: 4.7123 - acc: 0.025 - ETA: 2:19 - loss: 4.7138 - acc: 0.025 - ETA: 2:18 - loss: 4.7141 - acc: 0.024 - ETA: 2:18 - loss: 4.7138 - acc: 0.025 - ETA: 2:17 - loss: 4.7122 - acc: 0.025 - ETA: 2:16 - loss: 4.7116 - acc: 0.025 - ETA: 2:16 - loss: 4.7124 - acc: 0.025 - ETA: 2:15 - loss: 4.7102 - acc: 0.025 - ETA: 2:14 - loss: 4.7101 - acc: 0.024 - ETA: 2:13 - loss: 4.7117 - acc: 0.025 - ETA: 2:13 - loss: 4.7134 - acc: 0.024 - ETA: 2:12 - loss: 4.7131 - acc: 0.025 - ETA: 2:11 - loss: 4.7125 - acc: 0.025 - ETA: 2:11 - loss: 4.7116 - acc: 0.025 - ETA: 2:10 - loss: 4.7114 - acc: 0.025 - ETA: 2:09 - loss: 4.7109 - acc: 0.025 - ETA: 2:08 - loss: 4.7139 - acc: 0.025 - ETA: 2:08 - loss: 4.7134 - acc: 0.025 - ETA: 2:07 - loss: 4.7130 - acc: 0.025 - ETA: 2:06 - loss: 4.7120 - acc: 0.025 - ETA: 2:05 - loss: 4.7121 - acc: 0.025 - ETA: 2:05 - loss: 4.7127 - acc: 0.025 - ETA: 2:04 - loss: 4.7128 - acc: 0.024 - ETA: 2:03 - loss: 4.7130 - acc: 0.025 - ETA: 2:02 - loss: 4.7132 - acc: 0.024 - ETA: 2:02 - loss: 4.7140 - acc: 0.025 - ETA: 2:01 - loss: 4.7139 - acc: 0.025 - ETA: 2:00 - loss: 4.7134 - acc: 0.025 - ETA: 2:00 - loss: 4.7135 - acc: 0.025 - ETA: 1:59 - loss: 4.7128 - acc: 0.025 - ETA: 1:58 - loss: 4.7132 - acc: 0.025 - ETA: 1:57 - loss: 4.7150 - acc: 0.025 - ETA: 1:57 - loss: 4.7137 - acc: 0.025 - ETA: 1:56 - loss: 4.7125 - acc: 0.025 - ETA: 1:55 - loss: 4.7114 - acc: 0.025 - ETA: 1:55 - loss: 4.7106 - acc: 0.026 - ETA: 1:54 - loss: 4.7113 - acc: 0.026 - ETA: 1:53 - loss: 4.7095 - acc: 0.026 - ETA: 1:52 - loss: 4.7099 - acc: 0.026 - ETA: 1:52 - loss: 4.7104 - acc: 0.026 - ETA: 1:51 - loss: 4.7100 - acc: 0.026 - ETA: 1:50 - loss: 4.7084 - acc: 0.026 - ETA: 1:50 - loss: 4.7072 - acc: 0.026 - ETA: 1:49 - loss: 4.7066 - acc: 0.026 - ETA: 1:48 - loss: 4.7064 - acc: 0.026 - ETA: 1:47 - loss: 4.7067 - acc: 0.026 - ETA: 1:47 - loss: 4.7072 - acc: 0.026 - ETA: 1:46 - loss: 4.7071 - acc: 0.026 - ETA: 1:45 - loss: 4.7079 - acc: 0.026 - ETA: 1:45 - loss: 4.7083 - acc: 0.026 - ETA: 1:44 - loss: 4.7079 - acc: 0.026 - ETA: 1:43 - loss: 4.7073 - acc: 0.026 - ETA: 1:42 - loss: 4.7070 - acc: 0.026 - ETA: 1:42 - loss: 4.7086 - acc: 0.026 - ETA: 1:41 - loss: 4.7091 - acc: 0.026 - ETA: 1:40 - loss: 4.7096 - acc: 0.026 - ETA: 1:40 - loss: 4.7099 - acc: 0.025 - ETA: 1:39 - loss: 4.7094 - acc: 0.026 - ETA: 1:38 - loss: 4.7082 - acc: 0.026 - ETA: 1:37 - loss: 4.7084 - acc: 0.026 - ETA: 1:37 - loss: 4.7081 - acc: 0.025 - ETA: 1:36 - loss: 4.7075 - acc: 0.026 - ETA: 1:35 - loss: 4.7067 - acc: 0.026 - ETA: 1:35 - loss: 4.7073 - acc: 0.026 - ETA: 1:34 - loss: 4.7069 - acc: 0.026 - ETA: 1:33 - loss: 4.7077 - acc: 0.026 - ETA: 1:33 - loss: 4.7077 - acc: 0.026 - ETA: 1:32 - loss: 4.7088 - acc: 0.026 - ETA: 1:31 - loss: 4.7082 - acc: 0.026 - ETA: 1:30 - loss: 4.7068 - acc: 0.026 - ETA: 1:30 - loss: 4.7079 - acc: 0.026 - ETA: 1:29 - loss: 4.7089 - acc: 0.0260"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 1:28 - loss: 4.7093 - acc: 0.026 - ETA: 1:28 - loss: 4.7099 - acc: 0.026 - ETA: 1:27 - loss: 4.7101 - acc: 0.025 - ETA: 1:26 - loss: 4.7105 - acc: 0.025 - ETA: 1:26 - loss: 4.7104 - acc: 0.025 - ETA: 1:25 - loss: 4.7116 - acc: 0.025 - ETA: 1:24 - loss: 4.7115 - acc: 0.025 - ETA: 1:23 - loss: 4.7116 - acc: 0.025 - ETA: 1:23 - loss: 4.7123 - acc: 0.025 - ETA: 1:22 - loss: 4.7124 - acc: 0.025 - ETA: 1:21 - loss: 4.7115 - acc: 0.025 - ETA: 1:21 - loss: 4.7120 - acc: 0.025 - ETA: 1:20 - loss: 4.7116 - acc: 0.025 - ETA: 1:19 - loss: 4.7115 - acc: 0.025 - ETA: 1:19 - loss: 4.7105 - acc: 0.025 - ETA: 1:18 - loss: 4.7099 - acc: 0.025 - ETA: 1:17 - loss: 4.7105 - acc: 0.025 - ETA: 1:16 - loss: 4.7104 - acc: 0.025 - ETA: 1:16 - loss: 4.7103 - acc: 0.025 - ETA: 1:15 - loss: 4.7104 - acc: 0.025 - ETA: 1:14 - loss: 4.7095 - acc: 0.025 - ETA: 1:14 - loss: 4.7096 - acc: 0.025 - ETA: 1:13 - loss: 4.7103 - acc: 0.025 - ETA: 1:12 - loss: 4.7113 - acc: 0.025 - ETA: 1:12 - loss: 4.7106 - acc: 0.025 - ETA: 1:11 - loss: 4.7107 - acc: 0.025 - ETA: 1:10 - loss: 4.7098 - acc: 0.025 - ETA: 1:09 - loss: 4.7099 - acc: 0.025 - ETA: 1:09 - loss: 4.7107 - acc: 0.025 - ETA: 1:08 - loss: 4.7106 - acc: 0.025 - ETA: 1:07 - loss: 4.7113 - acc: 0.025 - ETA: 1:07 - loss: 4.7115 - acc: 0.025 - ETA: 1:06 - loss: 4.7109 - acc: 0.025 - ETA: 1:05 - loss: 4.7107 - acc: 0.025 - ETA: 1:05 - loss: 4.7110 - acc: 0.025 - ETA: 1:04 - loss: 4.7107 - acc: 0.025 - ETA: 1:03 - loss: 4.7098 - acc: 0.025 - ETA: 1:02 - loss: 4.7096 - acc: 0.025 - ETA: 1:02 - loss: 4.7094 - acc: 0.025 - ETA: 1:01 - loss: 4.7091 - acc: 0.025 - ETA: 1:00 - loss: 4.7081 - acc: 0.025 - ETA: 1:00 - loss: 4.7080 - acc: 0.025 - ETA: 59s - loss: 4.7080 - acc: 0.025 - ETA: 58s - loss: 4.7078 - acc: 0.02 - ETA: 58s - loss: 4.7074 - acc: 0.02 - ETA: 57s - loss: 4.7069 - acc: 0.02 - ETA: 56s - loss: 4.7061 - acc: 0.02 - ETA: 56s - loss: 4.7054 - acc: 0.02 - ETA: 55s - loss: 4.7050 - acc: 0.02 - ETA: 54s - loss: 4.7047 - acc: 0.02 - ETA: 54s - loss: 4.7047 - acc: 0.02 - ETA: 53s - loss: 4.7053 - acc: 0.02 - ETA: 52s - loss: 4.7050 - acc: 0.02 - ETA: 51s - loss: 4.7057 - acc: 0.02 - ETA: 51s - loss: 4.7062 - acc: 0.02 - ETA: 50s - loss: 4.7065 - acc: 0.02 - ETA: 49s - loss: 4.7061 - acc: 0.02 - ETA: 49s - loss: 4.7057 - acc: 0.02 - ETA: 48s - loss: 4.7059 - acc: 0.02 - ETA: 47s - loss: 4.7059 - acc: 0.02 - ETA: 47s - loss: 4.7049 - acc: 0.02 - ETA: 46s - loss: 4.7048 - acc: 0.02 - ETA: 45s - loss: 4.7049 - acc: 0.02 - ETA: 45s - loss: 4.7041 - acc: 0.02 - ETA: 44s - loss: 4.7037 - acc: 0.02 - ETA: 43s - loss: 4.7034 - acc: 0.02 - ETA: 43s - loss: 4.7043 - acc: 0.02 - ETA: 42s - loss: 4.7042 - acc: 0.02 - ETA: 41s - loss: 4.7039 - acc: 0.02 - ETA: 40s - loss: 4.7039 - acc: 0.02 - ETA: 40s - loss: 4.7043 - acc: 0.02 - ETA: 39s - loss: 4.7037 - acc: 0.02 - ETA: 38s - loss: 4.7039 - acc: 0.02 - ETA: 38s - loss: 4.7045 - acc: 0.02 - ETA: 37s - loss: 4.7043 - acc: 0.02 - ETA: 36s - loss: 4.7042 - acc: 0.02 - ETA: 36s - loss: 4.7044 - acc: 0.02 - ETA: 35s - loss: 4.7038 - acc: 0.02 - ETA: 34s - loss: 4.7039 - acc: 0.02 - ETA: 34s - loss: 4.7051 - acc: 0.02 - ETA: 33s - loss: 4.7045 - acc: 0.02 - ETA: 32s - loss: 4.7043 - acc: 0.02 - ETA: 32s - loss: 4.7050 - acc: 0.02 - ETA: 31s - loss: 4.7054 - acc: 0.02 - ETA: 30s - loss: 4.7057 - acc: 0.02 - ETA: 30s - loss: 4.7060 - acc: 0.02 - ETA: 29s - loss: 4.7063 - acc: 0.02 - ETA: 28s - loss: 4.7061 - acc: 0.02 - ETA: 27s - loss: 4.7059 - acc: 0.02 - ETA: 27s - loss: 4.7063 - acc: 0.02 - ETA: 26s - loss: 4.7063 - acc: 0.02 - ETA: 25s - loss: 4.7067 - acc: 0.02 - ETA: 25s - loss: 4.7077 - acc: 0.02 - ETA: 24s - loss: 4.7082 - acc: 0.02 - ETA: 23s - loss: 4.7079 - acc: 0.02 - ETA: 23s - loss: 4.7078 - acc: 0.02 - ETA: 22s - loss: 4.7076 - acc: 0.02 - ETA: 21s - loss: 4.7075 - acc: 0.02 - ETA: 21s - loss: 4.7083 - acc: 0.02 - ETA: 20s - loss: 4.7081 - acc: 0.02 - ETA: 19s - loss: 4.7081 - acc: 0.02 - ETA: 19s - loss: 4.7081 - acc: 0.02 - ETA: 18s - loss: 4.7085 - acc: 0.02 - ETA: 17s - loss: 4.7084 - acc: 0.02 - ETA: 17s - loss: 4.7086 - acc: 0.02 - ETA: 16s - loss: 4.7081 - acc: 0.02 - ETA: 15s - loss: 4.7086 - acc: 0.02 - ETA: 15s - loss: 4.7090 - acc: 0.02 - ETA: 14s - loss: 4.7090 - acc: 0.02 - ETA: 13s - loss: 4.7088 - acc: 0.02 - ETA: 13s - loss: 4.7082 - acc: 0.02 - ETA: 12s - loss: 4.7077 - acc: 0.02 - ETA: 11s - loss: 4.7080 - acc: 0.02 - ETA: 10s - loss: 4.7074 - acc: 0.02 - ETA: 10s - loss: 4.7078 - acc: 0.02 - ETA: 9s - loss: 4.7080 - acc: 0.0252 - ETA: 8s - loss: 4.7083 - acc: 0.025 - ETA: 8s - loss: 4.7094 - acc: 0.025 - ETA: 7s - loss: 4.7093 - acc: 0.025 - ETA: 6s - loss: 4.7090 - acc: 0.025 - ETA: 6s - loss: 4.7087 - acc: 0.024 - ETA: 5s - loss: 4.7080 - acc: 0.025 - ETA: 4s - loss: 4.7086 - acc: 0.025 - ETA: 4s - loss: 4.7090 - acc: 0.025 - ETA: 3s - loss: 4.7093 - acc: 0.025 - ETA: 2s - loss: 4.7088 - acc: 0.025 - ETA: 2s - loss: 4.7094 - acc: 0.025 - ETA: 1s - loss: 4.7091 - acc: 0.025 - ETA: 0s - loss: 4.7095 - acc: 0.0251Epoch 00006: val_loss improved from 4.75539 to 4.73650, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 239s 36ms/step - loss: 4.7088 - acc: 0.0251 - val_loss: 4.7365 - val_acc: 0.0251\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/6680 [=================>............] - ETA: 3:43 - loss: 4.7424 - acc: 0.0000e+0 - ETA: 3:46 - loss: 4.6567 - acc: 0.0000e+0 - ETA: 3:44 - loss: 4.5700 - acc: 0.0167    - ETA: 3:47 - loss: 4.6123 - acc: 0.025 - ETA: 4:06 - loss: 4.6232 - acc: 0.020 - ETA: 4:09 - loss: 4.6408 - acc: 0.025 - ETA: 4:06 - loss: 4.6691 - acc: 0.021 - ETA: 4:04 - loss: 4.6865 - acc: 0.018 - ETA: 4:00 - loss: 4.6883 - acc: 0.016 - ETA: 3:56 - loss: 4.6861 - acc: 0.020 - ETA: 3:54 - loss: 4.6850 - acc: 0.031 - ETA: 3:51 - loss: 4.6706 - acc: 0.029 - ETA: 3:50 - loss: 4.6704 - acc: 0.030 - ETA: 3:49 - loss: 4.6711 - acc: 0.028 - ETA: 3:51 - loss: 4.6802 - acc: 0.030 - ETA: 3:53 - loss: 4.6827 - acc: 0.034 - ETA: 3:52 - loss: 4.6859 - acc: 0.032 - ETA: 3:51 - loss: 4.6847 - acc: 0.033 - ETA: 3:50 - loss: 4.6813 - acc: 0.034 - ETA: 3:49 - loss: 4.6883 - acc: 0.035 - ETA: 3:48 - loss: 4.6932 - acc: 0.035 - ETA: 3:46 - loss: 4.6911 - acc: 0.036 - ETA: 3:46 - loss: 4.6927 - acc: 0.034 - ETA: 3:45 - loss: 4.6950 - acc: 0.033 - ETA: 3:44 - loss: 4.6877 - acc: 0.034 - ETA: 3:45 - loss: 4.6842 - acc: 0.034 - ETA: 3:46 - loss: 4.6794 - acc: 0.037 - ETA: 3:46 - loss: 4.6799 - acc: 0.037 - ETA: 3:45 - loss: 4.6754 - acc: 0.037 - ETA: 3:44 - loss: 4.6756 - acc: 0.040 - ETA: 3:43 - loss: 4.6691 - acc: 0.043 - ETA: 3:41 - loss: 4.6681 - acc: 0.045 - ETA: 3:40 - loss: 4.6699 - acc: 0.043 - ETA: 3:38 - loss: 4.6775 - acc: 0.042 - ETA: 3:37 - loss: 4.6772 - acc: 0.041 - ETA: 3:36 - loss: 4.6817 - acc: 0.040 - ETA: 3:35 - loss: 4.6761 - acc: 0.041 - ETA: 3:33 - loss: 4.6774 - acc: 0.040 - ETA: 3:32 - loss: 4.6772 - acc: 0.041 - ETA: 3:31 - loss: 4.6794 - acc: 0.040 - ETA: 3:30 - loss: 4.6798 - acc: 0.039 - ETA: 3:29 - loss: 4.6809 - acc: 0.038 - ETA: 3:28 - loss: 4.6806 - acc: 0.037 - ETA: 3:27 - loss: 4.6846 - acc: 0.036 - ETA: 3:26 - loss: 4.6860 - acc: 0.036 - ETA: 3:25 - loss: 4.6859 - acc: 0.035 - ETA: 3:25 - loss: 4.6835 - acc: 0.035 - ETA: 3:25 - loss: 4.6840 - acc: 0.035 - ETA: 3:25 - loss: 4.6868 - acc: 0.035 - ETA: 3:25 - loss: 4.6854 - acc: 0.035 - ETA: 3:24 - loss: 4.6888 - acc: 0.034 - ETA: 3:24 - loss: 4.6864 - acc: 0.033 - ETA: 3:23 - loss: 4.6869 - acc: 0.034 - ETA: 3:22 - loss: 4.6855 - acc: 0.034 - ETA: 3:22 - loss: 4.6914 - acc: 0.033 - ETA: 3:21 - loss: 4.6880 - acc: 0.033 - ETA: 3:20 - loss: 4.6901 - acc: 0.032 - ETA: 3:19 - loss: 4.6899 - acc: 0.031 - ETA: 3:18 - loss: 4.6899 - acc: 0.031 - ETA: 3:17 - loss: 4.6885 - acc: 0.032 - ETA: 3:16 - loss: 4.6891 - acc: 0.032 - ETA: 3:15 - loss: 4.6861 - acc: 0.033 - ETA: 3:14 - loss: 4.6871 - acc: 0.032 - ETA: 3:14 - loss: 4.6853 - acc: 0.032 - ETA: 3:13 - loss: 4.6871 - acc: 0.032 - ETA: 3:12 - loss: 4.6861 - acc: 0.031 - ETA: 3:11 - loss: 4.6859 - acc: 0.031 - ETA: 3:10 - loss: 4.6850 - acc: 0.030 - ETA: 3:09 - loss: 4.6796 - acc: 0.030 - ETA: 3:09 - loss: 4.6765 - acc: 0.030 - ETA: 3:08 - loss: 4.6761 - acc: 0.031 - ETA: 3:07 - loss: 4.6751 - acc: 0.031 - ETA: 3:06 - loss: 4.6752 - acc: 0.030 - ETA: 3:05 - loss: 4.6778 - acc: 0.030 - ETA: 3:04 - loss: 4.6779 - acc: 0.030 - ETA: 3:04 - loss: 4.6798 - acc: 0.031 - ETA: 3:03 - loss: 4.6790 - acc: 0.031 - ETA: 3:02 - loss: 4.6783 - acc: 0.030 - ETA: 3:01 - loss: 4.6767 - acc: 0.031 - ETA: 3:00 - loss: 4.6780 - acc: 0.030 - ETA: 2:59 - loss: 4.6778 - acc: 0.030 - ETA: 2:58 - loss: 4.6785 - acc: 0.030 - ETA: 2:58 - loss: 4.6781 - acc: 0.030 - ETA: 2:57 - loss: 4.6809 - acc: 0.031 - ETA: 2:56 - loss: 4.6817 - acc: 0.031 - ETA: 2:55 - loss: 4.6804 - acc: 0.031 - ETA: 2:54 - loss: 4.6799 - acc: 0.031 - ETA: 2:54 - loss: 4.6799 - acc: 0.031 - ETA: 2:53 - loss: 4.6789 - acc: 0.030 - ETA: 2:52 - loss: 4.6791 - acc: 0.030 - ETA: 2:51 - loss: 4.6761 - acc: 0.030 - ETA: 2:51 - loss: 4.6757 - acc: 0.029 - ETA: 2:50 - loss: 4.6766 - acc: 0.029 - ETA: 2:49 - loss: 4.6775 - acc: 0.029 - ETA: 2:48 - loss: 4.6787 - acc: 0.029 - ETA: 2:47 - loss: 4.6789 - acc: 0.029 - ETA: 2:47 - loss: 4.6782 - acc: 0.028 - ETA: 2:46 - loss: 4.6764 - acc: 0.029 - ETA: 2:45 - loss: 4.6743 - acc: 0.029 - ETA: 2:44 - loss: 4.6748 - acc: 0.029 - ETA: 2:44 - loss: 4.6743 - acc: 0.029 - ETA: 2:43 - loss: 4.6732 - acc: 0.028 - ETA: 2:42 - loss: 4.6720 - acc: 0.028 - ETA: 2:41 - loss: 4.6724 - acc: 0.028 - ETA: 2:40 - loss: 4.6709 - acc: 0.029 - ETA: 2:40 - loss: 4.6714 - acc: 0.029 - ETA: 2:39 - loss: 4.6737 - acc: 0.029 - ETA: 2:38 - loss: 4.6732 - acc: 0.029 - ETA: 2:37 - loss: 4.6726 - acc: 0.028 - ETA: 2:37 - loss: 4.6734 - acc: 0.029 - ETA: 2:36 - loss: 4.6749 - acc: 0.028 - ETA: 2:35 - loss: 4.6731 - acc: 0.029 - ETA: 2:34 - loss: 4.6710 - acc: 0.030 - ETA: 2:34 - loss: 4.6696 - acc: 0.030 - ETA: 2:33 - loss: 4.6701 - acc: 0.030 - ETA: 2:32 - loss: 4.6738 - acc: 0.030 - ETA: 2:31 - loss: 4.6731 - acc: 0.030 - ETA: 2:31 - loss: 4.6741 - acc: 0.030 - ETA: 2:30 - loss: 4.6727 - acc: 0.030 - ETA: 2:29 - loss: 4.6716 - acc: 0.030 - ETA: 2:28 - loss: 4.6716 - acc: 0.031 - ETA: 2:28 - loss: 4.6728 - acc: 0.030 - ETA: 2:27 - loss: 4.6715 - acc: 0.031 - ETA: 2:26 - loss: 4.6722 - acc: 0.031 - ETA: 2:25 - loss: 4.6733 - acc: 0.031 - ETA: 2:25 - loss: 4.6737 - acc: 0.031 - ETA: 2:24 - loss: 4.6744 - acc: 0.031 - ETA: 2:23 - loss: 4.6743 - acc: 0.030 - ETA: 2:23 - loss: 4.6740 - acc: 0.031 - ETA: 2:22 - loss: 4.6742 - acc: 0.031 - ETA: 2:21 - loss: 4.6781 - acc: 0.030 - ETA: 2:20 - loss: 4.6774 - acc: 0.030 - ETA: 2:20 - loss: 4.6769 - acc: 0.032 - ETA: 2:19 - loss: 4.6775 - acc: 0.032 - ETA: 2:19 - loss: 4.6799 - acc: 0.031 - ETA: 2:18 - loss: 4.6808 - acc: 0.031 - ETA: 2:17 - loss: 4.6812 - acc: 0.031 - ETA: 2:16 - loss: 4.6819 - acc: 0.031 - ETA: 2:16 - loss: 4.6834 - acc: 0.030 - ETA: 2:15 - loss: 4.6816 - acc: 0.031 - ETA: 2:14 - loss: 4.6806 - acc: 0.031 - ETA: 2:14 - loss: 4.6806 - acc: 0.031 - ETA: 2:13 - loss: 4.6806 - acc: 0.031 - ETA: 2:13 - loss: 4.6805 - acc: 0.031 - ETA: 2:12 - loss: 4.6816 - acc: 0.031 - ETA: 2:11 - loss: 4.6809 - acc: 0.031 - ETA: 2:11 - loss: 4.6804 - acc: 0.031 - ETA: 2:10 - loss: 4.6811 - acc: 0.031 - ETA: 2:10 - loss: 4.6810 - acc: 0.031 - ETA: 2:09 - loss: 4.6807 - acc: 0.031 - ETA: 2:08 - loss: 4.6794 - acc: 0.031 - ETA: 2:08 - loss: 4.6788 - acc: 0.031 - ETA: 2:07 - loss: 4.6791 - acc: 0.031 - ETA: 2:06 - loss: 4.6778 - acc: 0.032 - ETA: 2:06 - loss: 4.6774 - acc: 0.032 - ETA: 2:05 - loss: 4.6781 - acc: 0.032 - ETA: 2:04 - loss: 4.6793 - acc: 0.031 - ETA: 2:04 - loss: 4.6791 - acc: 0.031 - ETA: 2:03 - loss: 4.6790 - acc: 0.031 - ETA: 2:02 - loss: 4.6802 - acc: 0.031 - ETA: 2:01 - loss: 4.6804 - acc: 0.031 - ETA: 2:01 - loss: 4.6810 - acc: 0.030 - ETA: 2:00 - loss: 4.6826 - acc: 0.031 - ETA: 1:59 - loss: 4.6832 - acc: 0.030 - ETA: 1:59 - loss: 4.6821 - acc: 0.031 - ETA: 1:58 - loss: 4.6819 - acc: 0.031 - ETA: 1:57 - loss: 4.6811 - acc: 0.032 - ETA: 1:56 - loss: 4.6817 - acc: 0.032 - ETA: 1:56 - loss: 4.6819 - acc: 0.032 - ETA: 1:55 - loss: 4.6830 - acc: 0.032 - ETA: 1:54 - loss: 4.6834 - acc: 0.032 - ETA: 1:53 - loss: 4.6827 - acc: 0.032 - ETA: 1:53 - loss: 4.6823 - acc: 0.031 - ETA: 1:52 - loss: 4.6820 - acc: 0.032 - ETA: 1:51 - loss: 4.6834 - acc: 0.032 - ETA: 1:51 - loss: 4.6837 - acc: 0.031 - ETA: 1:50 - loss: 4.6835 - acc: 0.031 - ETA: 1:49 - loss: 4.6861 - acc: 0.031 - ETA: 1:49 - loss: 4.6869 - acc: 0.031 - ETA: 1:48 - loss: 4.6859 - acc: 0.031 - ETA: 1:47 - loss: 4.6848 - acc: 0.031 - ETA: 1:47 - loss: 4.6846 - acc: 0.031 - ETA: 1:46 - loss: 4.6841 - acc: 0.031 - ETA: 1:45 - loss: 4.6836 - acc: 0.031 - ETA: 1:44 - loss: 4.6832 - acc: 0.031 - ETA: 1:44 - loss: 4.6816 - acc: 0.031 - ETA: 1:43 - loss: 4.6818 - acc: 0.031 - ETA: 1:42 - loss: 4.6816 - acc: 0.031 - ETA: 1:42 - loss: 4.6805 - acc: 0.032 - ETA: 1:41 - loss: 4.6808 - acc: 0.032 - ETA: 1:40 - loss: 4.6804 - acc: 0.031 - ETA: 1:39 - loss: 4.6806 - acc: 0.032 - ETA: 1:39 - loss: 4.6801 - acc: 0.032 - ETA: 1:38 - loss: 4.6793 - acc: 0.032 - ETA: 1:37 - loss: 4.6786 - acc: 0.032 - ETA: 1:37 - loss: 4.6795 - acc: 0.032 - ETA: 1:36 - loss: 4.6800 - acc: 0.032 - ETA: 1:35 - loss: 4.6803 - acc: 0.032 - ETA: 1:34 - loss: 4.6813 - acc: 0.032 - ETA: 1:34 - loss: 4.6815 - acc: 0.032 - ETA: 1:33 - loss: 4.6811 - acc: 0.032 - ETA: 1:32 - loss: 4.6811 - acc: 0.032 - ETA: 1:32 - loss: 4.6812 - acc: 0.032 - ETA: 1:31 - loss: 4.6825 - acc: 0.0326"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 1:30 - loss: 4.6814 - acc: 0.032 - ETA: 1:30 - loss: 4.6817 - acc: 0.032 - ETA: 1:29 - loss: 4.6818 - acc: 0.032 - ETA: 1:28 - loss: 4.6819 - acc: 0.032 - ETA: 1:28 - loss: 4.6826 - acc: 0.032 - ETA: 1:27 - loss: 4.6810 - acc: 0.032 - ETA: 1:26 - loss: 4.6816 - acc: 0.032 - ETA: 1:25 - loss: 4.6826 - acc: 0.032 - ETA: 1:25 - loss: 4.6816 - acc: 0.032 - ETA: 1:24 - loss: 4.6822 - acc: 0.032 - ETA: 1:23 - loss: 4.6821 - acc: 0.032 - ETA: 1:23 - loss: 4.6824 - acc: 0.031 - ETA: 1:22 - loss: 4.6823 - acc: 0.031 - ETA: 1:21 - loss: 4.6817 - acc: 0.031 - ETA: 1:20 - loss: 4.6820 - acc: 0.031 - ETA: 1:20 - loss: 4.6806 - acc: 0.031 - ETA: 1:19 - loss: 4.6807 - acc: 0.031 - ETA: 1:18 - loss: 4.6810 - acc: 0.031 - ETA: 1:18 - loss: 4.6819 - acc: 0.030 - ETA: 1:17 - loss: 4.6813 - acc: 0.031 - ETA: 1:16 - loss: 4.6808 - acc: 0.031 - ETA: 1:16 - loss: 4.6802 - acc: 0.031 - ETA: 1:15 - loss: 4.6794 - acc: 0.031 - ETA: 1:14 - loss: 4.6791 - acc: 0.031 - ETA: 1:13 - loss: 4.6788 - acc: 0.031 - ETA: 1:13 - loss: 4.6792 - acc: 0.031 - ETA: 1:12 - loss: 4.6787 - acc: 0.031 - ETA: 1:11 - loss: 4.6785 - acc: 0.031 - ETA: 1:11 - loss: 4.6783 - acc: 0.030 - ETA: 1:10 - loss: 4.6794 - acc: 0.031 - ETA: 1:09 - loss: 4.6797 - acc: 0.031 - ETA: 1:09 - loss: 4.6788 - acc: 0.031 - ETA: 1:08 - loss: 4.6821 - acc: 0.031 - ETA: 1:07 - loss: 4.6817 - acc: 0.030 - ETA: 1:07 - loss: 4.6820 - acc: 0.030 - ETA: 1:06 - loss: 4.6825 - acc: 0.030 - ETA: 1:05 - loss: 4.6834 - acc: 0.030 - ETA: 1:05 - loss: 4.6836 - acc: 0.030 - ETA: 1:04 - loss: 4.6847 - acc: 0.030 - ETA: 1:03 - loss: 4.6843 - acc: 0.030 - ETA: 1:02 - loss: 4.6847 - acc: 0.030 - ETA: 1:02 - loss: 4.6843 - acc: 0.030 - ETA: 1:01 - loss: 4.6846 - acc: 0.030 - ETA: 1:00 - loss: 4.6847 - acc: 0.030 - ETA: 1:00 - loss: 4.6845 - acc: 0.030 - ETA: 59s - loss: 4.6839 - acc: 0.030 - ETA: 58s - loss: 4.6840 - acc: 0.03 - ETA: 58s - loss: 4.6830 - acc: 0.03 - ETA: 57s - loss: 4.6828 - acc: 0.03 - ETA: 56s - loss: 4.6830 - acc: 0.02 - ETA: 56s - loss: 4.6820 - acc: 0.03 - ETA: 55s - loss: 4.6838 - acc: 0.02 - ETA: 54s - loss: 4.6838 - acc: 0.02 - ETA: 53s - loss: 4.6851 - acc: 0.02 - ETA: 53s - loss: 4.6861 - acc: 0.02 - ETA: 52s - loss: 4.6865 - acc: 0.02 - ETA: 51s - loss: 4.6861 - acc: 0.02 - ETA: 51s - loss: 4.6866 - acc: 0.02 - ETA: 50s - loss: 4.6871 - acc: 0.02 - ETA: 49s - loss: 4.6867 - acc: 0.02 - ETA: 48s - loss: 4.6862 - acc: 0.02 - ETA: 48s - loss: 4.6865 - acc: 0.02 - ETA: 47s - loss: 4.6872 - acc: 0.02 - ETA: 46s - loss: 4.6883 - acc: 0.02 - ETA: 45s - loss: 4.6886 - acc: 0.02 - ETA: 45s - loss: 4.6882 - acc: 0.02 - ETA: 44s - loss: 4.6880 - acc: 0.02 - ETA: 43s - loss: 4.6882 - acc: 0.02 - ETA: 43s - loss: 4.6881 - acc: 0.02 - ETA: 42s - loss: 4.6877 - acc: 0.02 - ETA: 41s - loss: 4.6887 - acc: 0.02 - ETA: 40s - loss: 4.6889 - acc: 0.02 - ETA: 40s - loss: 4.6891 - acc: 0.02 - ETA: 39s - loss: 4.6891 - acc: 0.02 - ETA: 38s - loss: 4.6888 - acc: 0.02 - ETA: 38s - loss: 4.6882 - acc: 0.02 - ETA: 37s - loss: 4.6884 - acc: 0.02 - ETA: 36s - loss: 4.6885 - acc: 0.02 - ETA: 35s - loss: 4.6892 - acc: 0.02 - ETA: 35s - loss: 4.6897 - acc: 0.02 - ETA: 34s - loss: 4.6898 - acc: 0.02 - ETA: 33s - loss: 4.6894 - acc: 0.02 - ETA: 33s - loss: 4.6896 - acc: 0.02 - ETA: 32s - loss: 4.6898 - acc: 0.02 - ETA: 31s - loss: 4.6898 - acc: 0.02 - ETA: 31s - loss: 4.6899 - acc: 0.02 - ETA: 30s - loss: 4.6901 - acc: 0.02 - ETA: 29s - loss: 4.6901 - acc: 0.02 - ETA: 28s - loss: 4.6898 - acc: 0.02 - ETA: 28s - loss: 4.6902 - acc: 0.02 - ETA: 27s - loss: 4.6898 - acc: 0.02 - ETA: 26s - loss: 4.6904 - acc: 0.02 - ETA: 26s - loss: 4.6905 - acc: 0.02 - ETA: 25s - loss: 4.6898 - acc: 0.03 - ETA: 24s - loss: 4.6894 - acc: 0.03 - ETA: 24s - loss: 4.6891 - acc: 0.03 - ETA: 23s - loss: 4.6882 - acc: 0.02 - ETA: 22s - loss: 4.6876 - acc: 0.03 - ETA: 21s - loss: 4.6869 - acc: 0.03 - ETA: 21s - loss: 4.6864 - acc: 0.03 - ETA: 20s - loss: 4.6862 - acc: 0.03 - ETA: 19s - loss: 4.6864 - acc: 0.03 - ETA: 19s - loss: 4.6866 - acc: 0.03 - ETA: 18s - loss: 4.6867 - acc: 0.02 - ETA: 17s - loss: 4.6858 - acc: 0.02 - ETA: 16s - loss: 4.6852 - acc: 0.02 - ETA: 16s - loss: 4.6844 - acc: 0.03 - ETA: 15s - loss: 4.6843 - acc: 0.03 - ETA: 14s - loss: 4.6843 - acc: 0.03 - ETA: 14s - loss: 4.6848 - acc: 0.03 - ETA: 13s - loss: 4.6836 - acc: 0.03 - ETA: 12s - loss: 4.6828 - acc: 0.03 - ETA: 12s - loss: 4.6830 - acc: 0.03 - ETA: 11s - loss: 4.6826 - acc: 0.03 - ETA: 10s - loss: 4.6818 - acc: 0.03 - ETA: 9s - loss: 4.6814 - acc: 0.0302 - ETA: 9s - loss: 4.6818 - acc: 0.030 - ETA: 8s - loss: 4.6812 - acc: 0.030 - ETA: 7s - loss: 4.6808 - acc: 0.030 - ETA: 7s - loss: 4.6811 - acc: 0.029 - ETA: 6s - loss: 4.6810 - acc: 0.029 - ETA: 5s - loss: 4.6809 - acc: 0.029 - ETA: 4s - loss: 4.6809 - acc: 0.029 - ETA: 4s - loss: 4.6813 - acc: 0.029 - ETA: 3s - loss: 4.6807 - acc: 0.029 - ETA: 2s - loss: 4.6809 - acc: 0.029 - ETA: 2s - loss: 4.6809 - acc: 0.029 - ETA: 1s - loss: 4.6811 - acc: 0.029 - ETA: 0s - loss: 4.6809 - acc: 0.0294Epoch 00007: val_loss improved from 4.73650 to 4.72443, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 247s 37ms/step - loss: 4.6806 - acc: 0.0293 - val_loss: 4.7244 - val_acc: 0.0240\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/6680 [=================>............] - ETA: 3:35 - loss: 4.4542 - acc: 0.0000e+0 - ETA: 3:37 - loss: 4.4590 - acc: 0.0000e+0 - ETA: 3:37 - loss: 4.6193 - acc: 0.0333    - ETA: 3:38 - loss: 4.6171 - acc: 0.037 - ETA: 3:36 - loss: 4.5897 - acc: 0.070 - ETA: 3:35 - loss: 4.6067 - acc: 0.066 - ETA: 3:33 - loss: 4.6319 - acc: 0.057 - ETA: 3:33 - loss: 4.6530 - acc: 0.062 - ETA: 3:32 - loss: 4.6514 - acc: 0.061 - ETA: 3:32 - loss: 4.6629 - acc: 0.060 - ETA: 3:31 - loss: 4.6572 - acc: 0.054 - ETA: 3:31 - loss: 4.6579 - acc: 0.050 - ETA: 3:31 - loss: 4.6563 - acc: 0.046 - ETA: 3:30 - loss: 4.6499 - acc: 0.046 - ETA: 3:29 - loss: 4.6614 - acc: 0.043 - ETA: 3:28 - loss: 4.6688 - acc: 0.040 - ETA: 3:29 - loss: 4.6640 - acc: 0.044 - ETA: 3:28 - loss: 4.6640 - acc: 0.044 - ETA: 3:27 - loss: 4.6627 - acc: 0.044 - ETA: 3:27 - loss: 4.6540 - acc: 0.045 - ETA: 3:26 - loss: 4.6550 - acc: 0.042 - ETA: 3:26 - loss: 4.6516 - acc: 0.043 - ETA: 3:25 - loss: 4.6523 - acc: 0.041 - ETA: 3:25 - loss: 4.6539 - acc: 0.039 - ETA: 3:24 - loss: 4.6552 - acc: 0.038 - ETA: 3:26 - loss: 4.6449 - acc: 0.040 - ETA: 3:27 - loss: 4.6575 - acc: 0.038 - ETA: 3:29 - loss: 4.6578 - acc: 0.037 - ETA: 3:29 - loss: 4.6529 - acc: 0.037 - ETA: 3:29 - loss: 4.6518 - acc: 0.036 - ETA: 3:29 - loss: 4.6636 - acc: 0.035 - ETA: 3:28 - loss: 4.6640 - acc: 0.035 - ETA: 3:30 - loss: 4.6709 - acc: 0.034 - ETA: 3:30 - loss: 4.6700 - acc: 0.033 - ETA: 3:30 - loss: 4.6774 - acc: 0.032 - ETA: 3:29 - loss: 4.6741 - acc: 0.031 - ETA: 3:29 - loss: 4.6779 - acc: 0.031 - ETA: 3:28 - loss: 4.6755 - acc: 0.030 - ETA: 3:27 - loss: 4.6775 - acc: 0.030 - ETA: 3:27 - loss: 4.6816 - acc: 0.030 - ETA: 3:27 - loss: 4.6765 - acc: 0.029 - ETA: 3:27 - loss: 4.6761 - acc: 0.028 - ETA: 3:28 - loss: 4.6737 - acc: 0.029 - ETA: 3:27 - loss: 4.6759 - acc: 0.029 - ETA: 3:27 - loss: 4.6760 - acc: 0.030 - ETA: 3:27 - loss: 4.6726 - acc: 0.031 - ETA: 3:26 - loss: 4.6742 - acc: 0.030 - ETA: 3:25 - loss: 4.6717 - acc: 0.031 - ETA: 3:25 - loss: 4.6730 - acc: 0.032 - ETA: 3:25 - loss: 4.6716 - acc: 0.033 - ETA: 3:24 - loss: 4.6740 - acc: 0.032 - ETA: 3:24 - loss: 4.6733 - acc: 0.031 - ETA: 3:24 - loss: 4.6731 - acc: 0.033 - ETA: 3:24 - loss: 4.6748 - acc: 0.032 - ETA: 3:23 - loss: 4.6706 - acc: 0.032 - ETA: 3:23 - loss: 4.6704 - acc: 0.033 - ETA: 3:22 - loss: 4.6674 - acc: 0.034 - ETA: 3:22 - loss: 4.6691 - acc: 0.033 - ETA: 3:21 - loss: 4.6665 - acc: 0.033 - ETA: 3:20 - loss: 4.6685 - acc: 0.033 - ETA: 3:20 - loss: 4.6688 - acc: 0.032 - ETA: 3:19 - loss: 4.6662 - acc: 0.032 - ETA: 3:19 - loss: 4.6655 - acc: 0.031 - ETA: 3:18 - loss: 4.6632 - acc: 0.032 - ETA: 3:17 - loss: 4.6666 - acc: 0.032 - ETA: 3:16 - loss: 4.6638 - acc: 0.031 - ETA: 3:15 - loss: 4.6667 - acc: 0.031 - ETA: 3:14 - loss: 4.6648 - acc: 0.030 - ETA: 3:13 - loss: 4.6632 - acc: 0.030 - ETA: 3:12 - loss: 4.6628 - acc: 0.032 - ETA: 3:12 - loss: 4.6641 - acc: 0.031 - ETA: 3:11 - loss: 4.6600 - acc: 0.032 - ETA: 3:11 - loss: 4.6599 - acc: 0.032 - ETA: 3:10 - loss: 4.6569 - acc: 0.032 - ETA: 3:09 - loss: 4.6555 - acc: 0.032 - ETA: 3:09 - loss: 4.6580 - acc: 0.031 - ETA: 3:08 - loss: 4.6588 - acc: 0.031 - ETA: 3:07 - loss: 4.6579 - acc: 0.031 - ETA: 3:06 - loss: 4.6592 - acc: 0.031 - ETA: 3:05 - loss: 4.6607 - acc: 0.031 - ETA: 3:05 - loss: 4.6606 - acc: 0.031 - ETA: 3:04 - loss: 4.6610 - acc: 0.031 - ETA: 3:03 - loss: 4.6599 - acc: 0.031 - ETA: 3:02 - loss: 4.6575 - acc: 0.031 - ETA: 3:01 - loss: 4.6582 - acc: 0.031 - ETA: 3:00 - loss: 4.6594 - acc: 0.031 - ETA: 2:59 - loss: 4.6598 - acc: 0.031 - ETA: 2:58 - loss: 4.6611 - acc: 0.031 - ETA: 2:58 - loss: 4.6597 - acc: 0.031 - ETA: 2:57 - loss: 4.6624 - acc: 0.031 - ETA: 2:56 - loss: 4.6664 - acc: 0.030 - ETA: 2:55 - loss: 4.6667 - acc: 0.031 - ETA: 2:54 - loss: 4.6676 - acc: 0.031 - ETA: 2:54 - loss: 4.6659 - acc: 0.031 - ETA: 2:53 - loss: 4.6643 - acc: 0.032 - ETA: 2:52 - loss: 4.6682 - acc: 0.031 - ETA: 2:51 - loss: 4.6661 - acc: 0.031 - ETA: 2:50 - loss: 4.6653 - acc: 0.031 - ETA: 2:49 - loss: 4.6653 - acc: 0.031 - ETA: 2:48 - loss: 4.6631 - acc: 0.032 - ETA: 2:47 - loss: 4.6623 - acc: 0.033 - ETA: 2:46 - loss: 4.6629 - acc: 0.032 - ETA: 2:45 - loss: 4.6642 - acc: 0.032 - ETA: 2:45 - loss: 4.6628 - acc: 0.032 - ETA: 2:44 - loss: 4.6627 - acc: 0.032 - ETA: 2:43 - loss: 4.6620 - acc: 0.032 - ETA: 2:42 - loss: 4.6626 - acc: 0.032 - ETA: 2:41 - loss: 4.6646 - acc: 0.031 - ETA: 2:41 - loss: 4.6664 - acc: 0.031 - ETA: 2:40 - loss: 4.6644 - acc: 0.031 - ETA: 2:39 - loss: 4.6634 - acc: 0.031 - ETA: 2:38 - loss: 4.6605 - acc: 0.031 - ETA: 2:37 - loss: 4.6600 - acc: 0.031 - ETA: 2:37 - loss: 4.6590 - acc: 0.031 - ETA: 2:36 - loss: 4.6568 - acc: 0.031 - ETA: 2:36 - loss: 4.6582 - acc: 0.031 - ETA: 2:36 - loss: 4.6560 - acc: 0.031 - ETA: 2:35 - loss: 4.6567 - acc: 0.031 - ETA: 2:34 - loss: 4.6556 - acc: 0.031 - ETA: 2:33 - loss: 4.6570 - acc: 0.031 - ETA: 2:33 - loss: 4.6584 - acc: 0.031 - ETA: 2:32 - loss: 4.6590 - acc: 0.031 - ETA: 2:31 - loss: 4.6600 - acc: 0.030 - ETA: 2:30 - loss: 4.6610 - acc: 0.031 - ETA: 2:29 - loss: 4.6610 - acc: 0.031 - ETA: 2:29 - loss: 4.6598 - acc: 0.031 - ETA: 2:28 - loss: 4.6588 - acc: 0.031 - ETA: 2:27 - loss: 4.6571 - acc: 0.030 - ETA: 2:26 - loss: 4.6557 - acc: 0.031 - ETA: 2:25 - loss: 4.6585 - acc: 0.031 - ETA: 2:25 - loss: 4.6562 - acc: 0.031 - ETA: 2:24 - loss: 4.6568 - acc: 0.031 - ETA: 2:23 - loss: 4.6555 - acc: 0.031 - ETA: 2:22 - loss: 4.6557 - acc: 0.032 - ETA: 2:22 - loss: 4.6549 - acc: 0.031 - ETA: 2:21 - loss: 4.6558 - acc: 0.032 - ETA: 2:20 - loss: 4.6549 - acc: 0.031 - ETA: 2:20 - loss: 4.6560 - acc: 0.031 - ETA: 2:19 - loss: 4.6560 - acc: 0.032 - ETA: 2:18 - loss: 4.6565 - acc: 0.031 - ETA: 2:18 - loss: 4.6564 - acc: 0.031 - ETA: 2:17 - loss: 4.6563 - acc: 0.032 - ETA: 2:16 - loss: 4.6569 - acc: 0.032 - ETA: 2:15 - loss: 4.6592 - acc: 0.031 - ETA: 2:15 - loss: 4.6583 - acc: 0.031 - ETA: 2:14 - loss: 4.6589 - acc: 0.031 - ETA: 2:13 - loss: 4.6576 - acc: 0.032 - ETA: 2:13 - loss: 4.6567 - acc: 0.032 - ETA: 2:12 - loss: 4.6575 - acc: 0.032 - ETA: 2:11 - loss: 4.6595 - acc: 0.032 - ETA: 2:10 - loss: 4.6601 - acc: 0.032 - ETA: 2:10 - loss: 4.6589 - acc: 0.032 - ETA: 2:09 - loss: 4.6582 - acc: 0.032 - ETA: 2:08 - loss: 4.6585 - acc: 0.032 - ETA: 2:07 - loss: 4.6591 - acc: 0.031 - ETA: 2:07 - loss: 4.6588 - acc: 0.031 - ETA: 2:06 - loss: 4.6584 - acc: 0.031 - ETA: 2:05 - loss: 4.6600 - acc: 0.031 - ETA: 2:04 - loss: 4.6602 - acc: 0.031 - ETA: 2:04 - loss: 4.6597 - acc: 0.031 - ETA: 2:03 - loss: 4.6598 - acc: 0.031 - ETA: 2:02 - loss: 4.6608 - acc: 0.031 - ETA: 2:01 - loss: 4.6632 - acc: 0.031 - ETA: 2:01 - loss: 4.6616 - acc: 0.031 - ETA: 2:00 - loss: 4.6607 - acc: 0.031 - ETA: 1:59 - loss: 4.6596 - acc: 0.032 - ETA: 1:58 - loss: 4.6610 - acc: 0.032 - ETA: 1:58 - loss: 4.6594 - acc: 0.032 - ETA: 1:57 - loss: 4.6591 - acc: 0.032 - ETA: 1:56 - loss: 4.6597 - acc: 0.032 - ETA: 1:55 - loss: 4.6604 - acc: 0.032 - ETA: 1:55 - loss: 4.6609 - acc: 0.032 - ETA: 1:54 - loss: 4.6599 - acc: 0.032 - ETA: 1:53 - loss: 4.6590 - acc: 0.032 - ETA: 1:53 - loss: 4.6588 - acc: 0.032 - ETA: 1:52 - loss: 4.6589 - acc: 0.032 - ETA: 1:51 - loss: 4.6597 - acc: 0.032 - ETA: 1:50 - loss: 4.6594 - acc: 0.032 - ETA: 1:50 - loss: 4.6587 - acc: 0.032 - ETA: 1:49 - loss: 4.6588 - acc: 0.032 - ETA: 1:48 - loss: 4.6588 - acc: 0.032 - ETA: 1:47 - loss: 4.6587 - acc: 0.031 - ETA: 1:47 - loss: 4.6584 - acc: 0.032 - ETA: 1:46 - loss: 4.6569 - acc: 0.032 - ETA: 1:45 - loss: 4.6550 - acc: 0.033 - ETA: 1:44 - loss: 4.6560 - acc: 0.033 - ETA: 1:44 - loss: 4.6563 - acc: 0.033 - ETA: 1:43 - loss: 4.6557 - acc: 0.033 - ETA: 1:42 - loss: 4.6564 - acc: 0.033 - ETA: 1:41 - loss: 4.6573 - acc: 0.033 - ETA: 1:41 - loss: 4.6585 - acc: 0.033 - ETA: 1:40 - loss: 4.6588 - acc: 0.032 - ETA: 1:39 - loss: 4.6608 - acc: 0.032 - ETA: 1:39 - loss: 4.6599 - acc: 0.033 - ETA: 1:38 - loss: 4.6590 - acc: 0.033 - ETA: 1:37 - loss: 4.6587 - acc: 0.033 - ETA: 1:36 - loss: 4.6585 - acc: 0.033 - ETA: 1:36 - loss: 4.6586 - acc: 0.033 - ETA: 1:35 - loss: 4.6602 - acc: 0.033 - ETA: 1:34 - loss: 4.6592 - acc: 0.033 - ETA: 1:33 - loss: 4.6590 - acc: 0.033 - ETA: 1:33 - loss: 4.6585 - acc: 0.033 - ETA: 1:32 - loss: 4.6580 - acc: 0.034 - ETA: 1:31 - loss: 4.6581 - acc: 0.0341"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 1:31 - loss: 4.6575 - acc: 0.033 - ETA: 1:30 - loss: 4.6575 - acc: 0.033 - ETA: 1:29 - loss: 4.6583 - acc: 0.033 - ETA: 1:29 - loss: 4.6573 - acc: 0.033 - ETA: 1:28 - loss: 4.6578 - acc: 0.033 - ETA: 1:27 - loss: 4.6586 - acc: 0.033 - ETA: 1:26 - loss: 4.6580 - acc: 0.032 - ETA: 1:26 - loss: 4.6580 - acc: 0.032 - ETA: 1:25 - loss: 4.6569 - acc: 0.032 - ETA: 1:24 - loss: 4.6567 - acc: 0.032 - ETA: 1:24 - loss: 4.6565 - acc: 0.032 - ETA: 1:23 - loss: 4.6559 - acc: 0.033 - ETA: 1:22 - loss: 4.6562 - acc: 0.033 - ETA: 1:22 - loss: 4.6549 - acc: 0.033 - ETA: 1:21 - loss: 4.6550 - acc: 0.033 - ETA: 1:20 - loss: 4.6564 - acc: 0.033 - ETA: 1:20 - loss: 4.6566 - acc: 0.033 - ETA: 1:19 - loss: 4.6562 - acc: 0.033 - ETA: 1:18 - loss: 4.6564 - acc: 0.033 - ETA: 1:18 - loss: 4.6572 - acc: 0.033 - ETA: 1:17 - loss: 4.6568 - acc: 0.033 - ETA: 1:16 - loss: 4.6566 - acc: 0.033 - ETA: 1:15 - loss: 4.6565 - acc: 0.032 - ETA: 1:15 - loss: 4.6572 - acc: 0.032 - ETA: 1:14 - loss: 4.6578 - acc: 0.032 - ETA: 1:13 - loss: 4.6580 - acc: 0.032 - ETA: 1:13 - loss: 4.6594 - acc: 0.032 - ETA: 1:12 - loss: 4.6594 - acc: 0.032 - ETA: 1:12 - loss: 4.6589 - acc: 0.032 - ETA: 1:11 - loss: 4.6588 - acc: 0.032 - ETA: 1:10 - loss: 4.6592 - acc: 0.032 - ETA: 1:09 - loss: 4.6615 - acc: 0.032 - ETA: 1:09 - loss: 4.6612 - acc: 0.032 - ETA: 1:08 - loss: 4.6616 - acc: 0.031 - ETA: 1:07 - loss: 4.6611 - acc: 0.031 - ETA: 1:06 - loss: 4.6614 - acc: 0.031 - ETA: 1:06 - loss: 4.6609 - acc: 0.031 - ETA: 1:05 - loss: 4.6603 - acc: 0.031 - ETA: 1:04 - loss: 4.6606 - acc: 0.031 - ETA: 1:04 - loss: 4.6593 - acc: 0.032 - ETA: 1:03 - loss: 4.6606 - acc: 0.032 - ETA: 1:02 - loss: 4.6617 - acc: 0.032 - ETA: 1:02 - loss: 4.6615 - acc: 0.032 - ETA: 1:01 - loss: 4.6626 - acc: 0.032 - ETA: 1:00 - loss: 4.6617 - acc: 0.032 - ETA: 1:00 - loss: 4.6623 - acc: 0.032 - ETA: 59s - loss: 4.6619 - acc: 0.032 - ETA: 58s - loss: 4.6615 - acc: 0.03 - ETA: 57s - loss: 4.6618 - acc: 0.03 - ETA: 57s - loss: 4.6615 - acc: 0.03 - ETA: 56s - loss: 4.6613 - acc: 0.03 - ETA: 55s - loss: 4.6607 - acc: 0.03 - ETA: 55s - loss: 4.6595 - acc: 0.03 - ETA: 54s - loss: 4.6587 - acc: 0.03 - ETA: 53s - loss: 4.6593 - acc: 0.03 - ETA: 52s - loss: 4.6592 - acc: 0.03 - ETA: 52s - loss: 4.6591 - acc: 0.03 - ETA: 51s - loss: 4.6593 - acc: 0.03 - ETA: 50s - loss: 4.6586 - acc: 0.03 - ETA: 49s - loss: 4.6577 - acc: 0.03 - ETA: 49s - loss: 4.6568 - acc: 0.03 - ETA: 48s - loss: 4.6579 - acc: 0.03 - ETA: 47s - loss: 4.6569 - acc: 0.03 - ETA: 47s - loss: 4.6576 - acc: 0.03 - ETA: 46s - loss: 4.6577 - acc: 0.03 - ETA: 45s - loss: 4.6570 - acc: 0.03 - ETA: 44s - loss: 4.6563 - acc: 0.03 - ETA: 44s - loss: 4.6559 - acc: 0.03 - ETA: 43s - loss: 4.6563 - acc: 0.03 - ETA: 42s - loss: 4.6574 - acc: 0.03 - ETA: 41s - loss: 4.6568 - acc: 0.03 - ETA: 41s - loss: 4.6569 - acc: 0.03 - ETA: 40s - loss: 4.6572 - acc: 0.03 - ETA: 39s - loss: 4.6590 - acc: 0.03 - ETA: 39s - loss: 4.6586 - acc: 0.03 - ETA: 38s - loss: 4.6573 - acc: 0.03 - ETA: 37s - loss: 4.6572 - acc: 0.03 - ETA: 36s - loss: 4.6580 - acc: 0.03 - ETA: 36s - loss: 4.6588 - acc: 0.03 - ETA: 35s - loss: 4.6589 - acc: 0.03 - ETA: 34s - loss: 4.6587 - acc: 0.03 - ETA: 34s - loss: 4.6590 - acc: 0.03 - ETA: 33s - loss: 4.6584 - acc: 0.03 - ETA: 32s - loss: 4.6585 - acc: 0.03 - ETA: 31s - loss: 4.6577 - acc: 0.03 - ETA: 31s - loss: 4.6580 - acc: 0.03 - ETA: 30s - loss: 4.6589 - acc: 0.03 - ETA: 29s - loss: 4.6591 - acc: 0.03 - ETA: 29s - loss: 4.6599 - acc: 0.03 - ETA: 28s - loss: 4.6594 - acc: 0.03 - ETA: 27s - loss: 4.6592 - acc: 0.03 - ETA: 26s - loss: 4.6586 - acc: 0.03 - ETA: 26s - loss: 4.6592 - acc: 0.03 - ETA: 25s - loss: 4.6584 - acc: 0.03 - ETA: 24s - loss: 4.6586 - acc: 0.03 - ETA: 24s - loss: 4.6585 - acc: 0.03 - ETA: 23s - loss: 4.6579 - acc: 0.03 - ETA: 22s - loss: 4.6596 - acc: 0.03 - ETA: 21s - loss: 4.6600 - acc: 0.03 - ETA: 21s - loss: 4.6606 - acc: 0.03 - ETA: 20s - loss: 4.6609 - acc: 0.03 - ETA: 19s - loss: 4.6608 - acc: 0.03 - ETA: 19s - loss: 4.6607 - acc: 0.03 - ETA: 18s - loss: 4.6606 - acc: 0.03 - ETA: 17s - loss: 4.6602 - acc: 0.03 - ETA: 17s - loss: 4.6604 - acc: 0.03 - ETA: 16s - loss: 4.6612 - acc: 0.03 - ETA: 15s - loss: 4.6616 - acc: 0.03 - ETA: 14s - loss: 4.6617 - acc: 0.03 - ETA: 14s - loss: 4.6610 - acc: 0.03 - ETA: 13s - loss: 4.6606 - acc: 0.03 - ETA: 12s - loss: 4.6601 - acc: 0.03 - ETA: 12s - loss: 4.6600 - acc: 0.03 - ETA: 11s - loss: 4.6596 - acc: 0.03 - ETA: 10s - loss: 4.6598 - acc: 0.03 - ETA: 9s - loss: 4.6598 - acc: 0.0317 - ETA: 9s - loss: 4.6601 - acc: 0.031 - ETA: 8s - loss: 4.6609 - acc: 0.031 - ETA: 7s - loss: 4.6604 - acc: 0.031 - ETA: 7s - loss: 4.6599 - acc: 0.031 - ETA: 6s - loss: 4.6591 - acc: 0.031 - ETA: 5s - loss: 4.6593 - acc: 0.031 - ETA: 4s - loss: 4.6599 - acc: 0.031 - ETA: 4s - loss: 4.6601 - acc: 0.031 - ETA: 3s - loss: 4.6598 - acc: 0.031 - ETA: 2s - loss: 4.6592 - acc: 0.031 - ETA: 2s - loss: 4.6594 - acc: 0.031 - ETA: 1s - loss: 4.6593 - acc: 0.031 - ETA: 0s - loss: 4.6594 - acc: 0.0314Epoch 00008: val_loss improved from 4.72443 to 4.70445, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 249s 37ms/step - loss: 4.6587 - acc: 0.0313 - val_loss: 4.7045 - val_acc: 0.0192\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/6680 [=================>............] - ETA: 3:44 - loss: 4.5613 - acc: 0.050 - ETA: 3:44 - loss: 4.5756 - acc: 0.025 - ETA: 3:43 - loss: 4.6025 - acc: 0.033 - ETA: 3:41 - loss: 4.6054 - acc: 0.062 - ETA: 3:40 - loss: 4.6434 - acc: 0.060 - ETA: 3:38 - loss: 4.6456 - acc: 0.058 - ETA: 3:38 - loss: 4.5856 - acc: 0.057 - ETA: 3:38 - loss: 4.5592 - acc: 0.056 - ETA: 3:37 - loss: 4.5570 - acc: 0.050 - ETA: 3:36 - loss: 4.5705 - acc: 0.055 - ETA: 3:35 - loss: 4.5858 - acc: 0.050 - ETA: 3:34 - loss: 4.5831 - acc: 0.045 - ETA: 3:34 - loss: 4.6037 - acc: 0.046 - ETA: 3:33 - loss: 4.5948 - acc: 0.042 - ETA: 3:36 - loss: 4.5993 - acc: 0.040 - ETA: 3:40 - loss: 4.5980 - acc: 0.037 - ETA: 3:43 - loss: 4.6081 - acc: 0.035 - ETA: 3:46 - loss: 4.6074 - acc: 0.036 - ETA: 3:47 - loss: 4.5981 - acc: 0.034 - ETA: 3:48 - loss: 4.6189 - acc: 0.032 - ETA: 3:47 - loss: 4.6083 - acc: 0.035 - ETA: 3:46 - loss: 4.6009 - acc: 0.038 - ETA: 3:45 - loss: 4.6058 - acc: 0.039 - ETA: 3:44 - loss: 4.6070 - acc: 0.037 - ETA: 3:43 - loss: 4.6099 - acc: 0.036 - ETA: 3:41 - loss: 4.6076 - acc: 0.034 - ETA: 3:40 - loss: 4.6111 - acc: 0.035 - ETA: 3:39 - loss: 4.6058 - acc: 0.035 - ETA: 3:37 - loss: 4.6020 - acc: 0.034 - ETA: 3:36 - loss: 4.6050 - acc: 0.035 - ETA: 3:35 - loss: 4.5976 - acc: 0.035 - ETA: 3:35 - loss: 4.6040 - acc: 0.035 - ETA: 3:34 - loss: 4.6066 - acc: 0.034 - ETA: 3:33 - loss: 4.6060 - acc: 0.035 - ETA: 3:32 - loss: 4.6230 - acc: 0.034 - ETA: 3:31 - loss: 4.6217 - acc: 0.037 - ETA: 3:30 - loss: 4.6232 - acc: 0.036 - ETA: 3:29 - loss: 4.6241 - acc: 0.036 - ETA: 3:28 - loss: 4.6261 - acc: 0.037 - ETA: 3:27 - loss: 4.6221 - acc: 0.038 - ETA: 3:26 - loss: 4.6215 - acc: 0.039 - ETA: 3:25 - loss: 4.6254 - acc: 0.038 - ETA: 3:24 - loss: 4.6253 - acc: 0.038 - ETA: 3:23 - loss: 4.6194 - acc: 0.039 - ETA: 3:22 - loss: 4.6171 - acc: 0.038 - ETA: 3:21 - loss: 4.6233 - acc: 0.038 - ETA: 3:20 - loss: 4.6243 - acc: 0.037 - ETA: 3:19 - loss: 4.6267 - acc: 0.036 - ETA: 3:18 - loss: 4.6261 - acc: 0.035 - ETA: 3:18 - loss: 4.6229 - acc: 0.035 - ETA: 3:17 - loss: 4.6249 - acc: 0.034 - ETA: 3:16 - loss: 4.6212 - acc: 0.035 - ETA: 3:15 - loss: 4.6272 - acc: 0.035 - ETA: 3:14 - loss: 4.6275 - acc: 0.035 - ETA: 3:13 - loss: 4.6237 - acc: 0.035 - ETA: 3:13 - loss: 4.6255 - acc: 0.034 - ETA: 3:12 - loss: 4.6230 - acc: 0.035 - ETA: 3:11 - loss: 4.6210 - acc: 0.034 - ETA: 3:10 - loss: 4.6211 - acc: 0.034 - ETA: 3:10 - loss: 4.6216 - acc: 0.034 - ETA: 3:09 - loss: 4.6189 - acc: 0.033 - ETA: 3:08 - loss: 4.6228 - acc: 0.033 - ETA: 3:07 - loss: 4.6235 - acc: 0.032 - ETA: 3:06 - loss: 4.6243 - acc: 0.032 - ETA: 3:06 - loss: 4.6263 - acc: 0.031 - ETA: 3:05 - loss: 4.6225 - acc: 0.031 - ETA: 3:04 - loss: 4.6203 - acc: 0.032 - ETA: 3:03 - loss: 4.6201 - acc: 0.033 - ETA: 3:03 - loss: 4.6197 - acc: 0.033 - ETA: 3:02 - loss: 4.6181 - acc: 0.032 - ETA: 3:01 - loss: 4.6170 - acc: 0.032 - ETA: 3:00 - loss: 4.6207 - acc: 0.031 - ETA: 3:00 - loss: 4.6214 - acc: 0.032 - ETA: 2:59 - loss: 4.6188 - acc: 0.033 - ETA: 2:58 - loss: 4.6193 - acc: 0.032 - ETA: 2:57 - loss: 4.6209 - acc: 0.032 - ETA: 2:56 - loss: 4.6176 - acc: 0.032 - ETA: 2:56 - loss: 4.6174 - acc: 0.032 - ETA: 2:55 - loss: 4.6144 - acc: 0.032 - ETA: 2:54 - loss: 4.6180 - acc: 0.031 - ETA: 2:53 - loss: 4.6177 - acc: 0.032 - ETA: 2:52 - loss: 4.6157 - acc: 0.031 - ETA: 2:52 - loss: 4.6168 - acc: 0.031 - ETA: 2:51 - loss: 4.6152 - acc: 0.032 - ETA: 2:50 - loss: 4.6176 - acc: 0.032 - ETA: 2:49 - loss: 4.6184 - acc: 0.032 - ETA: 2:49 - loss: 4.6216 - acc: 0.031 - ETA: 2:48 - loss: 4.6200 - acc: 0.031 - ETA: 2:47 - loss: 4.6183 - acc: 0.032 - ETA: 2:46 - loss: 4.6192 - acc: 0.032 - ETA: 2:46 - loss: 4.6170 - acc: 0.032 - ETA: 2:45 - loss: 4.6171 - acc: 0.032 - ETA: 2:44 - loss: 4.6153 - acc: 0.031 - ETA: 2:44 - loss: 4.6148 - acc: 0.031 - ETA: 2:43 - loss: 4.6140 - acc: 0.031 - ETA: 2:42 - loss: 4.6147 - acc: 0.031 - ETA: 2:42 - loss: 4.6121 - acc: 0.030 - ETA: 2:41 - loss: 4.6114 - acc: 0.030 - ETA: 2:41 - loss: 4.6149 - acc: 0.030 - ETA: 2:41 - loss: 4.6142 - acc: 0.030 - ETA: 2:40 - loss: 4.6148 - acc: 0.031 - ETA: 2:39 - loss: 4.6172 - acc: 0.030 - ETA: 2:39 - loss: 4.6144 - acc: 0.032 - ETA: 2:38 - loss: 4.6174 - acc: 0.031 - ETA: 2:37 - loss: 4.6169 - acc: 0.032 - ETA: 2:36 - loss: 4.6174 - acc: 0.032 - ETA: 2:36 - loss: 4.6168 - acc: 0.032 - ETA: 2:35 - loss: 4.6173 - acc: 0.032 - ETA: 2:34 - loss: 4.6160 - acc: 0.032 - ETA: 2:33 - loss: 4.6164 - acc: 0.032 - ETA: 2:33 - loss: 4.6156 - acc: 0.032 - ETA: 2:32 - loss: 4.6170 - acc: 0.031 - ETA: 2:31 - loss: 4.6159 - acc: 0.031 - ETA: 2:31 - loss: 4.6179 - acc: 0.031 - ETA: 2:30 - loss: 4.6192 - acc: 0.031 - ETA: 2:29 - loss: 4.6191 - acc: 0.031 - ETA: 2:28 - loss: 4.6198 - acc: 0.030 - ETA: 2:28 - loss: 4.6187 - acc: 0.030 - ETA: 2:27 - loss: 4.6194 - acc: 0.030 - ETA: 2:26 - loss: 4.6200 - acc: 0.030 - ETA: 2:25 - loss: 4.6212 - acc: 0.030 - ETA: 2:25 - loss: 4.6213 - acc: 0.030 - ETA: 2:24 - loss: 4.6219 - acc: 0.030 - ETA: 2:23 - loss: 4.6219 - acc: 0.030 - ETA: 2:23 - loss: 4.6201 - acc: 0.030 - ETA: 2:22 - loss: 4.6200 - acc: 0.030 - ETA: 2:22 - loss: 4.6190 - acc: 0.030 - ETA: 2:21 - loss: 4.6198 - acc: 0.030 - ETA: 2:21 - loss: 4.6203 - acc: 0.030 - ETA: 2:20 - loss: 4.6198 - acc: 0.030 - ETA: 2:20 - loss: 4.6199 - acc: 0.030 - ETA: 2:19 - loss: 4.6204 - acc: 0.029 - ETA: 2:18 - loss: 4.6191 - acc: 0.030 - ETA: 2:18 - loss: 4.6215 - acc: 0.029 - ETA: 2:17 - loss: 4.6224 - acc: 0.030 - ETA: 2:16 - loss: 4.6236 - acc: 0.029 - ETA: 2:15 - loss: 4.6241 - acc: 0.029 - ETA: 2:15 - loss: 4.6234 - acc: 0.029 - ETA: 2:14 - loss: 4.6254 - acc: 0.029 - ETA: 2:14 - loss: 4.6268 - acc: 0.029 - ETA: 2:13 - loss: 4.6272 - acc: 0.029 - ETA: 2:13 - loss: 4.6288 - acc: 0.028 - ETA: 2:12 - loss: 4.6285 - acc: 0.029 - ETA: 2:11 - loss: 4.6279 - acc: 0.029 - ETA: 2:11 - loss: 4.6296 - acc: 0.029 - ETA: 2:10 - loss: 4.6293 - acc: 0.029 - ETA: 2:09 - loss: 4.6293 - acc: 0.029 - ETA: 2:09 - loss: 4.6287 - acc: 0.029 - ETA: 2:08 - loss: 4.6290 - acc: 0.029 - ETA: 2:07 - loss: 4.6278 - acc: 0.029 - ETA: 2:07 - loss: 4.6274 - acc: 0.029 - ETA: 2:06 - loss: 4.6275 - acc: 0.028 - ETA: 2:05 - loss: 4.6295 - acc: 0.028 - ETA: 2:05 - loss: 4.6305 - acc: 0.028 - ETA: 2:04 - loss: 4.6286 - acc: 0.029 - ETA: 2:03 - loss: 4.6299 - acc: 0.028 - ETA: 2:02 - loss: 4.6296 - acc: 0.029 - ETA: 2:02 - loss: 4.6305 - acc: 0.029 - ETA: 2:01 - loss: 4.6315 - acc: 0.029 - ETA: 2:01 - loss: 4.6314 - acc: 0.029 - ETA: 2:00 - loss: 4.6318 - acc: 0.029 - ETA: 1:59 - loss: 4.6309 - acc: 0.029 - ETA: 1:59 - loss: 4.6294 - acc: 0.029 - ETA: 1:58 - loss: 4.6298 - acc: 0.029 - ETA: 1:57 - loss: 4.6318 - acc: 0.029 - ETA: 1:57 - loss: 4.6311 - acc: 0.029 - ETA: 1:56 - loss: 4.6318 - acc: 0.029 - ETA: 1:56 - loss: 4.6307 - acc: 0.030 - ETA: 1:55 - loss: 4.6318 - acc: 0.029 - ETA: 1:54 - loss: 4.6315 - acc: 0.030 - ETA: 1:54 - loss: 4.6322 - acc: 0.029 - ETA: 1:53 - loss: 4.6313 - acc: 0.029 - ETA: 1:52 - loss: 4.6319 - acc: 0.029 - ETA: 1:51 - loss: 4.6308 - acc: 0.029 - ETA: 1:51 - loss: 4.6313 - acc: 0.030 - ETA: 1:50 - loss: 4.6316 - acc: 0.030 - ETA: 1:50 - loss: 4.6323 - acc: 0.030 - ETA: 1:49 - loss: 4.6330 - acc: 0.030 - ETA: 1:48 - loss: 4.6333 - acc: 0.030 - ETA: 1:47 - loss: 4.6341 - acc: 0.030 - ETA: 1:47 - loss: 4.6355 - acc: 0.030 - ETA: 1:46 - loss: 4.6373 - acc: 0.030 - ETA: 1:45 - loss: 4.6373 - acc: 0.030 - ETA: 1:45 - loss: 4.6372 - acc: 0.030 - ETA: 1:44 - loss: 4.6382 - acc: 0.030 - ETA: 1:43 - loss: 4.6374 - acc: 0.030 - ETA: 1:43 - loss: 4.6379 - acc: 0.030 - ETA: 1:42 - loss: 4.6383 - acc: 0.030 - ETA: 1:41 - loss: 4.6373 - acc: 0.030 - ETA: 1:40 - loss: 4.6390 - acc: 0.030 - ETA: 1:40 - loss: 4.6385 - acc: 0.030 - ETA: 1:39 - loss: 4.6398 - acc: 0.029 - ETA: 1:38 - loss: 4.6388 - acc: 0.029 - ETA: 1:38 - loss: 4.6379 - acc: 0.029 - ETA: 1:37 - loss: 4.6373 - acc: 0.030 - ETA: 1:36 - loss: 4.6370 - acc: 0.029 - ETA: 1:35 - loss: 4.6373 - acc: 0.029 - ETA: 1:35 - loss: 4.6369 - acc: 0.029 - ETA: 1:34 - loss: 4.6359 - acc: 0.029 - ETA: 1:33 - loss: 4.6346 - acc: 0.030 - ETA: 1:33 - loss: 4.6348 - acc: 0.030 - ETA: 1:32 - loss: 4.6359 - acc: 0.030 - ETA: 1:31 - loss: 4.6368 - acc: 0.030 - ETA: 1:30 - loss: 4.6378 - acc: 0.0301"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 1:30 - loss: 4.6368 - acc: 0.030 - ETA: 1:29 - loss: 4.6348 - acc: 0.031 - ETA: 1:28 - loss: 4.6350 - acc: 0.030 - ETA: 1:28 - loss: 4.6348 - acc: 0.031 - ETA: 1:27 - loss: 4.6346 - acc: 0.031 - ETA: 1:26 - loss: 4.6344 - acc: 0.031 - ETA: 1:26 - loss: 4.6348 - acc: 0.030 - ETA: 1:25 - loss: 4.6339 - acc: 0.030 - ETA: 1:24 - loss: 4.6341 - acc: 0.031 - ETA: 1:24 - loss: 4.6350 - acc: 0.030 - ETA: 1:23 - loss: 4.6354 - acc: 0.030 - ETA: 1:22 - loss: 4.6355 - acc: 0.031 - ETA: 1:22 - loss: 4.6348 - acc: 0.031 - ETA: 1:21 - loss: 4.6345 - acc: 0.031 - ETA: 1:20 - loss: 4.6342 - acc: 0.031 - ETA: 1:19 - loss: 4.6344 - acc: 0.031 - ETA: 1:19 - loss: 4.6353 - acc: 0.031 - ETA: 1:18 - loss: 4.6347 - acc: 0.031 - ETA: 1:17 - loss: 4.6353 - acc: 0.031 - ETA: 1:17 - loss: 4.6348 - acc: 0.031 - ETA: 1:16 - loss: 4.6348 - acc: 0.031 - ETA: 1:15 - loss: 4.6346 - acc: 0.031 - ETA: 1:14 - loss: 4.6348 - acc: 0.031 - ETA: 1:14 - loss: 4.6345 - acc: 0.031 - ETA: 1:13 - loss: 4.6349 - acc: 0.031 - ETA: 1:12 - loss: 4.6347 - acc: 0.031 - ETA: 1:12 - loss: 4.6334 - acc: 0.031 - ETA: 1:11 - loss: 4.6330 - acc: 0.031 - ETA: 1:10 - loss: 4.6334 - acc: 0.031 - ETA: 1:09 - loss: 4.6342 - acc: 0.031 - ETA: 1:09 - loss: 4.6341 - acc: 0.031 - ETA: 1:08 - loss: 4.6344 - acc: 0.031 - ETA: 1:07 - loss: 4.6344 - acc: 0.031 - ETA: 1:07 - loss: 4.6360 - acc: 0.031 - ETA: 1:06 - loss: 4.6342 - acc: 0.031 - ETA: 1:05 - loss: 4.6340 - acc: 0.031 - ETA: 1:04 - loss: 4.6337 - acc: 0.031 - ETA: 1:04 - loss: 4.6332 - acc: 0.031 - ETA: 1:03 - loss: 4.6343 - acc: 0.031 - ETA: 1:02 - loss: 4.6341 - acc: 0.031 - ETA: 1:02 - loss: 4.6349 - acc: 0.031 - ETA: 1:01 - loss: 4.6345 - acc: 0.031 - ETA: 1:00 - loss: 4.6346 - acc: 0.031 - ETA: 1:00 - loss: 4.6341 - acc: 0.031 - ETA: 59s - loss: 4.6344 - acc: 0.031 - ETA: 58s - loss: 4.6352 - acc: 0.03 - ETA: 57s - loss: 4.6353 - acc: 0.03 - ETA: 57s - loss: 4.6351 - acc: 0.03 - ETA: 56s - loss: 4.6358 - acc: 0.03 - ETA: 55s - loss: 4.6362 - acc: 0.03 - ETA: 55s - loss: 4.6379 - acc: 0.03 - ETA: 54s - loss: 4.6382 - acc: 0.03 - ETA: 53s - loss: 4.6388 - acc: 0.03 - ETA: 52s - loss: 4.6385 - acc: 0.03 - ETA: 52s - loss: 4.6376 - acc: 0.03 - ETA: 51s - loss: 4.6370 - acc: 0.03 - ETA: 50s - loss: 4.6393 - acc: 0.03 - ETA: 50s - loss: 4.6391 - acc: 0.03 - ETA: 49s - loss: 4.6393 - acc: 0.03 - ETA: 48s - loss: 4.6387 - acc: 0.03 - ETA: 47s - loss: 4.6380 - acc: 0.03 - ETA: 47s - loss: 4.6384 - acc: 0.03 - ETA: 46s - loss: 4.6377 - acc: 0.03 - ETA: 45s - loss: 4.6385 - acc: 0.03 - ETA: 45s - loss: 4.6380 - acc: 0.03 - ETA: 44s - loss: 4.6373 - acc: 0.03 - ETA: 43s - loss: 4.6369 - acc: 0.03 - ETA: 43s - loss: 4.6375 - acc: 0.03 - ETA: 42s - loss: 4.6383 - acc: 0.03 - ETA: 41s - loss: 4.6382 - acc: 0.03 - ETA: 40s - loss: 4.6384 - acc: 0.03 - ETA: 40s - loss: 4.6382 - acc: 0.03 - ETA: 39s - loss: 4.6393 - acc: 0.03 - ETA: 38s - loss: 4.6398 - acc: 0.03 - ETA: 38s - loss: 4.6400 - acc: 0.03 - ETA: 37s - loss: 4.6390 - acc: 0.03 - ETA: 36s - loss: 4.6383 - acc: 0.03 - ETA: 36s - loss: 4.6381 - acc: 0.03 - ETA: 35s - loss: 4.6380 - acc: 0.03 - ETA: 34s - loss: 4.6384 - acc: 0.03 - ETA: 34s - loss: 4.6383 - acc: 0.03 - ETA: 33s - loss: 4.6386 - acc: 0.03 - ETA: 32s - loss: 4.6388 - acc: 0.03 - ETA: 31s - loss: 4.6387 - acc: 0.03 - ETA: 31s - loss: 4.6395 - acc: 0.03 - ETA: 30s - loss: 4.6398 - acc: 0.03 - ETA: 29s - loss: 4.6394 - acc: 0.03 - ETA: 29s - loss: 4.6391 - acc: 0.03 - ETA: 28s - loss: 4.6397 - acc: 0.03 - ETA: 27s - loss: 4.6405 - acc: 0.03 - ETA: 27s - loss: 4.6406 - acc: 0.03 - ETA: 26s - loss: 4.6405 - acc: 0.03 - ETA: 25s - loss: 4.6403 - acc: 0.03 - ETA: 25s - loss: 4.6404 - acc: 0.03 - ETA: 24s - loss: 4.6403 - acc: 0.03 - ETA: 23s - loss: 4.6398 - acc: 0.03 - ETA: 22s - loss: 4.6400 - acc: 0.03 - ETA: 22s - loss: 4.6395 - acc: 0.03 - ETA: 21s - loss: 4.6393 - acc: 0.03 - ETA: 20s - loss: 4.6391 - acc: 0.03 - ETA: 20s - loss: 4.6386 - acc: 0.03 - ETA: 19s - loss: 4.6384 - acc: 0.03 - ETA: 18s - loss: 4.6387 - acc: 0.03 - ETA: 18s - loss: 4.6385 - acc: 0.03 - ETA: 17s - loss: 4.6395 - acc: 0.03 - ETA: 16s - loss: 4.6398 - acc: 0.03 - ETA: 16s - loss: 4.6403 - acc: 0.03 - ETA: 15s - loss: 4.6408 - acc: 0.03 - ETA: 14s - loss: 4.6407 - acc: 0.03 - ETA: 13s - loss: 4.6408 - acc: 0.03 - ETA: 13s - loss: 4.6403 - acc: 0.03 - ETA: 12s - loss: 4.6406 - acc: 0.03 - ETA: 11s - loss: 4.6406 - acc: 0.03 - ETA: 11s - loss: 4.6409 - acc: 0.03 - ETA: 10s - loss: 4.6398 - acc: 0.03 - ETA: 9s - loss: 4.6396 - acc: 0.0328 - ETA: 9s - loss: 4.6401 - acc: 0.032 - ETA: 8s - loss: 4.6395 - acc: 0.032 - ETA: 7s - loss: 4.6392 - acc: 0.032 - ETA: 6s - loss: 4.6393 - acc: 0.032 - ETA: 6s - loss: 4.6386 - acc: 0.032 - ETA: 5s - loss: 4.6391 - acc: 0.032 - ETA: 4s - loss: 4.6388 - acc: 0.032 - ETA: 4s - loss: 4.6386 - acc: 0.032 - ETA: 3s - loss: 4.6390 - acc: 0.032 - ETA: 2s - loss: 4.6400 - acc: 0.032 - ETA: 2s - loss: 4.6397 - acc: 0.032 - ETA: 1s - loss: 4.6392 - acc: 0.032 - ETA: 0s - loss: 4.6394 - acc: 0.0324Epoch 00009: val_loss improved from 4.70445 to 4.69405, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 243s 36ms/step - loss: 4.6407 - acc: 0.0323 - val_loss: 4.6941 - val_acc: 0.0311\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/6680 [=================>............] - ETA: 3:54 - loss: 4.8739 - acc: 0.050 - ETA: 3:48 - loss: 4.7962 - acc: 0.025 - ETA: 3:49 - loss: 4.7189 - acc: 0.033 - ETA: 3:45 - loss: 4.7068 - acc: 0.037 - ETA: 3:43 - loss: 4.6550 - acc: 0.040 - ETA: 3:49 - loss: 4.6763 - acc: 0.041 - ETA: 3:51 - loss: 4.6338 - acc: 0.042 - ETA: 4:00 - loss: 4.6312 - acc: 0.050 - ETA: 4:06 - loss: 4.6163 - acc: 0.050 - ETA: 4:05 - loss: 4.6039 - acc: 0.050 - ETA: 4:05 - loss: 4.5940 - acc: 0.054 - ETA: 4:10 - loss: 4.6418 - acc: 0.050 - ETA: 4:12 - loss: 4.6300 - acc: 0.046 - ETA: 4:09 - loss: 4.6366 - acc: 0.042 - ETA: 4:08 - loss: 4.6428 - acc: 0.040 - ETA: 4:05 - loss: 4.6347 - acc: 0.046 - ETA: 4:02 - loss: 4.6251 - acc: 0.044 - ETA: 4:00 - loss: 4.6092 - acc: 0.044 - ETA: 3:57 - loss: 4.6190 - acc: 0.042 - ETA: 3:57 - loss: 4.6200 - acc: 0.040 - ETA: 3:57 - loss: 4.6171 - acc: 0.040 - ETA: 3:58 - loss: 4.6236 - acc: 0.038 - ETA: 3:57 - loss: 4.6088 - acc: 0.041 - ETA: 3:55 - loss: 4.6106 - acc: 0.039 - ETA: 3:54 - loss: 4.6144 - acc: 0.038 - ETA: 3:53 - loss: 4.6215 - acc: 0.036 - ETA: 3:51 - loss: 4.6122 - acc: 0.035 - ETA: 3:50 - loss: 4.6063 - acc: 0.037 - ETA: 3:48 - loss: 4.6116 - acc: 0.039 - ETA: 3:47 - loss: 4.6158 - acc: 0.038 - ETA: 3:46 - loss: 4.6132 - acc: 0.037 - ETA: 3:45 - loss: 4.6182 - acc: 0.037 - ETA: 3:44 - loss: 4.6194 - acc: 0.037 - ETA: 3:42 - loss: 4.6190 - acc: 0.036 - ETA: 3:43 - loss: 4.6197 - acc: 0.037 - ETA: 3:43 - loss: 4.6198 - acc: 0.037 - ETA: 3:42 - loss: 4.6196 - acc: 0.037 - ETA: 3:42 - loss: 4.6190 - acc: 0.039 - ETA: 3:41 - loss: 4.6221 - acc: 0.038 - ETA: 3:39 - loss: 4.6197 - acc: 0.038 - ETA: 3:38 - loss: 4.6130 - acc: 0.039 - ETA: 3:37 - loss: 4.6106 - acc: 0.039 - ETA: 3:36 - loss: 4.6086 - acc: 0.040 - ETA: 3:36 - loss: 4.6115 - acc: 0.039 - ETA: 3:35 - loss: 4.6154 - acc: 0.038 - ETA: 3:34 - loss: 4.6155 - acc: 0.040 - ETA: 3:32 - loss: 4.6154 - acc: 0.040 - ETA: 3:31 - loss: 4.6198 - acc: 0.039 - ETA: 3:30 - loss: 4.6164 - acc: 0.039 - ETA: 3:29 - loss: 4.6115 - acc: 0.041 - ETA: 3:28 - loss: 4.6112 - acc: 0.040 - ETA: 3:27 - loss: 4.6136 - acc: 0.041 - ETA: 3:26 - loss: 4.6112 - acc: 0.040 - ETA: 3:25 - loss: 4.6098 - acc: 0.041 - ETA: 3:23 - loss: 4.6083 - acc: 0.040 - ETA: 3:22 - loss: 4.6114 - acc: 0.040 - ETA: 3:21 - loss: 4.6117 - acc: 0.040 - ETA: 3:20 - loss: 4.6125 - acc: 0.040 - ETA: 3:19 - loss: 4.6147 - acc: 0.039 - ETA: 3:19 - loss: 4.6111 - acc: 0.040 - ETA: 3:18 - loss: 4.6129 - acc: 0.041 - ETA: 3:17 - loss: 4.6103 - acc: 0.041 - ETA: 3:16 - loss: 4.6070 - acc: 0.042 - ETA: 3:15 - loss: 4.6117 - acc: 0.043 - ETA: 3:14 - loss: 4.6126 - acc: 0.043 - ETA: 3:13 - loss: 4.6105 - acc: 0.043 - ETA: 3:12 - loss: 4.6113 - acc: 0.043 - ETA: 3:11 - loss: 4.6113 - acc: 0.042 - ETA: 3:10 - loss: 4.6110 - acc: 0.042 - ETA: 3:09 - loss: 4.6151 - acc: 0.042 - ETA: 3:08 - loss: 4.6153 - acc: 0.042 - ETA: 3:08 - loss: 4.6154 - acc: 0.041 - ETA: 3:07 - loss: 4.6152 - acc: 0.041 - ETA: 3:06 - loss: 4.6140 - acc: 0.041 - ETA: 3:05 - loss: 4.6182 - acc: 0.042 - ETA: 3:04 - loss: 4.6198 - acc: 0.042 - ETA: 3:03 - loss: 4.6175 - acc: 0.043 - ETA: 3:02 - loss: 4.6141 - acc: 0.045 - ETA: 3:01 - loss: 4.6105 - acc: 0.045 - ETA: 3:00 - loss: 4.6112 - acc: 0.045 - ETA: 3:00 - loss: 4.6085 - acc: 0.045 - ETA: 2:59 - loss: 4.6062 - acc: 0.045 - ETA: 2:58 - loss: 4.6043 - acc: 0.045 - ETA: 2:57 - loss: 4.6028 - acc: 0.045 - ETA: 2:56 - loss: 4.5999 - acc: 0.045 - ETA: 2:55 - loss: 4.6009 - acc: 0.044 - ETA: 2:55 - loss: 4.6001 - acc: 0.044 - ETA: 2:54 - loss: 4.5971 - acc: 0.044 - ETA: 2:53 - loss: 4.5968 - acc: 0.043 - ETA: 2:52 - loss: 4.5988 - acc: 0.043 - ETA: 2:51 - loss: 4.5998 - acc: 0.043 - ETA: 2:51 - loss: 4.6011 - acc: 0.043 - ETA: 2:50 - loss: 4.6024 - acc: 0.043 - ETA: 2:49 - loss: 4.6028 - acc: 0.043 - ETA: 2:48 - loss: 4.6028 - acc: 0.043 - ETA: 2:47 - loss: 4.6037 - acc: 0.043 - ETA: 2:47 - loss: 4.6023 - acc: 0.043 - ETA: 2:46 - loss: 4.6039 - acc: 0.043 - ETA: 2:45 - loss: 4.6017 - acc: 0.042 - ETA: 2:44 - loss: 4.6005 - acc: 0.043 - ETA: 2:44 - loss: 4.6009 - acc: 0.044 - ETA: 2:43 - loss: 4.5997 - acc: 0.044 - ETA: 2:42 - loss: 4.6017 - acc: 0.043 - ETA: 2:41 - loss: 4.6018 - acc: 0.043 - ETA: 2:42 - loss: 4.6016 - acc: 0.042 - ETA: 2:41 - loss: 4.5999 - acc: 0.043 - ETA: 2:41 - loss: 4.6009 - acc: 0.043 - ETA: 2:40 - loss: 4.6011 - acc: 0.042 - ETA: 2:39 - loss: 4.6002 - acc: 0.042 - ETA: 2:39 - loss: 4.6004 - acc: 0.041 - ETA: 2:38 - loss: 4.5989 - acc: 0.041 - ETA: 2:38 - loss: 4.5977 - acc: 0.042 - ETA: 2:37 - loss: 4.5998 - acc: 0.042 - ETA: 2:37 - loss: 4.5971 - acc: 0.042 - ETA: 2:36 - loss: 4.5964 - acc: 0.041 - ETA: 2:36 - loss: 4.5937 - acc: 0.041 - ETA: 2:35 - loss: 4.5978 - acc: 0.041 - ETA: 2:34 - loss: 4.5976 - acc: 0.041 - ETA: 2:34 - loss: 4.5966 - acc: 0.041 - ETA: 2:33 - loss: 4.5959 - acc: 0.041 - ETA: 2:32 - loss: 4.5958 - acc: 0.040 - ETA: 2:31 - loss: 4.5943 - acc: 0.040 - ETA: 2:30 - loss: 4.5920 - acc: 0.040 - ETA: 2:30 - loss: 4.5914 - acc: 0.040 - ETA: 2:29 - loss: 4.5922 - acc: 0.040 - ETA: 2:28 - loss: 4.5933 - acc: 0.039 - ETA: 2:28 - loss: 4.5905 - acc: 0.039 - ETA: 2:27 - loss: 4.5882 - acc: 0.039 - ETA: 2:26 - loss: 4.5883 - acc: 0.039 - ETA: 2:25 - loss: 4.5912 - acc: 0.039 - ETA: 2:25 - loss: 4.5907 - acc: 0.038 - ETA: 2:24 - loss: 4.5893 - acc: 0.039 - ETA: 2:23 - loss: 4.5896 - acc: 0.038 - ETA: 2:23 - loss: 4.5890 - acc: 0.038 - ETA: 2:22 - loss: 4.5895 - acc: 0.038 - ETA: 2:21 - loss: 4.5908 - acc: 0.038 - ETA: 2:20 - loss: 4.5908 - acc: 0.038 - ETA: 2:20 - loss: 4.5897 - acc: 0.038 - ETA: 2:19 - loss: 4.5897 - acc: 0.038 - ETA: 2:19 - loss: 4.5900 - acc: 0.038 - ETA: 2:18 - loss: 4.5902 - acc: 0.038 - ETA: 2:17 - loss: 4.5905 - acc: 0.039 - ETA: 2:17 - loss: 4.5886 - acc: 0.039 - ETA: 2:16 - loss: 4.5893 - acc: 0.039 - ETA: 2:15 - loss: 4.5899 - acc: 0.039 - ETA: 2:14 - loss: 4.5898 - acc: 0.039 - ETA: 2:14 - loss: 4.5903 - acc: 0.039 - ETA: 2:13 - loss: 4.5907 - acc: 0.039 - ETA: 2:12 - loss: 4.5898 - acc: 0.039 - ETA: 2:12 - loss: 4.5913 - acc: 0.039 - ETA: 2:11 - loss: 4.5925 - acc: 0.039 - ETA: 2:10 - loss: 4.5922 - acc: 0.039 - ETA: 2:09 - loss: 4.5907 - acc: 0.039 - ETA: 2:09 - loss: 4.5910 - acc: 0.039 - ETA: 2:08 - loss: 4.5916 - acc: 0.039 - ETA: 2:07 - loss: 4.5904 - acc: 0.039 - ETA: 2:06 - loss: 4.5907 - acc: 0.039 - ETA: 2:06 - loss: 4.5902 - acc: 0.039 - ETA: 2:05 - loss: 4.5896 - acc: 0.039 - ETA: 2:04 - loss: 4.5893 - acc: 0.039 - ETA: 2:03 - loss: 4.5907 - acc: 0.038 - ETA: 2:02 - loss: 4.5898 - acc: 0.038 - ETA: 2:02 - loss: 4.5906 - acc: 0.038 - ETA: 2:01 - loss: 4.5909 - acc: 0.038 - ETA: 2:00 - loss: 4.5905 - acc: 0.038 - ETA: 1:59 - loss: 4.5913 - acc: 0.038 - ETA: 1:59 - loss: 4.5903 - acc: 0.038 - ETA: 1:58 - loss: 4.5906 - acc: 0.039 - ETA: 1:57 - loss: 4.5914 - acc: 0.039 - ETA: 1:56 - loss: 4.5914 - acc: 0.038 - ETA: 1:56 - loss: 4.5922 - acc: 0.038 - ETA: 1:55 - loss: 4.5920 - acc: 0.038 - ETA: 1:54 - loss: 4.5903 - acc: 0.038 - ETA: 1:53 - loss: 4.5897 - acc: 0.039 - ETA: 1:53 - loss: 4.5897 - acc: 0.039 - ETA: 1:52 - loss: 4.5895 - acc: 0.038 - ETA: 1:51 - loss: 4.5884 - acc: 0.039 - ETA: 1:50 - loss: 4.5904 - acc: 0.038 - ETA: 1:50 - loss: 4.5916 - acc: 0.038 - ETA: 1:49 - loss: 4.5921 - acc: 0.038 - ETA: 1:48 - loss: 4.5936 - acc: 0.038 - ETA: 1:47 - loss: 4.5929 - acc: 0.038 - ETA: 1:47 - loss: 4.5933 - acc: 0.038 - ETA: 1:46 - loss: 4.5942 - acc: 0.038 - ETA: 1:45 - loss: 4.5956 - acc: 0.037 - ETA: 1:45 - loss: 4.5950 - acc: 0.037 - ETA: 1:44 - loss: 4.5955 - acc: 0.037 - ETA: 1:43 - loss: 4.5956 - acc: 0.037 - ETA: 1:42 - loss: 4.5955 - acc: 0.037 - ETA: 1:42 - loss: 4.5973 - acc: 0.037 - ETA: 1:41 - loss: 4.5968 - acc: 0.037 - ETA: 1:40 - loss: 4.5979 - acc: 0.037 - ETA: 1:39 - loss: 4.5975 - acc: 0.037 - ETA: 1:39 - loss: 4.5987 - acc: 0.037 - ETA: 1:38 - loss: 4.5992 - acc: 0.037 - ETA: 1:37 - loss: 4.5990 - acc: 0.038 - ETA: 1:36 - loss: 4.5994 - acc: 0.037 - ETA: 1:36 - loss: 4.6000 - acc: 0.037 - ETA: 1:35 - loss: 4.6010 - acc: 0.037 - ETA: 1:34 - loss: 4.6013 - acc: 0.038 - ETA: 1:33 - loss: 4.6006 - acc: 0.037 - ETA: 1:33 - loss: 4.6010 - acc: 0.037 - ETA: 1:32 - loss: 4.6009 - acc: 0.037 - ETA: 1:31 - loss: 4.6004 - acc: 0.0375"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 1:31 - loss: 4.6004 - acc: 0.037 - ETA: 1:30 - loss: 4.6000 - acc: 0.037 - ETA: 1:29 - loss: 4.6003 - acc: 0.037 - ETA: 1:29 - loss: 4.6000 - acc: 0.036 - ETA: 1:28 - loss: 4.6007 - acc: 0.036 - ETA: 1:27 - loss: 4.6009 - acc: 0.036 - ETA: 1:27 - loss: 4.6013 - acc: 0.036 - ETA: 1:26 - loss: 4.6004 - acc: 0.036 - ETA: 1:25 - loss: 4.6006 - acc: 0.036 - ETA: 1:25 - loss: 4.5995 - acc: 0.036 - ETA: 1:24 - loss: 4.6006 - acc: 0.037 - ETA: 1:23 - loss: 4.6018 - acc: 0.037 - ETA: 1:22 - loss: 4.6026 - acc: 0.036 - ETA: 1:22 - loss: 4.6042 - acc: 0.036 - ETA: 1:21 - loss: 4.6031 - acc: 0.036 - ETA: 1:20 - loss: 4.6035 - acc: 0.036 - ETA: 1:19 - loss: 4.6034 - acc: 0.036 - ETA: 1:19 - loss: 4.6035 - acc: 0.036 - ETA: 1:18 - loss: 4.6034 - acc: 0.036 - ETA: 1:17 - loss: 4.6025 - acc: 0.036 - ETA: 1:17 - loss: 4.6023 - acc: 0.037 - ETA: 1:16 - loss: 4.6017 - acc: 0.037 - ETA: 1:15 - loss: 4.6023 - acc: 0.037 - ETA: 1:14 - loss: 4.6029 - acc: 0.037 - ETA: 1:14 - loss: 4.6030 - acc: 0.037 - ETA: 1:13 - loss: 4.6011 - acc: 0.037 - ETA: 1:12 - loss: 4.6021 - acc: 0.037 - ETA: 1:12 - loss: 4.6021 - acc: 0.037 - ETA: 1:11 - loss: 4.6008 - acc: 0.037 - ETA: 1:10 - loss: 4.6020 - acc: 0.037 - ETA: 1:10 - loss: 4.6027 - acc: 0.037 - ETA: 1:09 - loss: 4.6028 - acc: 0.037 - ETA: 1:08 - loss: 4.6027 - acc: 0.037 - ETA: 1:07 - loss: 4.6020 - acc: 0.037 - ETA: 1:07 - loss: 4.6014 - acc: 0.037 - ETA: 1:06 - loss: 4.6007 - acc: 0.037 - ETA: 1:05 - loss: 4.6010 - acc: 0.038 - ETA: 1:05 - loss: 4.6021 - acc: 0.037 - ETA: 1:04 - loss: 4.6022 - acc: 0.037 - ETA: 1:03 - loss: 4.6025 - acc: 0.037 - ETA: 1:02 - loss: 4.6030 - acc: 0.037 - ETA: 1:02 - loss: 4.6030 - acc: 0.038 - ETA: 1:01 - loss: 4.6034 - acc: 0.037 - ETA: 1:00 - loss: 4.6041 - acc: 0.037 - ETA: 1:00 - loss: 4.6042 - acc: 0.038 - ETA: 59s - loss: 4.6038 - acc: 0.038 - ETA: 58s - loss: 4.6035 - acc: 0.03 - ETA: 58s - loss: 4.6041 - acc: 0.03 - ETA: 57s - loss: 4.6046 - acc: 0.03 - ETA: 56s - loss: 4.6045 - acc: 0.03 - ETA: 56s - loss: 4.6054 - acc: 0.03 - ETA: 55s - loss: 4.6070 - acc: 0.03 - ETA: 54s - loss: 4.6074 - acc: 0.03 - ETA: 53s - loss: 4.6075 - acc: 0.03 - ETA: 53s - loss: 4.6086 - acc: 0.03 - ETA: 52s - loss: 4.6085 - acc: 0.03 - ETA: 51s - loss: 4.6089 - acc: 0.03 - ETA: 51s - loss: 4.6092 - acc: 0.03 - ETA: 50s - loss: 4.6090 - acc: 0.03 - ETA: 49s - loss: 4.6087 - acc: 0.03 - ETA: 48s - loss: 4.6092 - acc: 0.03 - ETA: 48s - loss: 4.6090 - acc: 0.03 - ETA: 47s - loss: 4.6082 - acc: 0.03 - ETA: 46s - loss: 4.6094 - acc: 0.03 - ETA: 46s - loss: 4.6096 - acc: 0.03 - ETA: 45s - loss: 4.6085 - acc: 0.03 - ETA: 44s - loss: 4.6090 - acc: 0.03 - ETA: 43s - loss: 4.6099 - acc: 0.03 - ETA: 43s - loss: 4.6091 - acc: 0.03 - ETA: 42s - loss: 4.6093 - acc: 0.03 - ETA: 41s - loss: 4.6097 - acc: 0.03 - ETA: 41s - loss: 4.6097 - acc: 0.03 - ETA: 40s - loss: 4.6103 - acc: 0.03 - ETA: 39s - loss: 4.6104 - acc: 0.03 - ETA: 38s - loss: 4.6112 - acc: 0.03 - ETA: 38s - loss: 4.6115 - acc: 0.03 - ETA: 37s - loss: 4.6112 - acc: 0.03 - ETA: 36s - loss: 4.6111 - acc: 0.03 - ETA: 36s - loss: 4.6110 - acc: 0.03 - ETA: 35s - loss: 4.6116 - acc: 0.03 - ETA: 34s - loss: 4.6117 - acc: 0.03 - ETA: 33s - loss: 4.6118 - acc: 0.03 - ETA: 33s - loss: 4.6113 - acc: 0.03 - ETA: 32s - loss: 4.6117 - acc: 0.03 - ETA: 31s - loss: 4.6124 - acc: 0.03 - ETA: 31s - loss: 4.6122 - acc: 0.03 - ETA: 30s - loss: 4.6122 - acc: 0.03 - ETA: 29s - loss: 4.6122 - acc: 0.03 - ETA: 28s - loss: 4.6121 - acc: 0.03 - ETA: 28s - loss: 4.6131 - acc: 0.03 - ETA: 27s - loss: 4.6132 - acc: 0.03 - ETA: 26s - loss: 4.6130 - acc: 0.03 - ETA: 26s - loss: 4.6134 - acc: 0.03 - ETA: 25s - loss: 4.6131 - acc: 0.03 - ETA: 24s - loss: 4.6135 - acc: 0.03 - ETA: 23s - loss: 4.6138 - acc: 0.03 - ETA: 23s - loss: 4.6133 - acc: 0.03 - ETA: 22s - loss: 4.6130 - acc: 0.03 - ETA: 21s - loss: 4.6129 - acc: 0.03 - ETA: 21s - loss: 4.6129 - acc: 0.03 - ETA: 20s - loss: 4.6130 - acc: 0.03 - ETA: 19s - loss: 4.6131 - acc: 0.03 - ETA: 19s - loss: 4.6133 - acc: 0.03 - ETA: 18s - loss: 4.6138 - acc: 0.03 - ETA: 17s - loss: 4.6145 - acc: 0.03 - ETA: 16s - loss: 4.6148 - acc: 0.03 - ETA: 16s - loss: 4.6145 - acc: 0.03 - ETA: 15s - loss: 4.6148 - acc: 0.03 - ETA: 14s - loss: 4.6154 - acc: 0.03 - ETA: 14s - loss: 4.6152 - acc: 0.03 - ETA: 13s - loss: 4.6157 - acc: 0.03 - ETA: 12s - loss: 4.6159 - acc: 0.03 - ETA: 11s - loss: 4.6154 - acc: 0.03 - ETA: 11s - loss: 4.6158 - acc: 0.03 - ETA: 10s - loss: 4.6155 - acc: 0.03 - ETA: 9s - loss: 4.6151 - acc: 0.0381 - ETA: 9s - loss: 4.6147 - acc: 0.038 - ETA: 8s - loss: 4.6145 - acc: 0.037 - ETA: 7s - loss: 4.6151 - acc: 0.038 - ETA: 7s - loss: 4.6162 - acc: 0.038 - ETA: 6s - loss: 4.6163 - acc: 0.038 - ETA: 5s - loss: 4.6166 - acc: 0.038 - ETA: 4s - loss: 4.6167 - acc: 0.038 - ETA: 4s - loss: 4.6169 - acc: 0.038 - ETA: 3s - loss: 4.6159 - acc: 0.038 - ETA: 2s - loss: 4.6166 - acc: 0.038 - ETA: 2s - loss: 4.6168 - acc: 0.038 - ETA: 1s - loss: 4.6167 - acc: 0.038 - ETA: 0s - loss: 4.6161 - acc: 0.0381Epoch 00010: val_loss improved from 4.69405 to 4.66921, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 244s 37ms/step - loss: 4.6168 - acc: 0.0382 - val_loss: 4.6692 - val_acc: 0.0347\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aa84aa8c18>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "### TODO: specify the number of epochs that you would like to use to train the model.\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "### Do NOT modify the code below this line.\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "hint_model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hint_model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 4.3062%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "dog_breed_predictions = [np.argmax(hint_model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step4'></a>\n",
    "## Step 4: Use a CNN to Classify Dog Breeds\n",
    "\n",
    "To reduce training time without sacrificing accuracy, we show you how to train a CNN using transfer learning.  In the following step, you will get a chance to use transfer learning to train your own CNN.\n",
    "\n",
    "### Obtain Bottleneck Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bottleneck_features = np.load('bottleneck_features/DogVGG16Data.npz')\n",
    "train_VGG16 = bottleneck_features['train']\n",
    "valid_VGG16 = bottleneck_features['valid']\n",
    "test_VGG16 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "The model uses the the pre-trained VGG-16 model as a fixed feature extractor, where the last convolutional output of VGG-16 is fed as input to our model.  We only add a global average pooling layer and a fully connected layer, where the latter contains one node for each dog category and is equipped with a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_2 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 68,229\n",
      "Trainable params: 68,229\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VGG16_model = Sequential()\n",
    "VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))\n",
    "VGG16_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "VGG16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VGG16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6580/6680 [============================>.] - ETA: 4:14 - loss: 15.0569 - acc: 0.0000e+ - ETA: 1:07 - loss: 15.1638 - acc: 0.0125   - ETA: 35s - loss: 15.1214 - acc: 0.0063 - ETA: 22s - loss: 14.8180 - acc: 0.011 - ETA: 16s - loss: 14.9024 - acc: 0.010 - ETA: 14s - loss: 14.8433 - acc: 0.010 - ETA: 11s - loss: 14.8931 - acc: 0.010 - ETA: 10s - loss: 14.7810 - acc: 0.012 - ETA: 9s - loss: 14.6033 - acc: 0.023 - ETA: 8s - loss: 14.5344 - acc: 0.02 - ETA: 8s - loss: 14.5578 - acc: 0.02 - ETA: 7s - loss: 14.4958 - acc: 0.02 - ETA: 7s - loss: 14.5380 - acc: 0.02 - ETA: 6s - loss: 14.5058 - acc: 0.02 - ETA: 6s - loss: 14.4790 - acc: 0.02 - ETA: 6s - loss: 14.4149 - acc: 0.02 - ETA: 5s - loss: 14.3310 - acc: 0.02 - ETA: 5s - loss: 14.2850 - acc: 0.02 - ETA: 5s - loss: 14.2401 - acc: 0.02 - ETA: 5s - loss: 14.2023 - acc: 0.03 - ETA: 5s - loss: 14.1099 - acc: 0.03 - ETA: 4s - loss: 14.0570 - acc: 0.03 - ETA: 4s - loss: 13.9746 - acc: 0.03 - ETA: 4s - loss: 13.9475 - acc: 0.03 - ETA: 4s - loss: 13.8813 - acc: 0.03 - ETA: 4s - loss: 13.8043 - acc: 0.03 - ETA: 4s - loss: 13.7559 - acc: 0.04 - ETA: 4s - loss: 13.7210 - acc: 0.04 - ETA: 4s - loss: 13.6783 - acc: 0.04 - ETA: 3s - loss: 13.5931 - acc: 0.04 - ETA: 3s - loss: 13.5734 - acc: 0.04 - ETA: 3s - loss: 13.5122 - acc: 0.04 - ETA: 3s - loss: 13.4653 - acc: 0.05 - ETA: 3s - loss: 13.3511 - acc: 0.05 - ETA: 3s - loss: 13.2745 - acc: 0.05 - ETA: 2s - loss: 13.2180 - acc: 0.05 - ETA: 2s - loss: 13.1737 - acc: 0.06 - ETA: 2s - loss: 13.0945 - acc: 0.06 - ETA: 2s - loss: 13.0737 - acc: 0.06 - ETA: 2s - loss: 13.0447 - acc: 0.06 - ETA: 2s - loss: 13.0323 - acc: 0.06 - ETA: 2s - loss: 12.9827 - acc: 0.06 - ETA: 2s - loss: 12.9407 - acc: 0.07 - ETA: 2s - loss: 12.9082 - acc: 0.07 - ETA: 1s - loss: 12.8618 - acc: 0.07 - ETA: 1s - loss: 12.8007 - acc: 0.07 - ETA: 1s - loss: 12.7494 - acc: 0.08 - ETA: 1s - loss: 12.6633 - acc: 0.08 - ETA: 1s - loss: 12.6291 - acc: 0.08 - ETA: 1s - loss: 12.5813 - acc: 0.08 - ETA: 1s - loss: 12.5208 - acc: 0.09 - ETA: 1s - loss: 12.4837 - acc: 0.09 - ETA: 1s - loss: 12.4239 - acc: 0.09 - ETA: 1s - loss: 12.3846 - acc: 0.10 - ETA: 0s - loss: 12.3587 - acc: 0.10 - ETA: 0s - loss: 12.3045 - acc: 0.10 - ETA: 0s - loss: 12.2759 - acc: 0.10 - ETA: 0s - loss: 12.2428 - acc: 0.10 - ETA: 0s - loss: 12.2184 - acc: 0.10 - ETA: 0s - loss: 12.1862 - acc: 0.11 - ETA: 0s - loss: 12.1317 - acc: 0.11 - ETA: 0s - loss: 12.0890 - acc: 0.11 - ETA: 0s - loss: 12.0588 - acc: 0.11 - ETA: 0s - loss: 12.0514 - acc: 0.11 - ETA: 0s - loss: 12.0248 - acc: 0.11 - ETA: 0s - loss: 11.9762 - acc: 0.1204Epoch 00001: val_loss improved from inf to 10.11371, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 5s 731us/step - loss: 11.9351 - acc: 0.1229 - val_loss: 10.1137 - val_acc: 0.2120\n",
      "Epoch 2/20\n",
      "6620/6680 [============================>.] - ETA: 3s - loss: 9.6832 - acc: 0.300 - ETA: 2s - loss: 9.3353 - acc: 0.275 - ETA: 2s - loss: 9.4450 - acc: 0.263 - ETA: 2s - loss: 9.3624 - acc: 0.281 - ETA: 2s - loss: 9.6416 - acc: 0.271 - ETA: 2s - loss: 9.5974 - acc: 0.266 - ETA: 2s - loss: 9.5520 - acc: 0.269 - ETA: 2s - loss: 9.5366 - acc: 0.271 - ETA: 2s - loss: 9.4650 - acc: 0.270 - ETA: 2s - loss: 9.6549 - acc: 0.262 - ETA: 2s - loss: 9.6985 - acc: 0.257 - ETA: 2s - loss: 9.7095 - acc: 0.257 - ETA: 2s - loss: 9.7202 - acc: 0.253 - ETA: 2s - loss: 9.7209 - acc: 0.257 - ETA: 2s - loss: 9.6663 - acc: 0.261 - ETA: 2s - loss: 9.7328 - acc: 0.259 - ETA: 2s - loss: 9.6207 - acc: 0.268 - ETA: 2s - loss: 9.6117 - acc: 0.269 - ETA: 2s - loss: 9.6099 - acc: 0.269 - ETA: 1s - loss: 9.6034 - acc: 0.270 - ETA: 1s - loss: 9.5294 - acc: 0.274 - ETA: 1s - loss: 9.4465 - acc: 0.276 - ETA: 1s - loss: 9.4353 - acc: 0.278 - ETA: 1s - loss: 9.4517 - acc: 0.278 - ETA: 1s - loss: 9.4833 - acc: 0.277 - ETA: 1s - loss: 9.4576 - acc: 0.279 - ETA: 1s - loss: 9.4593 - acc: 0.279 - ETA: 1s - loss: 9.4643 - acc: 0.281 - ETA: 1s - loss: 9.4709 - acc: 0.280 - ETA: 1s - loss: 9.4616 - acc: 0.282 - ETA: 1s - loss: 9.4089 - acc: 0.285 - ETA: 1s - loss: 9.3802 - acc: 0.286 - ETA: 1s - loss: 9.3782 - acc: 0.287 - ETA: 1s - loss: 9.4128 - acc: 0.285 - ETA: 1s - loss: 9.4128 - acc: 0.285 - ETA: 1s - loss: 9.4108 - acc: 0.287 - ETA: 1s - loss: 9.3921 - acc: 0.288 - ETA: 0s - loss: 9.3795 - acc: 0.289 - ETA: 0s - loss: 9.4023 - acc: 0.289 - ETA: 0s - loss: 9.4266 - acc: 0.288 - ETA: 0s - loss: 9.3957 - acc: 0.291 - ETA: 0s - loss: 9.3903 - acc: 0.291 - ETA: 0s - loss: 9.4168 - acc: 0.289 - ETA: 0s - loss: 9.3682 - acc: 0.291 - ETA: 0s - loss: 9.3672 - acc: 0.292 - ETA: 0s - loss: 9.3637 - acc: 0.293 - ETA: 0s - loss: 9.3482 - acc: 0.294 - ETA: 0s - loss: 9.3463 - acc: 0.295 - ETA: 0s - loss: 9.3373 - acc: 0.296 - ETA: 0s - loss: 9.3232 - acc: 0.298 - ETA: 0s - loss: 9.3318 - acc: 0.297 - ETA: 0s - loss: 9.3192 - acc: 0.298 - ETA: 0s - loss: 9.3072 - acc: 0.299 - ETA: 0s - loss: 9.3178 - acc: 0.299 - ETA: 0s - loss: 9.3121 - acc: 0.3006Epoch 00002: val_loss improved from 10.11371 to 9.38799, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s 483us/step - loss: 9.2959 - acc: 0.3013 - val_loss: 9.3880 - val_acc: 0.3042\n",
      "Epoch 3/20\n",
      "6660/6680 [============================>.] - ETA: 2s - loss: 7.8721 - acc: 0.500 - ETA: 3s - loss: 7.5590 - acc: 0.483 - ETA: 2s - loss: 8.2596 - acc: 0.412 - ETA: 2s - loss: 8.2323 - acc: 0.402 - ETA: 2s - loss: 8.3166 - acc: 0.404 - ETA: 2s - loss: 8.2875 - acc: 0.409 - ETA: 2s - loss: 8.4042 - acc: 0.405 - ETA: 2s - loss: 8.3938 - acc: 0.404 - ETA: 2s - loss: 8.4147 - acc: 0.400 - ETA: 2s - loss: 8.2576 - acc: 0.412 - ETA: 2s - loss: 8.3008 - acc: 0.407 - ETA: 2s - loss: 8.3909 - acc: 0.402 - ETA: 2s - loss: 8.3572 - acc: 0.404 - ETA: 2s - loss: 8.3598 - acc: 0.404 - ETA: 2s - loss: 8.4090 - acc: 0.401 - ETA: 1s - loss: 8.3846 - acc: 0.403 - ETA: 1s - loss: 8.4477 - acc: 0.399 - ETA: 1s - loss: 8.4818 - acc: 0.398 - ETA: 1s - loss: 8.4826 - acc: 0.395 - ETA: 1s - loss: 8.5324 - acc: 0.392 - ETA: 1s - loss: 8.5141 - acc: 0.392 - ETA: 1s - loss: 8.5615 - acc: 0.388 - ETA: 1s - loss: 8.5872 - acc: 0.387 - ETA: 1s - loss: 8.6311 - acc: 0.383 - ETA: 1s - loss: 8.5971 - acc: 0.385 - ETA: 1s - loss: 8.6026 - acc: 0.385 - ETA: 1s - loss: 8.5854 - acc: 0.386 - ETA: 1s - loss: 8.5836 - acc: 0.386 - ETA: 1s - loss: 8.5971 - acc: 0.386 - ETA: 1s - loss: 8.6138 - acc: 0.385 - ETA: 1s - loss: 8.5757 - acc: 0.388 - ETA: 1s - loss: 8.5740 - acc: 0.388 - ETA: 1s - loss: 8.5918 - acc: 0.388 - ETA: 1s - loss: 8.5896 - acc: 0.388 - ETA: 0s - loss: 8.5839 - acc: 0.389 - ETA: 0s - loss: 8.6044 - acc: 0.388 - ETA: 0s - loss: 8.6060 - acc: 0.388 - ETA: 0s - loss: 8.6170 - acc: 0.387 - ETA: 0s - loss: 8.6264 - acc: 0.387 - ETA: 0s - loss: 8.6359 - acc: 0.387 - ETA: 0s - loss: 8.6737 - acc: 0.385 - ETA: 0s - loss: 8.6320 - acc: 0.388 - ETA: 0s - loss: 8.6416 - acc: 0.387 - ETA: 0s - loss: 8.6308 - acc: 0.387 - ETA: 0s - loss: 8.6377 - acc: 0.386 - ETA: 0s - loss: 8.6405 - acc: 0.386 - ETA: 0s - loss: 8.6559 - acc: 0.385 - ETA: 0s - loss: 8.6418 - acc: 0.385 - ETA: 0s - loss: 8.6303 - acc: 0.386 - ETA: 0s - loss: 8.6430 - acc: 0.386 - ETA: 0s - loss: 8.6642 - acc: 0.385 - ETA: 0s - loss: 8.6736 - acc: 0.385 - ETA: 0s - loss: 8.6827 - acc: 0.3850Epoch 00003: val_loss improved from 9.38799 to 9.13593, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s 461us/step - loss: 8.6803 - acc: 0.3853 - val_loss: 9.1359 - val_acc: 0.3246\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6680 [============================>.] - ETA: 3s - loss: 7.4299 - acc: 0.500 - ETA: 2s - loss: 8.7381 - acc: 0.393 - ETA: 2s - loss: 8.3589 - acc: 0.423 - ETA: 2s - loss: 8.3960 - acc: 0.421 - ETA: 2s - loss: 8.2578 - acc: 0.431 - ETA: 2s - loss: 8.4320 - acc: 0.420 - ETA: 2s - loss: 8.3389 - acc: 0.420 - ETA: 2s - loss: 8.4122 - acc: 0.419 - ETA: 2s - loss: 8.5064 - acc: 0.414 - ETA: 2s - loss: 8.6249 - acc: 0.407 - ETA: 1s - loss: 8.6211 - acc: 0.407 - ETA: 1s - loss: 8.6361 - acc: 0.403 - ETA: 1s - loss: 8.6617 - acc: 0.401 - ETA: 1s - loss: 8.6726 - acc: 0.401 - ETA: 1s - loss: 8.6412 - acc: 0.405 - ETA: 1s - loss: 8.5174 - acc: 0.414 - ETA: 1s - loss: 8.5079 - acc: 0.412 - ETA: 1s - loss: 8.4899 - acc: 0.411 - ETA: 1s - loss: 8.4937 - acc: 0.411 - ETA: 1s - loss: 8.5098 - acc: 0.410 - ETA: 1s - loss: 8.5017 - acc: 0.411 - ETA: 1s - loss: 8.4703 - acc: 0.409 - ETA: 1s - loss: 8.4616 - acc: 0.408 - ETA: 1s - loss: 8.4349 - acc: 0.411 - ETA: 1s - loss: 8.4306 - acc: 0.412 - ETA: 1s - loss: 8.4184 - acc: 0.413 - ETA: 0s - loss: 8.3617 - acc: 0.416 - ETA: 0s - loss: 8.3592 - acc: 0.417 - ETA: 0s - loss: 8.3705 - acc: 0.417 - ETA: 0s - loss: 8.3959 - acc: 0.416 - ETA: 0s - loss: 8.3692 - acc: 0.416 - ETA: 0s - loss: 8.3713 - acc: 0.416 - ETA: 0s - loss: 8.3727 - acc: 0.415 - ETA: 0s - loss: 8.3772 - acc: 0.415 - ETA: 0s - loss: 8.3819 - acc: 0.414 - ETA: 0s - loss: 8.3468 - acc: 0.416 - ETA: 0s - loss: 8.3362 - acc: 0.416 - ETA: 0s - loss: 8.3213 - acc: 0.418 - ETA: 0s - loss: 8.3312 - acc: 0.417 - ETA: 0s - loss: 8.3556 - acc: 0.415 - ETA: 0s - loss: 8.3419 - acc: 0.416 - ETA: 0s - loss: 8.3347 - acc: 0.416 - ETA: 0s - loss: 8.3388 - acc: 0.4168Epoch 00004: val_loss improved from 9.13593 to 8.83908, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 2s 370us/step - loss: 8.3597 - acc: 0.4162 - val_loss: 8.8391 - val_acc: 0.3545\n",
      "Epoch 5/20\n",
      "6600/6680 [============================>.] - ETA: 3s - loss: 8.6158 - acc: 0.350 - ETA: 2s - loss: 8.5927 - acc: 0.438 - ETA: 2s - loss: 8.4978 - acc: 0.435 - ETA: 2s - loss: 8.2577 - acc: 0.452 - ETA: 1s - loss: 8.2362 - acc: 0.448 - ETA: 1s - loss: 8.1911 - acc: 0.452 - ETA: 1s - loss: 8.2873 - acc: 0.449 - ETA: 1s - loss: 8.2408 - acc: 0.450 - ETA: 1s - loss: 8.1570 - acc: 0.452 - ETA: 1s - loss: 8.1319 - acc: 0.454 - ETA: 1s - loss: 8.0488 - acc: 0.459 - ETA: 1s - loss: 8.0174 - acc: 0.461 - ETA: 1s - loss: 8.1034 - acc: 0.453 - ETA: 1s - loss: 8.0714 - acc: 0.453 - ETA: 1s - loss: 8.0495 - acc: 0.455 - ETA: 1s - loss: 8.0767 - acc: 0.452 - ETA: 1s - loss: 8.0288 - acc: 0.457 - ETA: 1s - loss: 8.0158 - acc: 0.458 - ETA: 1s - loss: 8.0619 - acc: 0.455 - ETA: 1s - loss: 8.0314 - acc: 0.455 - ETA: 1s - loss: 8.0229 - acc: 0.454 - ETA: 1s - loss: 8.0514 - acc: 0.452 - ETA: 1s - loss: 8.0859 - acc: 0.451 - ETA: 0s - loss: 8.1214 - acc: 0.449 - ETA: 0s - loss: 8.1591 - acc: 0.445 - ETA: 0s - loss: 8.1363 - acc: 0.446 - ETA: 0s - loss: 8.1059 - acc: 0.447 - ETA: 0s - loss: 8.0865 - acc: 0.448 - ETA: 0s - loss: 8.0786 - acc: 0.449 - ETA: 0s - loss: 8.1035 - acc: 0.448 - ETA: 0s - loss: 8.0886 - acc: 0.448 - ETA: 0s - loss: 8.0909 - acc: 0.448 - ETA: 0s - loss: 8.0972 - acc: 0.448 - ETA: 0s - loss: 8.0685 - acc: 0.450 - ETA: 0s - loss: 8.0778 - acc: 0.449 - ETA: 0s - loss: 8.0822 - acc: 0.448 - ETA: 0s - loss: 8.0885 - acc: 0.448 - ETA: 0s - loss: 8.0939 - acc: 0.448 - ETA: 0s - loss: 8.0941 - acc: 0.448 - ETA: 0s - loss: 8.0833 - acc: 0.448 - ETA: 0s - loss: 8.0947 - acc: 0.448 - ETA: 0s - loss: 8.1085 - acc: 0.4479Epoch 00005: val_loss improved from 8.83908 to 8.70854, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 2s 359us/step - loss: 8.1063 - acc: 0.4484 - val_loss: 8.7085 - val_acc: 0.3713\n",
      "Epoch 6/20\n",
      "6660/6680 [============================>.] - ETA: 2s - loss: 6.6755 - acc: 0.550 - ETA: 2s - loss: 8.3263 - acc: 0.475 - ETA: 2s - loss: 7.9400 - acc: 0.468 - ETA: 2s - loss: 7.9315 - acc: 0.472 - ETA: 2s - loss: 7.8652 - acc: 0.475 - ETA: 1s - loss: 7.9391 - acc: 0.470 - ETA: 1s - loss: 8.0646 - acc: 0.461 - ETA: 1s - loss: 8.0257 - acc: 0.460 - ETA: 1s - loss: 8.0990 - acc: 0.457 - ETA: 1s - loss: 8.0465 - acc: 0.465 - ETA: 1s - loss: 8.0409 - acc: 0.465 - ETA: 1s - loss: 7.9435 - acc: 0.469 - ETA: 1s - loss: 8.0380 - acc: 0.462 - ETA: 1s - loss: 7.9358 - acc: 0.468 - ETA: 1s - loss: 7.9621 - acc: 0.465 - ETA: 1s - loss: 7.9613 - acc: 0.467 - ETA: 1s - loss: 7.9003 - acc: 0.471 - ETA: 1s - loss: 7.9422 - acc: 0.468 - ETA: 1s - loss: 7.9491 - acc: 0.469 - ETA: 1s - loss: 7.9637 - acc: 0.468 - ETA: 1s - loss: 7.9274 - acc: 0.470 - ETA: 1s - loss: 7.9645 - acc: 0.467 - ETA: 1s - loss: 7.9599 - acc: 0.467 - ETA: 1s - loss: 7.9898 - acc: 0.464 - ETA: 0s - loss: 7.9563 - acc: 0.465 - ETA: 0s - loss: 7.9617 - acc: 0.465 - ETA: 0s - loss: 7.9150 - acc: 0.468 - ETA: 0s - loss: 7.9419 - acc: 0.466 - ETA: 0s - loss: 7.9784 - acc: 0.464 - ETA: 0s - loss: 7.9482 - acc: 0.465 - ETA: 0s - loss: 7.9434 - acc: 0.465 - ETA: 0s - loss: 7.9089 - acc: 0.466 - ETA: 0s - loss: 7.8913 - acc: 0.466 - ETA: 0s - loss: 7.8822 - acc: 0.467 - ETA: 0s - loss: 7.8982 - acc: 0.466 - ETA: 0s - loss: 7.8972 - acc: 0.467 - ETA: 0s - loss: 7.8815 - acc: 0.467 - ETA: 0s - loss: 7.8778 - acc: 0.467 - ETA: 0s - loss: 7.8775 - acc: 0.468 - ETA: 0s - loss: 7.8717 - acc: 0.468 - ETA: 0s - loss: 7.8612 - acc: 0.468 - ETA: 0s - loss: 7.8881 - acc: 0.467 - ETA: 0s - loss: 7.8969 - acc: 0.4668Epoch 00006: val_loss improved from 8.70854 to 8.60570, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 2s 360us/step - loss: 7.9022 - acc: 0.4666 - val_loss: 8.6057 - val_acc: 0.3593\n",
      "Epoch 7/20\n",
      "6660/6680 [============================>.] - ETA: 2s - loss: 11.3709 - acc: 0.25 - ETA: 1s - loss: 8.2838 - acc: 0.4550 - ETA: 1s - loss: 8.6643 - acc: 0.427 - ETA: 1s - loss: 8.7035 - acc: 0.422 - ETA: 1s - loss: 8.5893 - acc: 0.427 - ETA: 1s - loss: 8.2529 - acc: 0.443 - ETA: 1s - loss: 8.0019 - acc: 0.455 - ETA: 1s - loss: 7.9281 - acc: 0.465 - ETA: 1s - loss: 7.8279 - acc: 0.470 - ETA: 1s - loss: 7.7640 - acc: 0.475 - ETA: 1s - loss: 7.8043 - acc: 0.474 - ETA: 1s - loss: 7.8292 - acc: 0.473 - ETA: 1s - loss: 7.8073 - acc: 0.473 - ETA: 1s - loss: 7.8117 - acc: 0.471 - ETA: 1s - loss: 7.7752 - acc: 0.474 - ETA: 1s - loss: 7.7357 - acc: 0.476 - ETA: 1s - loss: 7.6656 - acc: 0.482 - ETA: 1s - loss: 7.6656 - acc: 0.482 - ETA: 1s - loss: 7.6348 - acc: 0.484 - ETA: 1s - loss: 7.6477 - acc: 0.484 - ETA: 1s - loss: 7.6524 - acc: 0.485 - ETA: 1s - loss: 7.6510 - acc: 0.485 - ETA: 0s - loss: 7.6619 - acc: 0.485 - ETA: 0s - loss: 7.6272 - acc: 0.487 - ETA: 0s - loss: 7.6570 - acc: 0.486 - ETA: 0s - loss: 7.6495 - acc: 0.486 - ETA: 0s - loss: 7.6893 - acc: 0.483 - ETA: 0s - loss: 7.6549 - acc: 0.486 - ETA: 0s - loss: 7.6394 - acc: 0.488 - ETA: 0s - loss: 7.6315 - acc: 0.489 - ETA: 0s - loss: 7.6897 - acc: 0.485 - ETA: 0s - loss: 7.6810 - acc: 0.486 - ETA: 0s - loss: 7.6820 - acc: 0.485 - ETA: 0s - loss: 7.6841 - acc: 0.486 - ETA: 0s - loss: 7.6904 - acc: 0.486 - ETA: 0s - loss: 7.6947 - acc: 0.485 - ETA: 0s - loss: 7.6809 - acc: 0.485 - ETA: 0s - loss: 7.6956 - acc: 0.485 - ETA: 0s - loss: 7.6820 - acc: 0.485 - ETA: 0s - loss: 7.6770 - acc: 0.485 - ETA: 0s - loss: 7.6946 - acc: 0.4847Epoch 00007: val_loss improved from 8.60570 to 8.42365, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 2s 350us/step - loss: 7.6862 - acc: 0.4852 - val_loss: 8.4237 - val_acc: 0.3868\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 2s - loss: 6.5462 - acc: 0.550 - ETA: 2s - loss: 7.1698 - acc: 0.533 - ETA: 2s - loss: 7.3756 - acc: 0.521 - ETA: 2s - loss: 7.4478 - acc: 0.516 - ETA: 1s - loss: 7.6769 - acc: 0.500 - ETA: 1s - loss: 7.5719 - acc: 0.507 - ETA: 1s - loss: 7.7694 - acc: 0.493 - ETA: 1s - loss: 7.7398 - acc: 0.495 - ETA: 1s - loss: 7.7620 - acc: 0.494 - ETA: 1s - loss: 7.8285 - acc: 0.490 - ETA: 1s - loss: 7.7830 - acc: 0.492 - ETA: 1s - loss: 7.7829 - acc: 0.491 - ETA: 1s - loss: 7.7786 - acc: 0.493 - ETA: 1s - loss: 7.7891 - acc: 0.491 - ETA: 1s - loss: 7.7354 - acc: 0.494 - ETA: 1s - loss: 7.6987 - acc: 0.497 - ETA: 1s - loss: 7.6240 - acc: 0.500 - ETA: 1s - loss: 7.6441 - acc: 0.499 - ETA: 1s - loss: 7.6391 - acc: 0.499 - ETA: 1s - loss: 7.6192 - acc: 0.501 - ETA: 1s - loss: 7.6010 - acc: 0.501 - ETA: 1s - loss: 7.5551 - acc: 0.504 - ETA: 1s - loss: 7.5601 - acc: 0.504 - ETA: 0s - loss: 7.5597 - acc: 0.504 - ETA: 0s - loss: 7.5842 - acc: 0.502 - ETA: 0s - loss: 7.5877 - acc: 0.502 - ETA: 0s - loss: 7.5963 - acc: 0.501 - ETA: 0s - loss: 7.6003 - acc: 0.500 - ETA: 0s - loss: 7.6291 - acc: 0.498 - ETA: 0s - loss: 7.6342 - acc: 0.498 - ETA: 0s - loss: 7.6208 - acc: 0.499 - ETA: 0s - loss: 7.5784 - acc: 0.502 - ETA: 0s - loss: 7.5933 - acc: 0.501 - ETA: 0s - loss: 7.5666 - acc: 0.502 - ETA: 0s - loss: 7.5325 - acc: 0.505 - ETA: 0s - loss: 7.5364 - acc: 0.504 - ETA: 0s - loss: 7.5580 - acc: 0.503 - ETA: 0s - loss: 7.5677 - acc: 0.503 - ETA: 0s - loss: 7.5518 - acc: 0.504 - ETA: 0s - loss: 7.5744 - acc: 0.502 - ETA: 0s - loss: 7.5832 - acc: 0.502 - ETA: 0s - loss: 7.5965 - acc: 0.5017Epoch 00008: val_loss improved from 8.42365 to 8.31621, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 2s 354us/step - loss: 7.5930 - acc: 0.5019 - val_loss: 8.3162 - val_acc: 0.3844\n",
      "Epoch 9/20\n",
      "6600/6680 [============================>.] - ETA: 2s - loss: 7.2594 - acc: 0.550 - ETA: 2s - loss: 8.1139 - acc: 0.488 - ETA: 2s - loss: 8.1771 - acc: 0.485 - ETA: 1s - loss: 8.1570 - acc: 0.488 - ETA: 1s - loss: 7.9335 - acc: 0.500 - ETA: 1s - loss: 7.8665 - acc: 0.496 - ETA: 1s - loss: 7.7837 - acc: 0.500 - ETA: 1s - loss: 7.8720 - acc: 0.495 - ETA: 1s - loss: 7.7579 - acc: 0.500 - ETA: 1s - loss: 7.7168 - acc: 0.501 - ETA: 1s - loss: 7.6985 - acc: 0.502 - ETA: 1s - loss: 7.6281 - acc: 0.507 - ETA: 1s - loss: 7.5952 - acc: 0.508 - ETA: 1s - loss: 7.5612 - acc: 0.509 - ETA: 1s - loss: 7.5224 - acc: 0.511 - ETA: 1s - loss: 7.4907 - acc: 0.513 - ETA: 1s - loss: 7.4525 - acc: 0.515 - ETA: 1s - loss: 7.3870 - acc: 0.519 - ETA: 1s - loss: 7.3582 - acc: 0.520 - ETA: 1s - loss: 7.3641 - acc: 0.520 - ETA: 0s - loss: 7.3499 - acc: 0.520 - ETA: 0s - loss: 7.3618 - acc: 0.519 - ETA: 0s - loss: 7.3743 - acc: 0.518 - ETA: 0s - loss: 7.3608 - acc: 0.518 - ETA: 0s - loss: 7.3430 - acc: 0.520 - ETA: 0s - loss: 7.3595 - acc: 0.519 - ETA: 0s - loss: 7.3675 - acc: 0.519 - ETA: 0s - loss: 7.3686 - acc: 0.519 - ETA: 0s - loss: 7.3852 - acc: 0.517 - ETA: 0s - loss: 7.3980 - acc: 0.516 - ETA: 0s - loss: 7.4576 - acc: 0.513 - ETA: 0s - loss: 7.4566 - acc: 0.513 - ETA: 0s - loss: 7.4541 - acc: 0.513 - ETA: 0s - loss: 7.4509 - acc: 0.513 - ETA: 0s - loss: 7.4455 - acc: 0.513 - ETA: 0s - loss: 7.4538 - acc: 0.512 - ETA: 0s - loss: 7.4726 - acc: 0.511 - ETA: 0s - loss: 7.4774 - acc: 0.511 - ETA: 0s - loss: 7.4419 - acc: 0.5127Epoch 00009: val_loss improved from 8.31621 to 8.29273, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 2s 322us/step - loss: 7.4597 - acc: 0.5117 - val_loss: 8.2927 - val_acc: 0.3844\n",
      "Epoch 10/20\n",
      "6640/6680 [============================>.] - ETA: 3s - loss: 8.0591 - acc: 0.500 - ETA: 1s - loss: 7.3146 - acc: 0.525 - ETA: 1s - loss: 6.9891 - acc: 0.547 - ETA: 1s - loss: 7.2966 - acc: 0.528 - ETA: 1s - loss: 7.2817 - acc: 0.529 - ETA: 1s - loss: 7.4591 - acc: 0.520 - ETA: 1s - loss: 7.6097 - acc: 0.510 - ETA: 1s - loss: 7.6048 - acc: 0.509 - ETA: 1s - loss: 7.5951 - acc: 0.508 - ETA: 1s - loss: 7.5092 - acc: 0.513 - ETA: 1s - loss: 7.5074 - acc: 0.514 - ETA: 1s - loss: 7.5661 - acc: 0.509 - ETA: 1s - loss: 7.6443 - acc: 0.506 - ETA: 1s - loss: 7.6195 - acc: 0.507 - ETA: 1s - loss: 7.6184 - acc: 0.506 - ETA: 1s - loss: 7.5620 - acc: 0.509 - ETA: 1s - loss: 7.5849 - acc: 0.508 - ETA: 1s - loss: 7.6130 - acc: 0.506 - ETA: 0s - loss: 7.6050 - acc: 0.506 - ETA: 0s - loss: 7.5741 - acc: 0.508 - ETA: 0s - loss: 7.5380 - acc: 0.510 - ETA: 0s - loss: 7.5435 - acc: 0.510 - ETA: 0s - loss: 7.4870 - acc: 0.513 - ETA: 0s - loss: 7.4721 - acc: 0.514 - ETA: 0s - loss: 7.4190 - acc: 0.517 - ETA: 0s - loss: 7.3479 - acc: 0.521 - ETA: 0s - loss: 7.3447 - acc: 0.521 - ETA: 0s - loss: 7.3442 - acc: 0.521 - ETA: 0s - loss: 7.3564 - acc: 0.521 - ETA: 0s - loss: 7.3525 - acc: 0.521 - ETA: 0s - loss: 7.3895 - acc: 0.519 - ETA: 0s - loss: 7.3599 - acc: 0.521 - ETA: 0s - loss: 7.3231 - acc: 0.524 - ETA: 0s - loss: 7.3182 - acc: 0.524 - ETA: 0s - loss: 7.3151 - acc: 0.523 - ETA: 0s - loss: 7.3352 - acc: 0.522 - ETA: 0s - loss: 7.3467 - acc: 0.521 - ETA: 0s - loss: 7.3638 - acc: 0.5209Epoch 00010: val_loss improved from 8.29273 to 8.20082, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 2s 315us/step - loss: 7.3637 - acc: 0.5207 - val_loss: 8.2008 - val_acc: 0.3988\n",
      "Epoch 11/20\n",
      "6640/6680 [============================>.] - ETA: 3s - loss: 8.0805 - acc: 0.500 - ETA: 1s - loss: 7.2499 - acc: 0.531 - ETA: 1s - loss: 7.5059 - acc: 0.517 - ETA: 1s - loss: 7.5349 - acc: 0.520 - ETA: 1s - loss: 7.2289 - acc: 0.539 - ETA: 1s - loss: 7.2346 - acc: 0.541 - ETA: 1s - loss: 7.2387 - acc: 0.542 - ETA: 1s - loss: 7.3024 - acc: 0.537 - ETA: 1s - loss: 7.3477 - acc: 0.534 - ETA: 1s - loss: 7.3801 - acc: 0.532 - ETA: 1s - loss: 7.3702 - acc: 0.532 - ETA: 1s - loss: 7.4815 - acc: 0.525 - ETA: 1s - loss: 7.5103 - acc: 0.522 - ETA: 1s - loss: 7.5071 - acc: 0.523 - ETA: 1s - loss: 7.4482 - acc: 0.526 - ETA: 1s - loss: 7.4412 - acc: 0.526 - ETA: 1s - loss: 7.3660 - acc: 0.530 - ETA: 1s - loss: 7.4473 - acc: 0.525 - ETA: 0s - loss: 7.4640 - acc: 0.523 - ETA: 0s - loss: 7.4722 - acc: 0.522 - ETA: 0s - loss: 7.4618 - acc: 0.522 - ETA: 0s - loss: 7.4554 - acc: 0.523 - ETA: 0s - loss: 7.5104 - acc: 0.519 - ETA: 0s - loss: 7.4385 - acc: 0.523 - ETA: 0s - loss: 7.3884 - acc: 0.525 - ETA: 0s - loss: 7.3797 - acc: 0.525 - ETA: 0s - loss: 7.3446 - acc: 0.526 - ETA: 0s - loss: 7.3201 - acc: 0.528 - ETA: 0s - loss: 7.2982 - acc: 0.529 - ETA: 0s - loss: 7.2802 - acc: 0.531 - ETA: 0s - loss: 7.3106 - acc: 0.529 - ETA: 0s - loss: 7.3151 - acc: 0.529 - ETA: 0s - loss: 7.3138 - acc: 0.529 - ETA: 0s - loss: 7.3019 - acc: 0.530 - ETA: 0s - loss: 7.2832 - acc: 0.530 - ETA: 0s - loss: 7.2887 - acc: 0.530 - ETA: 0s - loss: 7.2769 - acc: 0.531 - ETA: 0s - loss: 7.2455 - acc: 0.5331Epoch 00011: val_loss improved from 8.20082 to 7.96964, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 2s 311us/step - loss: 7.2531 - acc: 0.5326 - val_loss: 7.9696 - val_acc: 0.4168\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6580/6680 [============================>.] - ETA: 3s - loss: 6.4474 - acc: 0.600 - ETA: 1s - loss: 7.1236 - acc: 0.550 - ETA: 1s - loss: 7.2953 - acc: 0.539 - ETA: 1s - loss: 7.0806 - acc: 0.550 - ETA: 1s - loss: 6.9386 - acc: 0.558 - ETA: 1s - loss: 6.7652 - acc: 0.568 - ETA: 1s - loss: 6.9570 - acc: 0.554 - ETA: 1s - loss: 6.8710 - acc: 0.557 - ETA: 1s - loss: 6.9762 - acc: 0.553 - ETA: 1s - loss: 7.0184 - acc: 0.551 - ETA: 1s - loss: 6.9587 - acc: 0.555 - ETA: 1s - loss: 7.0882 - acc: 0.547 - ETA: 1s - loss: 7.1390 - acc: 0.543 - ETA: 1s - loss: 7.1739 - acc: 0.540 - ETA: 1s - loss: 7.2159 - acc: 0.537 - ETA: 1s - loss: 7.2167 - acc: 0.537 - ETA: 1s - loss: 7.2168 - acc: 0.537 - ETA: 1s - loss: 7.2301 - acc: 0.536 - ETA: 1s - loss: 7.1720 - acc: 0.540 - ETA: 0s - loss: 7.1540 - acc: 0.542 - ETA: 0s - loss: 7.1549 - acc: 0.543 - ETA: 0s - loss: 7.1837 - acc: 0.540 - ETA: 0s - loss: 7.1796 - acc: 0.541 - ETA: 0s - loss: 7.1527 - acc: 0.543 - ETA: 0s - loss: 7.1370 - acc: 0.543 - ETA: 0s - loss: 7.1318 - acc: 0.544 - ETA: 0s - loss: 7.0934 - acc: 0.546 - ETA: 0s - loss: 7.1061 - acc: 0.545 - ETA: 0s - loss: 7.1450 - acc: 0.543 - ETA: 0s - loss: 7.1501 - acc: 0.543 - ETA: 0s - loss: 7.1241 - acc: 0.544 - ETA: 0s - loss: 7.1069 - acc: 0.546 - ETA: 0s - loss: 7.1379 - acc: 0.544 - ETA: 0s - loss: 7.1389 - acc: 0.544 - ETA: 0s - loss: 7.1826 - acc: 0.541 - ETA: 0s - loss: 7.1615 - acc: 0.542 - ETA: 0s - loss: 7.1650 - acc: 0.542 - ETA: 0s - loss: 7.1400 - acc: 0.5442Epoch 00012: val_loss improved from 7.96964 to 7.92823, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 2s 318us/step - loss: 7.1179 - acc: 0.5457 - val_loss: 7.9282 - val_acc: 0.4240\n",
      "Epoch 13/20\n",
      "6580/6680 [============================>.] - ETA: 2s - loss: 3.2261 - acc: 0.800 - ETA: 1s - loss: 6.6215 - acc: 0.585 - ETA: 1s - loss: 6.7949 - acc: 0.576 - ETA: 1s - loss: 6.6871 - acc: 0.577 - ETA: 1s - loss: 6.8405 - acc: 0.565 - ETA: 1s - loss: 6.8424 - acc: 0.564 - ETA: 1s - loss: 6.8743 - acc: 0.561 - ETA: 1s - loss: 6.8790 - acc: 0.562 - ETA: 1s - loss: 6.9940 - acc: 0.556 - ETA: 1s - loss: 7.0234 - acc: 0.556 - ETA: 1s - loss: 7.0559 - acc: 0.555 - ETA: 1s - loss: 7.1015 - acc: 0.552 - ETA: 1s - loss: 7.0460 - acc: 0.555 - ETA: 1s - loss: 7.0147 - acc: 0.557 - ETA: 1s - loss: 7.0156 - acc: 0.556 - ETA: 1s - loss: 7.0498 - acc: 0.554 - ETA: 1s - loss: 7.0312 - acc: 0.555 - ETA: 1s - loss: 7.0511 - acc: 0.554 - ETA: 1s - loss: 7.0437 - acc: 0.554 - ETA: 0s - loss: 7.0515 - acc: 0.553 - ETA: 0s - loss: 7.0607 - acc: 0.552 - ETA: 0s - loss: 7.0808 - acc: 0.551 - ETA: 0s - loss: 7.0981 - acc: 0.550 - ETA: 0s - loss: 7.0774 - acc: 0.551 - ETA: 0s - loss: 7.0972 - acc: 0.550 - ETA: 0s - loss: 7.1083 - acc: 0.550 - ETA: 0s - loss: 7.1217 - acc: 0.549 - ETA: 0s - loss: 7.1285 - acc: 0.548 - ETA: 0s - loss: 7.1106 - acc: 0.550 - ETA: 0s - loss: 7.1338 - acc: 0.548 - ETA: 0s - loss: 7.1208 - acc: 0.549 - ETA: 0s - loss: 7.1164 - acc: 0.549 - ETA: 0s - loss: 7.1361 - acc: 0.548 - ETA: 0s - loss: 7.1089 - acc: 0.550 - ETA: 0s - loss: 7.0988 - acc: 0.550 - ETA: 0s - loss: 7.1060 - acc: 0.550 - ETA: 0s - loss: 7.1164 - acc: 0.549 - ETA: 0s - loss: 7.1009 - acc: 0.5506Epoch 00013: val_loss improved from 7.92823 to 7.91736, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 2s 319us/step - loss: 7.0987 - acc: 0.5507 - val_loss: 7.9174 - val_acc: 0.4275\n",
      "Epoch 14/20\n",
      "6500/6680 [============================>.] - ETA: 2s - loss: 7.2536 - acc: 0.550 - ETA: 1s - loss: 7.0242 - acc: 0.560 - ETA: 1s - loss: 6.4980 - acc: 0.594 - ETA: 1s - loss: 6.5692 - acc: 0.591 - ETA: 1s - loss: 6.6591 - acc: 0.584 - ETA: 1s - loss: 6.6903 - acc: 0.582 - ETA: 1s - loss: 6.8470 - acc: 0.571 - ETA: 1s - loss: 6.9651 - acc: 0.564 - ETA: 1s - loss: 6.9231 - acc: 0.566 - ETA: 1s - loss: 6.9614 - acc: 0.564 - ETA: 1s - loss: 7.0212 - acc: 0.559 - ETA: 1s - loss: 7.0432 - acc: 0.558 - ETA: 1s - loss: 7.1448 - acc: 0.552 - ETA: 1s - loss: 7.1030 - acc: 0.554 - ETA: 1s - loss: 7.1320 - acc: 0.552 - ETA: 1s - loss: 7.2132 - acc: 0.546 - ETA: 1s - loss: 7.1568 - acc: 0.550 - ETA: 1s - loss: 7.1342 - acc: 0.551 - ETA: 0s - loss: 7.1519 - acc: 0.550 - ETA: 0s - loss: 7.1392 - acc: 0.551 - ETA: 0s - loss: 7.1406 - acc: 0.551 - ETA: 0s - loss: 7.1042 - acc: 0.553 - ETA: 0s - loss: 7.1161 - acc: 0.552 - ETA: 0s - loss: 7.1278 - acc: 0.551 - ETA: 0s - loss: 7.1012 - acc: 0.553 - ETA: 0s - loss: 7.1075 - acc: 0.553 - ETA: 0s - loss: 7.0665 - acc: 0.555 - ETA: 0s - loss: 7.0488 - acc: 0.555 - ETA: 0s - loss: 7.0622 - acc: 0.554 - ETA: 0s - loss: 7.0841 - acc: 0.553 - ETA: 0s - loss: 7.0971 - acc: 0.553 - ETA: 0s - loss: 7.0823 - acc: 0.554 - ETA: 0s - loss: 7.0927 - acc: 0.553 - ETA: 0s - loss: 7.0761 - acc: 0.554 - ETA: 0s - loss: 7.0765 - acc: 0.554 - ETA: 0s - loss: 7.0617 - acc: 0.555 - ETA: 0s - loss: 7.0815 - acc: 0.5543Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s 308us/step - loss: 7.0869 - acc: 0.5540 - val_loss: 7.9554 - val_acc: 0.4228\n",
      "Epoch 15/20\n",
      "6580/6680 [============================>.] - ETA: 2s - loss: 11.2262 - acc: 0.30 - ETA: 1s - loss: 7.1192 - acc: 0.5500 - ETA: 1s - loss: 7.1542 - acc: 0.542 - ETA: 1s - loss: 7.2729 - acc: 0.530 - ETA: 1s - loss: 7.0930 - acc: 0.543 - ETA: 1s - loss: 7.2035 - acc: 0.538 - ETA: 1s - loss: 7.2723 - acc: 0.532 - ETA: 1s - loss: 7.2749 - acc: 0.532 - ETA: 1s - loss: 7.0826 - acc: 0.545 - ETA: 1s - loss: 7.0534 - acc: 0.548 - ETA: 1s - loss: 6.9824 - acc: 0.552 - ETA: 1s - loss: 6.9590 - acc: 0.554 - ETA: 1s - loss: 7.0009 - acc: 0.551 - ETA: 1s - loss: 6.9338 - acc: 0.556 - ETA: 1s - loss: 6.9813 - acc: 0.552 - ETA: 1s - loss: 6.8880 - acc: 0.557 - ETA: 1s - loss: 6.9214 - acc: 0.555 - ETA: 1s - loss: 6.8482 - acc: 0.559 - ETA: 1s - loss: 6.9038 - acc: 0.556 - ETA: 0s - loss: 6.8943 - acc: 0.558 - ETA: 0s - loss: 6.9186 - acc: 0.556 - ETA: 0s - loss: 6.9398 - acc: 0.555 - ETA: 0s - loss: 6.9146 - acc: 0.556 - ETA: 0s - loss: 6.8806 - acc: 0.559 - ETA: 0s - loss: 6.8657 - acc: 0.560 - ETA: 0s - loss: 6.8782 - acc: 0.559 - ETA: 0s - loss: 6.8992 - acc: 0.558 - ETA: 0s - loss: 6.9678 - acc: 0.553 - ETA: 0s - loss: 6.9794 - acc: 0.553 - ETA: 0s - loss: 6.9644 - acc: 0.554 - ETA: 0s - loss: 6.9860 - acc: 0.553 - ETA: 0s - loss: 7.0170 - acc: 0.551 - ETA: 0s - loss: 7.0202 - acc: 0.551 - ETA: 0s - loss: 6.9966 - acc: 0.552 - ETA: 0s - loss: 6.9973 - acc: 0.552 - ETA: 0s - loss: 6.9821 - acc: 0.553 - ETA: 0s - loss: 6.9571 - acc: 0.555 - ETA: 0s - loss: 6.9580 - acc: 0.5558Epoch 00015: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s 315us/step - loss: 6.9460 - acc: 0.5563 - val_loss: 7.9228 - val_acc: 0.4228\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6620/6680 [============================>.] - ETA: 2s - loss: 5.6746 - acc: 0.650 - ETA: 1s - loss: 6.4284 - acc: 0.585 - ETA: 1s - loss: 6.7169 - acc: 0.568 - ETA: 1s - loss: 6.5590 - acc: 0.576 - ETA: 1s - loss: 6.5862 - acc: 0.577 - ETA: 1s - loss: 6.5247 - acc: 0.583 - ETA: 1s - loss: 6.8018 - acc: 0.566 - ETA: 1s - loss: 6.7454 - acc: 0.569 - ETA: 1s - loss: 6.7787 - acc: 0.568 - ETA: 1s - loss: 6.7536 - acc: 0.570 - ETA: 1s - loss: 6.7596 - acc: 0.571 - ETA: 1s - loss: 6.7890 - acc: 0.570 - ETA: 1s - loss: 6.8063 - acc: 0.569 - ETA: 1s - loss: 6.7593 - acc: 0.572 - ETA: 1s - loss: 6.8827 - acc: 0.564 - ETA: 1s - loss: 6.8182 - acc: 0.568 - ETA: 1s - loss: 6.7875 - acc: 0.569 - ETA: 1s - loss: 6.7959 - acc: 0.569 - ETA: 0s - loss: 6.8224 - acc: 0.567 - ETA: 0s - loss: 6.8333 - acc: 0.566 - ETA: 0s - loss: 6.8211 - acc: 0.567 - ETA: 0s - loss: 6.7658 - acc: 0.570 - ETA: 0s - loss: 6.7378 - acc: 0.572 - ETA: 0s - loss: 6.7436 - acc: 0.571 - ETA: 0s - loss: 6.7656 - acc: 0.570 - ETA: 0s - loss: 6.7507 - acc: 0.571 - ETA: 0s - loss: 6.7682 - acc: 0.570 - ETA: 0s - loss: 6.7543 - acc: 0.571 - ETA: 0s - loss: 6.7643 - acc: 0.570 - ETA: 0s - loss: 6.7727 - acc: 0.570 - ETA: 0s - loss: 6.7804 - acc: 0.569 - ETA: 0s - loss: 6.7717 - acc: 0.570 - ETA: 0s - loss: 6.7836 - acc: 0.569 - ETA: 0s - loss: 6.7853 - acc: 0.569 - ETA: 0s - loss: 6.7890 - acc: 0.569 - ETA: 0s - loss: 6.8043 - acc: 0.567 - ETA: 0s - loss: 6.8357 - acc: 0.566 - ETA: 0s - loss: 6.8414 - acc: 0.5653Epoch 00016: val_loss improved from 7.91736 to 7.76023, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 2s 317us/step - loss: 6.8380 - acc: 0.5656 - val_loss: 7.7602 - val_acc: 0.4359\n",
      "Epoch 17/20\n",
      "6580/6680 [============================>.] - ETA: 2s - loss: 6.4472 - acc: 0.600 - ETA: 1s - loss: 7.1296 - acc: 0.550 - ETA: 1s - loss: 6.8524 - acc: 0.565 - ETA: 1s - loss: 6.7368 - acc: 0.567 - ETA: 1s - loss: 6.7324 - acc: 0.571 - ETA: 1s - loss: 6.9019 - acc: 0.559 - ETA: 1s - loss: 6.9040 - acc: 0.561 - ETA: 1s - loss: 6.8788 - acc: 0.564 - ETA: 1s - loss: 6.8089 - acc: 0.568 - ETA: 1s - loss: 6.7816 - acc: 0.570 - ETA: 1s - loss: 6.8387 - acc: 0.567 - ETA: 1s - loss: 6.8341 - acc: 0.567 - ETA: 1s - loss: 6.7575 - acc: 0.572 - ETA: 1s - loss: 6.6742 - acc: 0.575 - ETA: 1s - loss: 6.6773 - acc: 0.575 - ETA: 1s - loss: 6.6486 - acc: 0.577 - ETA: 1s - loss: 6.6523 - acc: 0.577 - ETA: 1s - loss: 6.6722 - acc: 0.575 - ETA: 1s - loss: 6.6745 - acc: 0.575 - ETA: 1s - loss: 6.7093 - acc: 0.573 - ETA: 1s - loss: 6.6657 - acc: 0.576 - ETA: 1s - loss: 6.6828 - acc: 0.575 - ETA: 0s - loss: 6.6760 - acc: 0.576 - ETA: 0s - loss: 6.6949 - acc: 0.574 - ETA: 0s - loss: 6.6879 - acc: 0.574 - ETA: 0s - loss: 6.6942 - acc: 0.574 - ETA: 0s - loss: 6.6889 - acc: 0.575 - ETA: 0s - loss: 6.6675 - acc: 0.576 - ETA: 0s - loss: 6.6958 - acc: 0.575 - ETA: 0s - loss: 6.6680 - acc: 0.576 - ETA: 0s - loss: 6.6511 - acc: 0.578 - ETA: 0s - loss: 6.6651 - acc: 0.577 - ETA: 0s - loss: 6.6825 - acc: 0.575 - ETA: 0s - loss: 6.6642 - acc: 0.576 - ETA: 0s - loss: 6.6785 - acc: 0.575 - ETA: 0s - loss: 6.6905 - acc: 0.574 - ETA: 0s - loss: 6.6876 - acc: 0.574 - ETA: 0s - loss: 6.7002 - acc: 0.574 - ETA: 0s - loss: 6.7398 - acc: 0.571 - ETA: 0s - loss: 6.7748 - acc: 0.569 - ETA: 0s - loss: 6.7801 - acc: 0.5690Epoch 00017: val_loss improved from 7.76023 to 7.75516, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 2s 343us/step - loss: 6.7756 - acc: 0.5692 - val_loss: 7.7552 - val_acc: 0.4395\n",
      "Epoch 18/20\n",
      "6660/6680 [============================>.] - ETA: 3s - loss: 7.2532 - acc: 0.550 - ETA: 2s - loss: 6.8522 - acc: 0.575 - ETA: 1s - loss: 6.8399 - acc: 0.572 - ETA: 1s - loss: 6.8809 - acc: 0.569 - ETA: 1s - loss: 6.8924 - acc: 0.567 - ETA: 1s - loss: 6.8609 - acc: 0.566 - ETA: 1s - loss: 6.7861 - acc: 0.570 - ETA: 1s - loss: 6.5848 - acc: 0.583 - ETA: 1s - loss: 6.5824 - acc: 0.583 - ETA: 1s - loss: 6.5804 - acc: 0.581 - ETA: 1s - loss: 6.5886 - acc: 0.581 - ETA: 1s - loss: 6.6659 - acc: 0.577 - ETA: 1s - loss: 6.6341 - acc: 0.579 - ETA: 1s - loss: 6.7052 - acc: 0.575 - ETA: 1s - loss: 6.7004 - acc: 0.575 - ETA: 1s - loss: 6.6955 - acc: 0.575 - ETA: 1s - loss: 6.7098 - acc: 0.575 - ETA: 1s - loss: 6.7650 - acc: 0.571 - ETA: 1s - loss: 6.7689 - acc: 0.571 - ETA: 0s - loss: 6.7858 - acc: 0.570 - ETA: 0s - loss: 6.8005 - acc: 0.569 - ETA: 0s - loss: 6.8362 - acc: 0.567 - ETA: 0s - loss: 6.8349 - acc: 0.568 - ETA: 0s - loss: 6.8386 - acc: 0.567 - ETA: 0s - loss: 6.8190 - acc: 0.569 - ETA: 0s - loss: 6.7967 - acc: 0.570 - ETA: 0s - loss: 6.7868 - acc: 0.571 - ETA: 0s - loss: 6.7801 - acc: 0.571 - ETA: 0s - loss: 6.7445 - acc: 0.573 - ETA: 0s - loss: 6.7561 - acc: 0.573 - ETA: 0s - loss: 6.7298 - acc: 0.574 - ETA: 0s - loss: 6.7166 - acc: 0.575 - ETA: 0s - loss: 6.7279 - acc: 0.575 - ETA: 0s - loss: 6.7178 - acc: 0.575 - ETA: 0s - loss: 6.7051 - acc: 0.576 - ETA: 0s - loss: 6.7003 - acc: 0.577 - ETA: 0s - loss: 6.7447 - acc: 0.574 - ETA: 0s - loss: 6.7429 - acc: 0.5745Epoch 00018: val_loss improved from 7.75516 to 7.72155, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 2s 315us/step - loss: 6.7444 - acc: 0.5744 - val_loss: 7.7215 - val_acc: 0.4467\n",
      "Epoch 19/20\n",
      "6660/6680 [============================>.] - ETA: 3s - loss: 4.0296 - acc: 0.750 - ETA: 1s - loss: 6.4485 - acc: 0.600 - ETA: 1s - loss: 6.6555 - acc: 0.585 - ETA: 1s - loss: 6.3416 - acc: 0.605 - ETA: 1s - loss: 6.6021 - acc: 0.588 - ETA: 1s - loss: 6.4557 - acc: 0.596 - ETA: 1s - loss: 6.6460 - acc: 0.584 - ETA: 1s - loss: 6.5612 - acc: 0.589 - ETA: 1s - loss: 6.5823 - acc: 0.588 - ETA: 1s - loss: 6.5186 - acc: 0.592 - ETA: 1s - loss: 6.5474 - acc: 0.591 - ETA: 1s - loss: 6.6046 - acc: 0.587 - ETA: 1s - loss: 6.6614 - acc: 0.583 - ETA: 1s - loss: 6.7002 - acc: 0.581 - ETA: 1s - loss: 6.7358 - acc: 0.579 - ETA: 1s - loss: 6.7349 - acc: 0.579 - ETA: 1s - loss: 6.7562 - acc: 0.577 - ETA: 1s - loss: 6.7750 - acc: 0.576 - ETA: 0s - loss: 6.7570 - acc: 0.578 - ETA: 0s - loss: 6.7978 - acc: 0.575 - ETA: 0s - loss: 6.7962 - acc: 0.575 - ETA: 0s - loss: 6.7913 - acc: 0.575 - ETA: 0s - loss: 6.7595 - acc: 0.577 - ETA: 0s - loss: 6.7580 - acc: 0.577 - ETA: 0s - loss: 6.7357 - acc: 0.578 - ETA: 0s - loss: 6.7458 - acc: 0.578 - ETA: 0s - loss: 6.7483 - acc: 0.578 - ETA: 0s - loss: 6.7224 - acc: 0.579 - ETA: 0s - loss: 6.7207 - acc: 0.579 - ETA: 0s - loss: 6.7397 - acc: 0.578 - ETA: 0s - loss: 6.7389 - acc: 0.578 - ETA: 0s - loss: 6.7359 - acc: 0.578 - ETA: 0s - loss: 6.7422 - acc: 0.577 - ETA: 0s - loss: 6.7490 - acc: 0.577 - ETA: 0s - loss: 6.7196 - acc: 0.579 - ETA: 0s - loss: 6.7231 - acc: 0.579 - ETA: 0s - loss: 6.7334 - acc: 0.578 - ETA: 0s - loss: 6.7219 - acc: 0.5790Epoch 00019: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s 310us/step - loss: 6.7283 - acc: 0.5786 - val_loss: 7.7245 - val_acc: 0.4371\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6560/6680 [============================>.] - ETA: 2s - loss: 4.8362 - acc: 0.700 - ETA: 1s - loss: 6.2069 - acc: 0.615 - ETA: 2s - loss: 6.7304 - acc: 0.577 - ETA: 1s - loss: 6.7266 - acc: 0.579 - ETA: 1s - loss: 6.7110 - acc: 0.580 - ETA: 1s - loss: 6.7854 - acc: 0.576 - ETA: 1s - loss: 6.8275 - acc: 0.574 - ETA: 1s - loss: 6.8884 - acc: 0.570 - ETA: 1s - loss: 6.8109 - acc: 0.575 - ETA: 1s - loss: 6.7525 - acc: 0.579 - ETA: 1s - loss: 6.6720 - acc: 0.583 - ETA: 1s - loss: 6.7318 - acc: 0.579 - ETA: 1s - loss: 6.7726 - acc: 0.575 - ETA: 1s - loss: 6.7754 - acc: 0.575 - ETA: 1s - loss: 6.8421 - acc: 0.571 - ETA: 1s - loss: 6.8151 - acc: 0.573 - ETA: 1s - loss: 6.7286 - acc: 0.578 - ETA: 1s - loss: 6.7358 - acc: 0.578 - ETA: 1s - loss: 6.7439 - acc: 0.576 - ETA: 0s - loss: 6.7429 - acc: 0.576 - ETA: 0s - loss: 6.7438 - acc: 0.576 - ETA: 0s - loss: 6.7367 - acc: 0.577 - ETA: 0s - loss: 6.7442 - acc: 0.577 - ETA: 0s - loss: 6.7036 - acc: 0.579 - ETA: 0s - loss: 6.7457 - acc: 0.577 - ETA: 0s - loss: 6.7664 - acc: 0.576 - ETA: 0s - loss: 6.7554 - acc: 0.577 - ETA: 0s - loss: 6.7776 - acc: 0.575 - ETA: 0s - loss: 6.7787 - acc: 0.576 - ETA: 0s - loss: 6.7389 - acc: 0.578 - ETA: 0s - loss: 6.7262 - acc: 0.579 - ETA: 0s - loss: 6.7588 - acc: 0.577 - ETA: 0s - loss: 6.7351 - acc: 0.578 - ETA: 0s - loss: 6.7100 - acc: 0.580 - ETA: 0s - loss: 6.7316 - acc: 0.579 - ETA: 0s - loss: 6.7315 - acc: 0.579 - ETA: 0s - loss: 6.7261 - acc: 0.579 - ETA: 0s - loss: 6.7272 - acc: 0.5793Epoch 00020: val_loss improved from 7.72155 to 7.69368, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 2s 319us/step - loss: 6.7206 - acc: 0.5796 - val_loss: 7.6937 - val_acc: 0.4383\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aaaa669d68>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG16.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "VGG16_model.fit(train_VGG16, train_targets, \n",
    "          validation_data=(valid_VGG16, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VGG16_model.load_weights('saved_models/weights.best.VGG16.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 46.6507%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Dog Breed with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from extract_bottleneck_features import *\n",
    "\n",
    "def VGG16_predict_breed(img_path):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_VGG16(path_to_tensor(img_path))\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = VGG16_model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step5'></a>\n",
    "## Step 5: Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "\n",
    "You will now use transfer learning to create a CNN that can identify dog breed from images.  Your CNN must attain at least 60% accuracy on the test set.\n",
    "\n",
    "In Step 4, we used transfer learning to create a CNN using VGG-16 bottleneck features.  In this section, you must use the bottleneck features from a different pre-trained model.  To make things easier for you, we have pre-computed the features for all of the networks that are currently available in Keras:\n",
    "- [VGG-19](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG19Data.npz) bottleneck features\n",
    "- [ResNet-50](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogResnet50Data.npz) bottleneck features\n",
    "- [Inception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogInceptionV3Data.npz) bottleneck features\n",
    "- [Xception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogXceptionData.npz) bottleneck features\n",
    "\n",
    "The files are encoded as such:\n",
    "\n",
    "    Dog{network}Data.npz\n",
    "    \n",
    "where `{network}`, in the above filename, can be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`.  Pick one of the above architectures, download the corresponding bottleneck features, and store the downloaded file in the `bottleneck_features/` folder in the repository.\n",
    "\n",
    "### (IMPLEMENTATION) Obtain Bottleneck Features\n",
    "\n",
    "In the code block below, extract the bottleneck features corresponding to the train, test, and validation sets by running the following:\n",
    "\n",
    "    bottleneck_features = np.load('bottleneck_features/Dog{network}Data.npz')\n",
    "    train_{network} = bottleneck_features['train']\n",
    "    valid_{network} = bottleneck_features['valid']\n",
    "    test_{network} = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Obtain bottleneck features from another pre-trained CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get bottleneck features from InceptionV3\n",
    "\n",
    "bottleneck_features = np.load('bottleneck_features/DogInceptionV3Data.npz')\n",
    "train_InceptionV3 = bottleneck_features['train']\n",
    "valid_InceptionV3 = bottleneck_features['valid']\n",
    "test_InceptionV3 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        <your model's name>.summary()\n",
    "   \n",
    "__Question 5:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem.\n",
    "\n",
    "__Answer:__ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying the same architecture as previous model(VGG16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_3 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 133)               272517    \n",
      "=================================================================\n",
      "Total params: 272,517\n",
      "Trainable params: 272,517\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create a model\n",
    "InceptionV3_model = Sequential()\n",
    "\n",
    "# add a global average pooling layer to prevent overfitting (same as VGG16)\n",
    "InceptionV3_model.add(GlobalAveragePooling2D(input_shape=train_InceptionV3.shape[1:]))\n",
    "\n",
    "# softmax layer with 133 output nodes (because there are 133 classes to classify) - same as VGG16\n",
    "InceptionV3_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "InceptionV3_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile\n",
    "InceptionV3_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6620/6680 [============================>.] - ETA: 3:17 - loss: 5.4235 - acc: 0.0000e+0 - ETA: 53s - loss: 5.5007 - acc: 0.0000e+0 - ETA: 28s - loss: 5.5345 - acc: 0.0438   - ETA: 20s - loss: 5.5569 - acc: 0.06 - ETA: 16s - loss: 5.2769 - acc: 0.10 - ETA: 13s - loss: 5.1413 - acc: 0.13 - ETA: 11s - loss: 4.8716 - acc: 0.18 - ETA: 10s - loss: 4.6345 - acc: 0.21 - ETA: 9s - loss: 4.3860 - acc: 0.2391 - ETA: 9s - loss: 4.1556 - acc: 0.272 - ETA: 8s - loss: 3.9114 - acc: 0.302 - ETA: 7s - loss: 3.7108 - acc: 0.328 - ETA: 7s - loss: 3.5272 - acc: 0.357 - ETA: 7s - loss: 3.3700 - acc: 0.380 - ETA: 6s - loss: 3.2068 - acc: 0.407 - ETA: 6s - loss: 3.0581 - acc: 0.428 - ETA: 6s - loss: 2.9483 - acc: 0.444 - ETA: 5s - loss: 2.8523 - acc: 0.458 - ETA: 5s - loss: 2.7469 - acc: 0.472 - ETA: 5s - loss: 2.6615 - acc: 0.484 - ETA: 5s - loss: 2.5850 - acc: 0.493 - ETA: 5s - loss: 2.5128 - acc: 0.501 - ETA: 4s - loss: 2.4261 - acc: 0.512 - ETA: 4s - loss: 2.3651 - acc: 0.522 - ETA: 4s - loss: 2.3049 - acc: 0.530 - ETA: 4s - loss: 2.2491 - acc: 0.539 - ETA: 4s - loss: 2.1937 - acc: 0.547 - ETA: 4s - loss: 2.1484 - acc: 0.552 - ETA: 4s - loss: 2.1118 - acc: 0.558 - ETA: 3s - loss: 2.0558 - acc: 0.566 - ETA: 3s - loss: 2.0180 - acc: 0.572 - ETA: 3s - loss: 1.9726 - acc: 0.580 - ETA: 3s - loss: 1.9502 - acc: 0.583 - ETA: 3s - loss: 1.9201 - acc: 0.587 - ETA: 3s - loss: 1.8833 - acc: 0.594 - ETA: 3s - loss: 1.8517 - acc: 0.598 - ETA: 3s - loss: 1.8238 - acc: 0.604 - ETA: 3s - loss: 1.7961 - acc: 0.608 - ETA: 3s - loss: 1.7761 - acc: 0.612 - ETA: 2s - loss: 1.7490 - acc: 0.617 - ETA: 2s - loss: 1.7287 - acc: 0.620 - ETA: 2s - loss: 1.7139 - acc: 0.622 - ETA: 2s - loss: 1.6989 - acc: 0.624 - ETA: 2s - loss: 1.6753 - acc: 0.627 - ETA: 2s - loss: 1.6569 - acc: 0.630 - ETA: 2s - loss: 1.6424 - acc: 0.632 - ETA: 2s - loss: 1.6232 - acc: 0.635 - ETA: 2s - loss: 1.5999 - acc: 0.640 - ETA: 2s - loss: 1.5794 - acc: 0.643 - ETA: 2s - loss: 1.5645 - acc: 0.645 - ETA: 2s - loss: 1.5504 - acc: 0.647 - ETA: 2s - loss: 1.5343 - acc: 0.649 - ETA: 1s - loss: 1.5160 - acc: 0.652 - ETA: 1s - loss: 1.5034 - acc: 0.653 - ETA: 1s - loss: 1.4867 - acc: 0.656 - ETA: 1s - loss: 1.4712 - acc: 0.657 - ETA: 1s - loss: 1.4575 - acc: 0.660 - ETA: 1s - loss: 1.4455 - acc: 0.662 - ETA: 1s - loss: 1.4320 - acc: 0.665 - ETA: 1s - loss: 1.4223 - acc: 0.666 - ETA: 1s - loss: 1.4091 - acc: 0.669 - ETA: 1s - loss: 1.3980 - acc: 0.670 - ETA: 1s - loss: 1.3818 - acc: 0.673 - ETA: 1s - loss: 1.3686 - acc: 0.676 - ETA: 1s - loss: 1.3565 - acc: 0.678 - ETA: 1s - loss: 1.3478 - acc: 0.680 - ETA: 0s - loss: 1.3381 - acc: 0.681 - ETA: 0s - loss: 1.3270 - acc: 0.683 - ETA: 0s - loss: 1.3177 - acc: 0.685 - ETA: 0s - loss: 1.3055 - acc: 0.687 - ETA: 0s - loss: 1.2958 - acc: 0.689 - ETA: 0s - loss: 1.2875 - acc: 0.690 - ETA: 0s - loss: 1.2809 - acc: 0.692 - ETA: 0s - loss: 1.2744 - acc: 0.693 - ETA: 0s - loss: 1.2657 - acc: 0.695 - ETA: 0s - loss: 1.2593 - acc: 0.696 - ETA: 0s - loss: 1.2528 - acc: 0.697 - ETA: 0s - loss: 1.2440 - acc: 0.698 - ETA: 0s - loss: 1.2367 - acc: 0.700 - ETA: 0s - loss: 1.2349 - acc: 0.701 - ETA: 0s - loss: 1.2257 - acc: 0.7030Epoch 00001: val_loss improved from inf to 0.66906, saving model to saved_models/weights.best.InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 6s 826us/step - loss: 1.2188 - acc: 0.7048 - val_loss: 0.6691 - val_acc: 0.8060\n",
      "Epoch 2/20\n",
      "6640/6680 [============================>.] - ETA: 4s - loss: 0.2137 - acc: 0.900 - ETA: 4s - loss: 0.2185 - acc: 0.920 - ETA: 4s - loss: 0.3677 - acc: 0.880 - ETA: 4s - loss: 0.3590 - acc: 0.880 - ETA: 3s - loss: 0.3587 - acc: 0.882 - ETA: 3s - loss: 0.3893 - acc: 0.881 - ETA: 3s - loss: 0.3662 - acc: 0.889 - ETA: 3s - loss: 0.3708 - acc: 0.889 - ETA: 3s - loss: 0.3685 - acc: 0.887 - ETA: 3s - loss: 0.3596 - acc: 0.890 - ETA: 3s - loss: 0.3575 - acc: 0.890 - ETA: 3s - loss: 0.3681 - acc: 0.886 - ETA: 3s - loss: 0.3747 - acc: 0.882 - ETA: 3s - loss: 0.3745 - acc: 0.881 - ETA: 3s - loss: 0.3805 - acc: 0.880 - ETA: 3s - loss: 0.3764 - acc: 0.883 - ETA: 3s - loss: 0.3731 - acc: 0.884 - ETA: 3s - loss: 0.3688 - acc: 0.885 - ETA: 3s - loss: 0.3623 - acc: 0.886 - ETA: 3s - loss: 0.3640 - acc: 0.886 - ETA: 3s - loss: 0.3585 - acc: 0.886 - ETA: 3s - loss: 0.3710 - acc: 0.884 - ETA: 3s - loss: 0.3717 - acc: 0.882 - ETA: 2s - loss: 0.3637 - acc: 0.886 - ETA: 2s - loss: 0.3610 - acc: 0.886 - ETA: 2s - loss: 0.3634 - acc: 0.886 - ETA: 2s - loss: 0.3681 - acc: 0.884 - ETA: 2s - loss: 0.3763 - acc: 0.882 - ETA: 2s - loss: 0.3801 - acc: 0.880 - ETA: 2s - loss: 0.3826 - acc: 0.879 - ETA: 2s - loss: 0.3869 - acc: 0.877 - ETA: 2s - loss: 0.3879 - acc: 0.877 - ETA: 2s - loss: 0.3905 - acc: 0.877 - ETA: 2s - loss: 0.3937 - acc: 0.876 - ETA: 2s - loss: 0.3949 - acc: 0.876 - ETA: 2s - loss: 0.3924 - acc: 0.877 - ETA: 2s - loss: 0.3904 - acc: 0.877 - ETA: 2s - loss: 0.3968 - acc: 0.875 - ETA: 2s - loss: 0.3938 - acc: 0.876 - ETA: 2s - loss: 0.3896 - acc: 0.877 - ETA: 2s - loss: 0.3918 - acc: 0.875 - ETA: 1s - loss: 0.3881 - acc: 0.876 - ETA: 1s - loss: 0.3887 - acc: 0.877 - ETA: 1s - loss: 0.3899 - acc: 0.876 - ETA: 1s - loss: 0.3873 - acc: 0.877 - ETA: 1s - loss: 0.3852 - acc: 0.879 - ETA: 1s - loss: 0.3861 - acc: 0.879 - ETA: 1s - loss: 0.3881 - acc: 0.879 - ETA: 1s - loss: 0.3913 - acc: 0.877 - ETA: 1s - loss: 0.3896 - acc: 0.877 - ETA: 1s - loss: 0.3926 - acc: 0.877 - ETA: 1s - loss: 0.3943 - acc: 0.877 - ETA: 1s - loss: 0.3934 - acc: 0.877 - ETA: 1s - loss: 0.3921 - acc: 0.877 - ETA: 1s - loss: 0.3905 - acc: 0.877 - ETA: 1s - loss: 0.3933 - acc: 0.877 - ETA: 1s - loss: 0.3955 - acc: 0.876 - ETA: 1s - loss: 0.3931 - acc: 0.877 - ETA: 1s - loss: 0.3919 - acc: 0.878 - ETA: 0s - loss: 0.3889 - acc: 0.879 - ETA: 0s - loss: 0.3879 - acc: 0.879 - ETA: 0s - loss: 0.3883 - acc: 0.879 - ETA: 0s - loss: 0.3870 - acc: 0.879 - ETA: 0s - loss: 0.3902 - acc: 0.879 - ETA: 0s - loss: 0.3922 - acc: 0.878 - ETA: 0s - loss: 0.3959 - acc: 0.877 - ETA: 0s - loss: 0.3965 - acc: 0.877 - ETA: 0s - loss: 0.3990 - acc: 0.876 - ETA: 0s - loss: 0.4022 - acc: 0.876 - ETA: 0s - loss: 0.4029 - acc: 0.876 - ETA: 0s - loss: 0.4037 - acc: 0.875 - ETA: 0s - loss: 0.4038 - acc: 0.876 - ETA: 0s - loss: 0.4030 - acc: 0.876 - ETA: 0s - loss: 0.4043 - acc: 0.875 - ETA: 0s - loss: 0.4041 - acc: 0.875 - ETA: 0s - loss: 0.4058 - acc: 0.875 - ETA: 0s - loss: 0.4044 - acc: 0.8753Epoch 00002: val_loss improved from 0.66906 to 0.65182, saving model to saved_models/weights.best.InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 4s 667us/step - loss: 0.4094 - acc: 0.8744 - val_loss: 0.6518 - val_acc: 0.8120\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 4s - loss: 0.2245 - acc: 0.900 - ETA: 3s - loss: 0.2408 - acc: 0.908 - ETA: 3s - loss: 0.2560 - acc: 0.904 - ETA: 3s - loss: 0.2437 - acc: 0.918 - ETA: 3s - loss: 0.2603 - acc: 0.921 - ETA: 3s - loss: 0.2580 - acc: 0.920 - ETA: 3s - loss: 0.2514 - acc: 0.919 - ETA: 3s - loss: 0.2434 - acc: 0.921 - ETA: 3s - loss: 0.2391 - acc: 0.920 - ETA: 3s - loss: 0.2323 - acc: 0.922 - ETA: 3s - loss: 0.2274 - acc: 0.923 - ETA: 3s - loss: 0.2274 - acc: 0.925 - ETA: 3s - loss: 0.2252 - acc: 0.925 - ETA: 3s - loss: 0.2234 - acc: 0.925 - ETA: 3s - loss: 0.2339 - acc: 0.923 - ETA: 3s - loss: 0.2359 - acc: 0.923 - ETA: 3s - loss: 0.2290 - acc: 0.925 - ETA: 3s - loss: 0.2229 - acc: 0.926 - ETA: 3s - loss: 0.2189 - acc: 0.927 - ETA: 3s - loss: 0.2156 - acc: 0.928 - ETA: 3s - loss: 0.2218 - acc: 0.926 - ETA: 3s - loss: 0.2235 - acc: 0.925 - ETA: 2s - loss: 0.2217 - acc: 0.924 - ETA: 2s - loss: 0.2183 - acc: 0.925 - ETA: 2s - loss: 0.2153 - acc: 0.925 - ETA: 2s - loss: 0.2138 - acc: 0.925 - ETA: 2s - loss: 0.2130 - acc: 0.926 - ETA: 2s - loss: 0.2128 - acc: 0.927 - ETA: 2s - loss: 0.2140 - acc: 0.926 - ETA: 2s - loss: 0.2122 - acc: 0.927 - ETA: 2s - loss: 0.2128 - acc: 0.927 - ETA: 2s - loss: 0.2131 - acc: 0.927 - ETA: 2s - loss: 0.2129 - acc: 0.928 - ETA: 2s - loss: 0.2104 - acc: 0.929 - ETA: 2s - loss: 0.2128 - acc: 0.928 - ETA: 2s - loss: 0.2130 - acc: 0.928 - ETA: 2s - loss: 0.2100 - acc: 0.930 - ETA: 2s - loss: 0.2107 - acc: 0.930 - ETA: 2s - loss: 0.2107 - acc: 0.929 - ETA: 2s - loss: 0.2117 - acc: 0.930 - ETA: 2s - loss: 0.2141 - acc: 0.928 - ETA: 1s - loss: 0.2135 - acc: 0.929 - ETA: 1s - loss: 0.2126 - acc: 0.930 - ETA: 1s - loss: 0.2130 - acc: 0.929 - ETA: 1s - loss: 0.2127 - acc: 0.930 - ETA: 1s - loss: 0.2122 - acc: 0.930 - ETA: 1s - loss: 0.2162 - acc: 0.929 - ETA: 1s - loss: 0.2173 - acc: 0.928 - ETA: 1s - loss: 0.2219 - acc: 0.928 - ETA: 1s - loss: 0.2223 - acc: 0.927 - ETA: 1s - loss: 0.2220 - acc: 0.927 - ETA: 1s - loss: 0.2240 - acc: 0.927 - ETA: 1s - loss: 0.2243 - acc: 0.926 - ETA: 1s - loss: 0.2231 - acc: 0.927 - ETA: 1s - loss: 0.2242 - acc: 0.926 - ETA: 1s - loss: 0.2240 - acc: 0.925 - ETA: 1s - loss: 0.2233 - acc: 0.926 - ETA: 1s - loss: 0.2267 - acc: 0.924 - ETA: 0s - loss: 0.2267 - acc: 0.924 - ETA: 0s - loss: 0.2292 - acc: 0.922 - ETA: 0s - loss: 0.2289 - acc: 0.922 - ETA: 0s - loss: 0.2311 - acc: 0.922 - ETA: 0s - loss: 0.2316 - acc: 0.922 - ETA: 0s - loss: 0.2304 - acc: 0.922 - ETA: 0s - loss: 0.2303 - acc: 0.922 - ETA: 0s - loss: 0.2304 - acc: 0.922 - ETA: 0s - loss: 0.2303 - acc: 0.922 - ETA: 0s - loss: 0.2297 - acc: 0.923 - ETA: 0s - loss: 0.2307 - acc: 0.922 - ETA: 0s - loss: 0.2309 - acc: 0.922 - ETA: 0s - loss: 0.2317 - acc: 0.921 - ETA: 0s - loss: 0.2330 - acc: 0.921 - ETA: 0s - loss: 0.2337 - acc: 0.921 - ETA: 0s - loss: 0.2349 - acc: 0.920 - ETA: 0s - loss: 0.2351 - acc: 0.920 - ETA: 0s - loss: 0.2352 - acc: 0.920 - ETA: 0s - loss: 0.2378 - acc: 0.9194Epoch 00003: val_loss improved from 0.65182 to 0.60987, saving model to saved_models/weights.best.InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 4s 667us/step - loss: 0.2388 - acc: 0.9192 - val_loss: 0.6099 - val_acc: 0.8192\n",
      "Epoch 4/20\n",
      "6640/6680 [============================>.] - ETA: 4s - loss: 0.3187 - acc: 0.950 - ETA: 3s - loss: 0.1473 - acc: 0.966 - ETA: 3s - loss: 0.1491 - acc: 0.954 - ETA: 3s - loss: 0.1477 - acc: 0.953 - ETA: 3s - loss: 0.1499 - acc: 0.952 - ETA: 3s - loss: 0.1534 - acc: 0.954 - ETA: 3s - loss: 0.1383 - acc: 0.960 - ETA: 3s - loss: 0.1499 - acc: 0.957 - ETA: 3s - loss: 0.1516 - acc: 0.957 - ETA: 3s - loss: 0.1487 - acc: 0.958 - ETA: 3s - loss: 0.1551 - acc: 0.954 - ETA: 3s - loss: 0.1582 - acc: 0.954 - ETA: 3s - loss: 0.1546 - acc: 0.955 - ETA: 3s - loss: 0.1592 - acc: 0.955 - ETA: 3s - loss: 0.1582 - acc: 0.953 - ETA: 3s - loss: 0.1623 - acc: 0.950 - ETA: 3s - loss: 0.1646 - acc: 0.950 - ETA: 3s - loss: 0.1623 - acc: 0.951 - ETA: 3s - loss: 0.1614 - acc: 0.953 - ETA: 3s - loss: 0.1584 - acc: 0.954 - ETA: 3s - loss: 0.1589 - acc: 0.954 - ETA: 2s - loss: 0.1596 - acc: 0.953 - ETA: 2s - loss: 0.1600 - acc: 0.953 - ETA: 2s - loss: 0.1566 - acc: 0.954 - ETA: 2s - loss: 0.1556 - acc: 0.954 - ETA: 2s - loss: 0.1549 - acc: 0.954 - ETA: 2s - loss: 0.1589 - acc: 0.953 - ETA: 2s - loss: 0.1551 - acc: 0.955 - ETA: 2s - loss: 0.1522 - acc: 0.956 - ETA: 2s - loss: 0.1505 - acc: 0.956 - ETA: 2s - loss: 0.1481 - acc: 0.957 - ETA: 2s - loss: 0.1508 - acc: 0.957 - ETA: 2s - loss: 0.1480 - acc: 0.958 - ETA: 2s - loss: 0.1465 - acc: 0.958 - ETA: 2s - loss: 0.1485 - acc: 0.957 - ETA: 2s - loss: 0.1490 - acc: 0.957 - ETA: 2s - loss: 0.1477 - acc: 0.957 - ETA: 2s - loss: 0.1480 - acc: 0.957 - ETA: 2s - loss: 0.1482 - acc: 0.957 - ETA: 1s - loss: 0.1468 - acc: 0.958 - ETA: 1s - loss: 0.1467 - acc: 0.957 - ETA: 1s - loss: 0.1453 - acc: 0.958 - ETA: 1s - loss: 0.1457 - acc: 0.958 - ETA: 1s - loss: 0.1445 - acc: 0.958 - ETA: 1s - loss: 0.1454 - acc: 0.958 - ETA: 1s - loss: 0.1454 - acc: 0.958 - ETA: 1s - loss: 0.1448 - acc: 0.958 - ETA: 1s - loss: 0.1459 - acc: 0.957 - ETA: 1s - loss: 0.1465 - acc: 0.957 - ETA: 1s - loss: 0.1464 - acc: 0.957 - ETA: 1s - loss: 0.1483 - acc: 0.956 - ETA: 1s - loss: 0.1494 - acc: 0.955 - ETA: 1s - loss: 0.1491 - acc: 0.955 - ETA: 1s - loss: 0.1499 - acc: 0.955 - ETA: 1s - loss: 0.1487 - acc: 0.955 - ETA: 1s - loss: 0.1489 - acc: 0.954 - ETA: 0s - loss: 0.1497 - acc: 0.954 - ETA: 0s - loss: 0.1496 - acc: 0.954 - ETA: 0s - loss: 0.1493 - acc: 0.954 - ETA: 0s - loss: 0.1498 - acc: 0.953 - ETA: 0s - loss: 0.1499 - acc: 0.953 - ETA: 0s - loss: 0.1519 - acc: 0.952 - ETA: 0s - loss: 0.1514 - acc: 0.952 - ETA: 0s - loss: 0.1529 - acc: 0.952 - ETA: 0s - loss: 0.1531 - acc: 0.952 - ETA: 0s - loss: 0.1529 - acc: 0.952 - ETA: 0s - loss: 0.1530 - acc: 0.952 - ETA: 0s - loss: 0.1528 - acc: 0.953 - ETA: 0s - loss: 0.1522 - acc: 0.953 - ETA: 0s - loss: 0.1522 - acc: 0.953 - ETA: 0s - loss: 0.1519 - acc: 0.953 - ETA: 0s - loss: 0.1531 - acc: 0.953 - ETA: 0s - loss: 0.1545 - acc: 0.952 - ETA: 0s - loss: 0.1543 - acc: 0.9524Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s 643us/step - loss: 0.1545 - acc: 0.9522 - val_loss: 0.6392 - val_acc: 0.8419\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 4s - loss: 0.0354 - acc: 1.000 - ETA: 3s - loss: 0.0945 - acc: 0.983 - ETA: 3s - loss: 0.0895 - acc: 0.986 - ETA: 3s - loss: 0.0920 - acc: 0.987 - ETA: 3s - loss: 0.0951 - acc: 0.981 - ETA: 3s - loss: 0.1015 - acc: 0.978 - ETA: 3s - loss: 0.0998 - acc: 0.979 - ETA: 3s - loss: 0.0975 - acc: 0.977 - ETA: 3s - loss: 0.0941 - acc: 0.976 - ETA: 3s - loss: 0.0965 - acc: 0.973 - ETA: 3s - loss: 0.0952 - acc: 0.972 - ETA: 3s - loss: 0.0983 - acc: 0.971 - ETA: 3s - loss: 0.0990 - acc: 0.970 - ETA: 3s - loss: 0.0953 - acc: 0.971 - ETA: 3s - loss: 0.0969 - acc: 0.970 - ETA: 3s - loss: 0.0968 - acc: 0.971 - ETA: 3s - loss: 0.0940 - acc: 0.973 - ETA: 3s - loss: 0.0954 - acc: 0.973 - ETA: 3s - loss: 0.0936 - acc: 0.974 - ETA: 3s - loss: 0.0920 - acc: 0.974 - ETA: 2s - loss: 0.0921 - acc: 0.973 - ETA: 2s - loss: 0.0905 - acc: 0.975 - ETA: 2s - loss: 0.0920 - acc: 0.973 - ETA: 2s - loss: 0.0919 - acc: 0.973 - ETA: 2s - loss: 0.0917 - acc: 0.974 - ETA: 2s - loss: 0.0939 - acc: 0.973 - ETA: 2s - loss: 0.0938 - acc: 0.973 - ETA: 2s - loss: 0.0924 - acc: 0.974 - ETA: 2s - loss: 0.0932 - acc: 0.974 - ETA: 2s - loss: 0.0930 - acc: 0.973 - ETA: 2s - loss: 0.0924 - acc: 0.973 - ETA: 2s - loss: 0.0921 - acc: 0.973 - ETA: 2s - loss: 0.0960 - acc: 0.972 - ETA: 2s - loss: 0.0964 - acc: 0.972 - ETA: 2s - loss: 0.0975 - acc: 0.972 - ETA: 2s - loss: 0.0964 - acc: 0.972 - ETA: 2s - loss: 0.0957 - acc: 0.972 - ETA: 2s - loss: 0.0962 - acc: 0.972 - ETA: 1s - loss: 0.0959 - acc: 0.972 - ETA: 1s - loss: 0.0960 - acc: 0.972 - ETA: 1s - loss: 0.0963 - acc: 0.973 - ETA: 1s - loss: 0.0982 - acc: 0.971 - ETA: 1s - loss: 0.0987 - acc: 0.972 - ETA: 1s - loss: 0.0993 - acc: 0.971 - ETA: 1s - loss: 0.0995 - acc: 0.971 - ETA: 1s - loss: 0.0990 - acc: 0.971 - ETA: 1s - loss: 0.0983 - acc: 0.971 - ETA: 1s - loss: 0.0991 - acc: 0.971 - ETA: 1s - loss: 0.0989 - acc: 0.971 - ETA: 1s - loss: 0.0997 - acc: 0.970 - ETA: 1s - loss: 0.0994 - acc: 0.970 - ETA: 1s - loss: 0.1020 - acc: 0.969 - ETA: 1s - loss: 0.1024 - acc: 0.969 - ETA: 1s - loss: 0.1026 - acc: 0.968 - ETA: 1s - loss: 0.1023 - acc: 0.968 - ETA: 1s - loss: 0.1018 - acc: 0.969 - ETA: 0s - loss: 0.1018 - acc: 0.968 - ETA: 0s - loss: 0.1013 - acc: 0.968 - ETA: 0s - loss: 0.1003 - acc: 0.969 - ETA: 0s - loss: 0.1008 - acc: 0.969 - ETA: 0s - loss: 0.1004 - acc: 0.969 - ETA: 0s - loss: 0.0993 - acc: 0.970 - ETA: 0s - loss: 0.0997 - acc: 0.969 - ETA: 0s - loss: 0.1006 - acc: 0.969 - ETA: 0s - loss: 0.1004 - acc: 0.969 - ETA: 0s - loss: 0.1004 - acc: 0.969 - ETA: 0s - loss: 0.1012 - acc: 0.968 - ETA: 0s - loss: 0.1047 - acc: 0.967 - ETA: 0s - loss: 0.1046 - acc: 0.967 - ETA: 0s - loss: 0.1045 - acc: 0.967 - ETA: 0s - loss: 0.1051 - acc: 0.967 - ETA: 0s - loss: 0.1048 - acc: 0.967 - ETA: 0s - loss: 0.1045 - acc: 0.967 - ETA: 0s - loss: 0.1044 - acc: 0.9677Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s 641us/step - loss: 0.1042 - acc: 0.9678 - val_loss: 0.6525 - val_acc: 0.8251\n",
      "Epoch 6/20\n",
      "6640/6680 [============================>.] - ETA: 4s - loss: 0.1663 - acc: 0.950 - ETA: 3s - loss: 0.0594 - acc: 0.983 - ETA: 4s - loss: 0.0565 - acc: 0.990 - ETA: 4s - loss: 0.0648 - acc: 0.989 - ETA: 4s - loss: 0.0714 - acc: 0.983 - ETA: 3s - loss: 0.0715 - acc: 0.984 - ETA: 3s - loss: 0.0685 - acc: 0.984 - ETA: 3s - loss: 0.0757 - acc: 0.979 - ETA: 3s - loss: 0.0746 - acc: 0.979 - ETA: 3s - loss: 0.0757 - acc: 0.980 - ETA: 3s - loss: 0.0723 - acc: 0.982 - ETA: 3s - loss: 0.0744 - acc: 0.981 - ETA: 3s - loss: 0.0718 - acc: 0.982 - ETA: 3s - loss: 0.0713 - acc: 0.982 - ETA: 3s - loss: 0.0746 - acc: 0.980 - ETA: 3s - loss: 0.0756 - acc: 0.981 - ETA: 3s - loss: 0.0772 - acc: 0.980 - ETA: 3s - loss: 0.0762 - acc: 0.979 - ETA: 3s - loss: 0.0754 - acc: 0.979 - ETA: 3s - loss: 0.0732 - acc: 0.980 - ETA: 3s - loss: 0.0728 - acc: 0.979 - ETA: 3s - loss: 0.0710 - acc: 0.980 - ETA: 2s - loss: 0.0715 - acc: 0.980 - ETA: 2s - loss: 0.0707 - acc: 0.980 - ETA: 2s - loss: 0.0699 - acc: 0.981 - ETA: 2s - loss: 0.0691 - acc: 0.980 - ETA: 2s - loss: 0.0687 - acc: 0.981 - ETA: 2s - loss: 0.0702 - acc: 0.980 - ETA: 2s - loss: 0.0711 - acc: 0.979 - ETA: 2s - loss: 0.0699 - acc: 0.980 - ETA: 2s - loss: 0.0703 - acc: 0.980 - ETA: 2s - loss: 0.0690 - acc: 0.981 - ETA: 2s - loss: 0.0697 - acc: 0.980 - ETA: 2s - loss: 0.0689 - acc: 0.981 - ETA: 2s - loss: 0.0690 - acc: 0.981 - ETA: 2s - loss: 0.0687 - acc: 0.981 - ETA: 2s - loss: 0.0686 - acc: 0.981 - ETA: 2s - loss: 0.0684 - acc: 0.981 - ETA: 2s - loss: 0.0674 - acc: 0.981 - ETA: 1s - loss: 0.0678 - acc: 0.981 - ETA: 1s - loss: 0.0671 - acc: 0.981 - ETA: 1s - loss: 0.0675 - acc: 0.982 - ETA: 1s - loss: 0.0676 - acc: 0.981 - ETA: 1s - loss: 0.0665 - acc: 0.982 - ETA: 1s - loss: 0.0664 - acc: 0.982 - ETA: 1s - loss: 0.0654 - acc: 0.982 - ETA: 1s - loss: 0.0652 - acc: 0.982 - ETA: 1s - loss: 0.0670 - acc: 0.982 - ETA: 1s - loss: 0.0673 - acc: 0.981 - ETA: 1s - loss: 0.0670 - acc: 0.982 - ETA: 1s - loss: 0.0666 - acc: 0.982 - ETA: 1s - loss: 0.0684 - acc: 0.981 - ETA: 1s - loss: 0.0689 - acc: 0.981 - ETA: 1s - loss: 0.0692 - acc: 0.981 - ETA: 1s - loss: 0.0686 - acc: 0.981 - ETA: 1s - loss: 0.0689 - acc: 0.981 - ETA: 0s - loss: 0.0691 - acc: 0.981 - ETA: 0s - loss: 0.0698 - acc: 0.980 - ETA: 0s - loss: 0.0692 - acc: 0.980 - ETA: 0s - loss: 0.0690 - acc: 0.981 - ETA: 0s - loss: 0.0706 - acc: 0.980 - ETA: 0s - loss: 0.0710 - acc: 0.980 - ETA: 0s - loss: 0.0709 - acc: 0.980 - ETA: 0s - loss: 0.0712 - acc: 0.980 - ETA: 0s - loss: 0.0712 - acc: 0.980 - ETA: 0s - loss: 0.0712 - acc: 0.979 - ETA: 0s - loss: 0.0709 - acc: 0.980 - ETA: 0s - loss: 0.0714 - acc: 0.979 - ETA: 0s - loss: 0.0710 - acc: 0.979 - ETA: 0s - loss: 0.0713 - acc: 0.979 - ETA: 0s - loss: 0.0708 - acc: 0.979 - ETA: 0s - loss: 0.0712 - acc: 0.979 - ETA: 0s - loss: 0.0720 - acc: 0.979 - ETA: 0s - loss: 0.0715 - acc: 0.9800Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s 642us/step - loss: 0.0715 - acc: 0.9799 - val_loss: 0.6453 - val_acc: 0.8180\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 4s - loss: 0.2024 - acc: 0.900 - ETA: 3s - loss: 0.0676 - acc: 0.975 - ETA: 3s - loss: 0.0487 - acc: 0.985 - ETA: 3s - loss: 0.0638 - acc: 0.973 - ETA: 3s - loss: 0.0710 - acc: 0.971 - ETA: 3s - loss: 0.0730 - acc: 0.970 - ETA: 3s - loss: 0.0741 - acc: 0.970 - ETA: 3s - loss: 0.0693 - acc: 0.974 - ETA: 3s - loss: 0.0699 - acc: 0.975 - ETA: 3s - loss: 0.0709 - acc: 0.976 - ETA: 3s - loss: 0.0664 - acc: 0.978 - ETA: 3s - loss: 0.0656 - acc: 0.979 - ETA: 3s - loss: 0.0637 - acc: 0.980 - ETA: 3s - loss: 0.0620 - acc: 0.981 - ETA: 3s - loss: 0.0634 - acc: 0.980 - ETA: 3s - loss: 0.0612 - acc: 0.981 - ETA: 3s - loss: 0.0639 - acc: 0.980 - ETA: 3s - loss: 0.0624 - acc: 0.981 - ETA: 3s - loss: 0.0614 - acc: 0.981 - ETA: 2s - loss: 0.0596 - acc: 0.982 - ETA: 2s - loss: 0.0585 - acc: 0.982 - ETA: 2s - loss: 0.0581 - acc: 0.982 - ETA: 2s - loss: 0.0577 - acc: 0.982 - ETA: 2s - loss: 0.0566 - acc: 0.982 - ETA: 2s - loss: 0.0581 - acc: 0.981 - ETA: 2s - loss: 0.0590 - acc: 0.980 - ETA: 2s - loss: 0.0580 - acc: 0.981 - ETA: 2s - loss: 0.0588 - acc: 0.980 - ETA: 2s - loss: 0.0575 - acc: 0.981 - ETA: 2s - loss: 0.0566 - acc: 0.982 - ETA: 2s - loss: 0.0560 - acc: 0.982 - ETA: 2s - loss: 0.0550 - acc: 0.983 - ETA: 2s - loss: 0.0545 - acc: 0.983 - ETA: 2s - loss: 0.0545 - acc: 0.983 - ETA: 2s - loss: 0.0547 - acc: 0.982 - ETA: 2s - loss: 0.0545 - acc: 0.983 - ETA: 2s - loss: 0.0556 - acc: 0.982 - ETA: 1s - loss: 0.0550 - acc: 0.983 - ETA: 1s - loss: 0.0547 - acc: 0.982 - ETA: 1s - loss: 0.0557 - acc: 0.982 - ETA: 1s - loss: 0.0548 - acc: 0.982 - ETA: 1s - loss: 0.0554 - acc: 0.982 - ETA: 1s - loss: 0.0555 - acc: 0.982 - ETA: 1s - loss: 0.0558 - acc: 0.982 - ETA: 1s - loss: 0.0565 - acc: 0.983 - ETA: 1s - loss: 0.0563 - acc: 0.983 - ETA: 1s - loss: 0.0565 - acc: 0.983 - ETA: 1s - loss: 0.0561 - acc: 0.983 - ETA: 1s - loss: 0.0555 - acc: 0.983 - ETA: 1s - loss: 0.0549 - acc: 0.984 - ETA: 1s - loss: 0.0561 - acc: 0.983 - ETA: 1s - loss: 0.0568 - acc: 0.983 - ETA: 1s - loss: 0.0572 - acc: 0.983 - ETA: 1s - loss: 0.0572 - acc: 0.983 - ETA: 0s - loss: 0.0565 - acc: 0.983 - ETA: 0s - loss: 0.0573 - acc: 0.983 - ETA: 0s - loss: 0.0575 - acc: 0.983 - ETA: 0s - loss: 0.0580 - acc: 0.983 - ETA: 0s - loss: 0.0579 - acc: 0.983 - ETA: 0s - loss: 0.0581 - acc: 0.983 - ETA: 0s - loss: 0.0579 - acc: 0.983 - ETA: 0s - loss: 0.0587 - acc: 0.982 - ETA: 0s - loss: 0.0601 - acc: 0.982 - ETA: 0s - loss: 0.0604 - acc: 0.982 - ETA: 0s - loss: 0.0605 - acc: 0.982 - ETA: 0s - loss: 0.0607 - acc: 0.982 - ETA: 0s - loss: 0.0602 - acc: 0.982 - ETA: 0s - loss: 0.0614 - acc: 0.982 - ETA: 0s - loss: 0.0610 - acc: 0.982 - ETA: 0s - loss: 0.0616 - acc: 0.982 - ETA: 0s - loss: 0.0611 - acc: 0.982 - ETA: 0s - loss: 0.0613 - acc: 0.9822Epoch 00007: val_loss improved from 0.60987 to 0.59028, saving model to saved_models/weights.best.InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 4s 638us/step - loss: 0.0613 - acc: 0.9822 - val_loss: 0.5903 - val_acc: 0.8503\n",
      "Epoch 8/20\n",
      "6620/6680 [============================>.] - ETA: 4s - loss: 0.0172 - acc: 1.000 - ETA: 3s - loss: 0.0313 - acc: 0.991 - ETA: 3s - loss: 0.0418 - acc: 0.990 - ETA: 3s - loss: 0.0449 - acc: 0.990 - ETA: 3s - loss: 0.0453 - acc: 0.990 - ETA: 3s - loss: 0.0409 - acc: 0.992 - ETA: 3s - loss: 0.0425 - acc: 0.990 - ETA: 3s - loss: 0.0466 - acc: 0.987 - ETA: 3s - loss: 0.0437 - acc: 0.989 - ETA: 3s - loss: 0.0435 - acc: 0.990 - ETA: 3s - loss: 0.0412 - acc: 0.990 - ETA: 3s - loss: 0.0387 - acc: 0.991 - ETA: 3s - loss: 0.0386 - acc: 0.990 - ETA: 3s - loss: 0.0385 - acc: 0.991 - ETA: 3s - loss: 0.0386 - acc: 0.991 - ETA: 3s - loss: 0.0384 - acc: 0.991 - ETA: 3s - loss: 0.0380 - acc: 0.992 - ETA: 3s - loss: 0.0394 - acc: 0.990 - ETA: 2s - loss: 0.0386 - acc: 0.991 - ETA: 2s - loss: 0.0395 - acc: 0.990 - ETA: 2s - loss: 0.0378 - acc: 0.991 - ETA: 2s - loss: 0.0375 - acc: 0.991 - ETA: 2s - loss: 0.0386 - acc: 0.990 - ETA: 2s - loss: 0.0386 - acc: 0.990 - ETA: 2s - loss: 0.0383 - acc: 0.991 - ETA: 2s - loss: 0.0384 - acc: 0.991 - ETA: 2s - loss: 0.0396 - acc: 0.991 - ETA: 2s - loss: 0.0400 - acc: 0.990 - ETA: 2s - loss: 0.0399 - acc: 0.990 - ETA: 2s - loss: 0.0394 - acc: 0.991 - ETA: 2s - loss: 0.0390 - acc: 0.991 - ETA: 2s - loss: 0.0401 - acc: 0.990 - ETA: 2s - loss: 0.0395 - acc: 0.990 - ETA: 2s - loss: 0.0394 - acc: 0.990 - ETA: 2s - loss: 0.0393 - acc: 0.990 - ETA: 2s - loss: 0.0390 - acc: 0.990 - ETA: 1s - loss: 0.0389 - acc: 0.990 - ETA: 1s - loss: 0.0386 - acc: 0.990 - ETA: 1s - loss: 0.0404 - acc: 0.990 - ETA: 1s - loss: 0.0402 - acc: 0.991 - ETA: 1s - loss: 0.0395 - acc: 0.991 - ETA: 1s - loss: 0.0399 - acc: 0.990 - ETA: 1s - loss: 0.0397 - acc: 0.990 - ETA: 1s - loss: 0.0397 - acc: 0.990 - ETA: 1s - loss: 0.0393 - acc: 0.990 - ETA: 1s - loss: 0.0403 - acc: 0.990 - ETA: 1s - loss: 0.0399 - acc: 0.990 - ETA: 1s - loss: 0.0402 - acc: 0.990 - ETA: 1s - loss: 0.0421 - acc: 0.989 - ETA: 1s - loss: 0.0418 - acc: 0.990 - ETA: 1s - loss: 0.0416 - acc: 0.990 - ETA: 1s - loss: 0.0419 - acc: 0.989 - ETA: 1s - loss: 0.0423 - acc: 0.989 - ETA: 1s - loss: 0.0421 - acc: 0.989 - ETA: 0s - loss: 0.0420 - acc: 0.989 - ETA: 0s - loss: 0.0427 - acc: 0.989 - ETA: 0s - loss: 0.0437 - acc: 0.988 - ETA: 0s - loss: 0.0448 - acc: 0.988 - ETA: 0s - loss: 0.0445 - acc: 0.988 - ETA: 0s - loss: 0.0446 - acc: 0.988 - ETA: 0s - loss: 0.0452 - acc: 0.988 - ETA: 0s - loss: 0.0465 - acc: 0.988 - ETA: 0s - loss: 0.0469 - acc: 0.987 - ETA: 0s - loss: 0.0471 - acc: 0.987 - ETA: 0s - loss: 0.0468 - acc: 0.988 - ETA: 0s - loss: 0.0470 - acc: 0.988 - ETA: 0s - loss: 0.0473 - acc: 0.987 - ETA: 0s - loss: 0.0469 - acc: 0.987 - ETA: 0s - loss: 0.0474 - acc: 0.987 - ETA: 0s - loss: 0.0474 - acc: 0.987 - ETA: 0s - loss: 0.0478 - acc: 0.987 - ETA: 0s - loss: 0.0482 - acc: 0.9873Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s 632us/step - loss: 0.0482 - acc: 0.9873 - val_loss: 0.6618 - val_acc: 0.8443\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 4s - loss: 0.0192 - acc: 1.000 - ETA: 3s - loss: 0.0285 - acc: 1.000 - ETA: 3s - loss: 0.0466 - acc: 0.990 - ETA: 3s - loss: 0.0376 - acc: 0.993 - ETA: 3s - loss: 0.0333 - acc: 0.995 - ETA: 3s - loss: 0.0301 - acc: 0.996 - ETA: 3s - loss: 0.0333 - acc: 0.995 - ETA: 3s - loss: 0.0346 - acc: 0.994 - ETA: 3s - loss: 0.0376 - acc: 0.993 - ETA: 3s - loss: 0.0406 - acc: 0.993 - ETA: 3s - loss: 0.0392 - acc: 0.992 - ETA: 3s - loss: 0.0373 - acc: 0.993 - ETA: 3s - loss: 0.0388 - acc: 0.992 - ETA: 3s - loss: 0.0372 - acc: 0.992 - ETA: 3s - loss: 0.0362 - acc: 0.992 - ETA: 3s - loss: 0.0354 - acc: 0.992 - ETA: 3s - loss: 0.0355 - acc: 0.992 - ETA: 3s - loss: 0.0341 - acc: 0.992 - ETA: 2s - loss: 0.0336 - acc: 0.992 - ETA: 2s - loss: 0.0328 - acc: 0.992 - ETA: 2s - loss: 0.0329 - acc: 0.992 - ETA: 2s - loss: 0.0321 - acc: 0.993 - ETA: 2s - loss: 0.0340 - acc: 0.992 - ETA: 2s - loss: 0.0352 - acc: 0.991 - ETA: 2s - loss: 0.0350 - acc: 0.991 - ETA: 2s - loss: 0.0347 - acc: 0.991 - ETA: 2s - loss: 0.0347 - acc: 0.991 - ETA: 2s - loss: 0.0342 - acc: 0.991 - ETA: 2s - loss: 0.0342 - acc: 0.991 - ETA: 2s - loss: 0.0343 - acc: 0.992 - ETA: 2s - loss: 0.0344 - acc: 0.992 - ETA: 2s - loss: 0.0351 - acc: 0.991 - ETA: 2s - loss: 0.0346 - acc: 0.992 - ETA: 2s - loss: 0.0340 - acc: 0.992 - ETA: 2s - loss: 0.0349 - acc: 0.992 - ETA: 2s - loss: 0.0347 - acc: 0.992 - ETA: 2s - loss: 0.0357 - acc: 0.992 - ETA: 1s - loss: 0.0357 - acc: 0.992 - ETA: 1s - loss: 0.0350 - acc: 0.992 - ETA: 1s - loss: 0.0345 - acc: 0.992 - ETA: 1s - loss: 0.0342 - acc: 0.992 - ETA: 1s - loss: 0.0339 - acc: 0.992 - ETA: 1s - loss: 0.0345 - acc: 0.992 - ETA: 1s - loss: 0.0341 - acc: 0.992 - ETA: 1s - loss: 0.0354 - acc: 0.992 - ETA: 1s - loss: 0.0357 - acc: 0.992 - ETA: 1s - loss: 0.0358 - acc: 0.992 - ETA: 1s - loss: 0.0374 - acc: 0.991 - ETA: 1s - loss: 0.0373 - acc: 0.992 - ETA: 1s - loss: 0.0385 - acc: 0.991 - ETA: 1s - loss: 0.0385 - acc: 0.991 - ETA: 1s - loss: 0.0383 - acc: 0.991 - ETA: 1s - loss: 0.0381 - acc: 0.991 - ETA: 1s - loss: 0.0377 - acc: 0.991 - ETA: 1s - loss: 0.0374 - acc: 0.991 - ETA: 0s - loss: 0.0379 - acc: 0.991 - ETA: 0s - loss: 0.0377 - acc: 0.991 - ETA: 0s - loss: 0.0378 - acc: 0.991 - ETA: 0s - loss: 0.0375 - acc: 0.991 - ETA: 0s - loss: 0.0372 - acc: 0.991 - ETA: 0s - loss: 0.0373 - acc: 0.991 - ETA: 0s - loss: 0.0370 - acc: 0.991 - ETA: 0s - loss: 0.0369 - acc: 0.991 - ETA: 0s - loss: 0.0379 - acc: 0.991 - ETA: 0s - loss: 0.0377 - acc: 0.991 - ETA: 0s - loss: 0.0390 - acc: 0.991 - ETA: 0s - loss: 0.0387 - acc: 0.991 - ETA: 0s - loss: 0.0385 - acc: 0.991 - ETA: 0s - loss: 0.0387 - acc: 0.991 - ETA: 0s - loss: 0.0387 - acc: 0.991 - ETA: 0s - loss: 0.0387 - acc: 0.991 - ETA: 0s - loss: 0.0391 - acc: 0.991 - ETA: 0s - loss: 0.0390 - acc: 0.9911Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s 635us/step - loss: 0.0389 - acc: 0.9912 - val_loss: 0.6399 - val_acc: 0.8527\n",
      "Epoch 10/20\n",
      "6620/6680 [============================>.] - ETA: 4s - loss: 0.0130 - acc: 1.000 - ETA: 3s - loss: 0.0178 - acc: 1.000 - ETA: 3s - loss: 0.0246 - acc: 0.995 - ETA: 3s - loss: 0.0311 - acc: 0.992 - ETA: 4s - loss: 0.0350 - acc: 0.988 - ETA: 3s - loss: 0.0344 - acc: 0.988 - ETA: 3s - loss: 0.0337 - acc: 0.988 - ETA: 3s - loss: 0.0318 - acc: 0.988 - ETA: 3s - loss: 0.0342 - acc: 0.988 - ETA: 3s - loss: 0.0343 - acc: 0.988 - ETA: 3s - loss: 0.0336 - acc: 0.989 - ETA: 3s - loss: 0.0318 - acc: 0.990 - ETA: 3s - loss: 0.0313 - acc: 0.990 - ETA: 3s - loss: 0.0304 - acc: 0.991 - ETA: 3s - loss: 0.0297 - acc: 0.991 - ETA: 3s - loss: 0.0314 - acc: 0.991 - ETA: 3s - loss: 0.0299 - acc: 0.992 - ETA: 3s - loss: 0.0327 - acc: 0.991 - ETA: 3s - loss: 0.0315 - acc: 0.992 - ETA: 3s - loss: 0.0314 - acc: 0.991 - ETA: 2s - loss: 0.0308 - acc: 0.992 - ETA: 2s - loss: 0.0303 - acc: 0.992 - ETA: 2s - loss: 0.0300 - acc: 0.993 - ETA: 2s - loss: 0.0295 - acc: 0.993 - ETA: 2s - loss: 0.0298 - acc: 0.993 - ETA: 2s - loss: 0.0297 - acc: 0.993 - ETA: 2s - loss: 0.0314 - acc: 0.992 - ETA: 2s - loss: 0.0308 - acc: 0.993 - ETA: 2s - loss: 0.0299 - acc: 0.993 - ETA: 2s - loss: 0.0296 - acc: 0.993 - ETA: 2s - loss: 0.0308 - acc: 0.992 - ETA: 2s - loss: 0.0301 - acc: 0.993 - ETA: 2s - loss: 0.0298 - acc: 0.993 - ETA: 2s - loss: 0.0298 - acc: 0.993 - ETA: 2s - loss: 0.0306 - acc: 0.993 - ETA: 2s - loss: 0.0312 - acc: 0.993 - ETA: 2s - loss: 0.0307 - acc: 0.993 - ETA: 2s - loss: 0.0305 - acc: 0.993 - ETA: 1s - loss: 0.0303 - acc: 0.993 - ETA: 1s - loss: 0.0311 - acc: 0.993 - ETA: 1s - loss: 0.0308 - acc: 0.993 - ETA: 1s - loss: 0.0305 - acc: 0.993 - ETA: 1s - loss: 0.0307 - acc: 0.993 - ETA: 1s - loss: 0.0305 - acc: 0.993 - ETA: 1s - loss: 0.0304 - acc: 0.993 - ETA: 1s - loss: 0.0302 - acc: 0.993 - ETA: 1s - loss: 0.0311 - acc: 0.993 - ETA: 1s - loss: 0.0321 - acc: 0.992 - ETA: 1s - loss: 0.0319 - acc: 0.992 - ETA: 1s - loss: 0.0317 - acc: 0.992 - ETA: 1s - loss: 0.0327 - acc: 0.992 - ETA: 1s - loss: 0.0327 - acc: 0.992 - ETA: 1s - loss: 0.0326 - acc: 0.992 - ETA: 1s - loss: 0.0325 - acc: 0.992 - ETA: 1s - loss: 0.0326 - acc: 0.992 - ETA: 0s - loss: 0.0325 - acc: 0.992 - ETA: 0s - loss: 0.0322 - acc: 0.992 - ETA: 0s - loss: 0.0321 - acc: 0.992 - ETA: 0s - loss: 0.0326 - acc: 0.992 - ETA: 0s - loss: 0.0327 - acc: 0.992 - ETA: 0s - loss: 0.0328 - acc: 0.991 - ETA: 0s - loss: 0.0328 - acc: 0.991 - ETA: 0s - loss: 0.0326 - acc: 0.991 - ETA: 0s - loss: 0.0332 - acc: 0.991 - ETA: 0s - loss: 0.0335 - acc: 0.991 - ETA: 0s - loss: 0.0332 - acc: 0.991 - ETA: 0s - loss: 0.0344 - acc: 0.991 - ETA: 0s - loss: 0.0346 - acc: 0.990 - ETA: 0s - loss: 0.0352 - acc: 0.990 - ETA: 0s - loss: 0.0358 - acc: 0.990 - ETA: 0s - loss: 0.0365 - acc: 0.990 - ETA: 0s - loss: 0.0374 - acc: 0.990 - ETA: 0s - loss: 0.0374 - acc: 0.9900Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s 641us/step - loss: 0.0383 - acc: 0.9897 - val_loss: 0.6894 - val_acc: 0.8431\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6580/6680 [============================>.] - ETA: 4s - loss: 0.8024 - acc: 0.950 - ETA: 4s - loss: 0.1609 - acc: 0.983 - ETA: 4s - loss: 0.1114 - acc: 0.985 - ETA: 3s - loss: 0.0882 - acc: 0.986 - ETA: 3s - loss: 0.0950 - acc: 0.984 - ETA: 3s - loss: 0.0900 - acc: 0.983 - ETA: 3s - loss: 0.0820 - acc: 0.984 - ETA: 3s - loss: 0.0772 - acc: 0.982 - ETA: 3s - loss: 0.0781 - acc: 0.983 - ETA: 3s - loss: 0.0750 - acc: 0.984 - ETA: 3s - loss: 0.0700 - acc: 0.985 - ETA: 3s - loss: 0.0654 - acc: 0.986 - ETA: 3s - loss: 0.0663 - acc: 0.986 - ETA: 3s - loss: 0.0765 - acc: 0.985 - ETA: 3s - loss: 0.0730 - acc: 0.985 - ETA: 3s - loss: 0.0707 - acc: 0.985 - ETA: 3s - loss: 0.0686 - acc: 0.986 - ETA: 3s - loss: 0.0656 - acc: 0.987 - ETA: 2s - loss: 0.0643 - acc: 0.987 - ETA: 2s - loss: 0.0630 - acc: 0.987 - ETA: 2s - loss: 0.0607 - acc: 0.987 - ETA: 2s - loss: 0.0588 - acc: 0.988 - ETA: 2s - loss: 0.0581 - acc: 0.988 - ETA: 2s - loss: 0.0572 - acc: 0.988 - ETA: 2s - loss: 0.0556 - acc: 0.988 - ETA: 2s - loss: 0.0583 - acc: 0.988 - ETA: 2s - loss: 0.0567 - acc: 0.988 - ETA: 2s - loss: 0.0555 - acc: 0.989 - ETA: 2s - loss: 0.0541 - acc: 0.989 - ETA: 2s - loss: 0.0533 - acc: 0.989 - ETA: 2s - loss: 0.0527 - acc: 0.989 - ETA: 2s - loss: 0.0521 - acc: 0.989 - ETA: 2s - loss: 0.0527 - acc: 0.988 - ETA: 2s - loss: 0.0514 - acc: 0.989 - ETA: 2s - loss: 0.0512 - acc: 0.989 - ETA: 1s - loss: 0.0505 - acc: 0.989 - ETA: 1s - loss: 0.0498 - acc: 0.989 - ETA: 1s - loss: 0.0487 - acc: 0.989 - ETA: 1s - loss: 0.0481 - acc: 0.990 - ETA: 1s - loss: 0.0492 - acc: 0.989 - ETA: 1s - loss: 0.0488 - acc: 0.989 - ETA: 1s - loss: 0.0479 - acc: 0.990 - ETA: 1s - loss: 0.0509 - acc: 0.989 - ETA: 1s - loss: 0.0503 - acc: 0.989 - ETA: 1s - loss: 0.0496 - acc: 0.989 - ETA: 1s - loss: 0.0496 - acc: 0.989 - ETA: 1s - loss: 0.0496 - acc: 0.989 - ETA: 1s - loss: 0.0492 - acc: 0.989 - ETA: 1s - loss: 0.0488 - acc: 0.989 - ETA: 1s - loss: 0.0482 - acc: 0.989 - ETA: 1s - loss: 0.0481 - acc: 0.989 - ETA: 1s - loss: 0.0480 - acc: 0.989 - ETA: 1s - loss: 0.0476 - acc: 0.989 - ETA: 0s - loss: 0.0477 - acc: 0.989 - ETA: 0s - loss: 0.0475 - acc: 0.989 - ETA: 0s - loss: 0.0479 - acc: 0.989 - ETA: 0s - loss: 0.0485 - acc: 0.989 - ETA: 0s - loss: 0.0482 - acc: 0.989 - ETA: 0s - loss: 0.0492 - acc: 0.988 - ETA: 0s - loss: 0.0487 - acc: 0.988 - ETA: 0s - loss: 0.0485 - acc: 0.988 - ETA: 0s - loss: 0.0480 - acc: 0.989 - ETA: 0s - loss: 0.0481 - acc: 0.988 - ETA: 0s - loss: 0.0486 - acc: 0.988 - ETA: 0s - loss: 0.0481 - acc: 0.988 - ETA: 0s - loss: 0.0477 - acc: 0.988 - ETA: 0s - loss: 0.0476 - acc: 0.988 - ETA: 0s - loss: 0.0480 - acc: 0.988 - ETA: 0s - loss: 0.0490 - acc: 0.988 - ETA: 0s - loss: 0.0493 - acc: 0.988 - ETA: 0s - loss: 0.0497 - acc: 0.988 - ETA: 0s - loss: 0.0493 - acc: 0.9884Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s 634us/step - loss: 0.0516 - acc: 0.9879 - val_loss: 0.7194 - val_acc: 0.8479\n",
      "Epoch 12/20\n",
      "6640/6680 [============================>.] - ETA: 4s - loss: 0.1518 - acc: 0.900 - ETA: 3s - loss: 0.0455 - acc: 0.975 - ETA: 3s - loss: 0.0462 - acc: 0.977 - ETA: 3s - loss: 0.0457 - acc: 0.981 - ETA: 3s - loss: 0.0466 - acc: 0.981 - ETA: 3s - loss: 0.0409 - acc: 0.982 - ETA: 3s - loss: 0.0419 - acc: 0.982 - ETA: 3s - loss: 0.0438 - acc: 0.983 - ETA: 3s - loss: 0.0391 - acc: 0.985 - ETA: 3s - loss: 0.0384 - acc: 0.985 - ETA: 3s - loss: 0.0369 - acc: 0.986 - ETA: 3s - loss: 0.0347 - acc: 0.987 - ETA: 3s - loss: 0.0352 - acc: 0.986 - ETA: 3s - loss: 0.0388 - acc: 0.985 - ETA: 3s - loss: 0.0378 - acc: 0.986 - ETA: 3s - loss: 0.0364 - acc: 0.986 - ETA: 3s - loss: 0.0358 - acc: 0.986 - ETA: 3s - loss: 0.0394 - acc: 0.985 - ETA: 3s - loss: 0.0421 - acc: 0.984 - ETA: 2s - loss: 0.0403 - acc: 0.985 - ETA: 2s - loss: 0.0442 - acc: 0.984 - ETA: 2s - loss: 0.0441 - acc: 0.984 - ETA: 2s - loss: 0.0463 - acc: 0.984 - ETA: 2s - loss: 0.0463 - acc: 0.984 - ETA: 2s - loss: 0.0455 - acc: 0.985 - ETA: 2s - loss: 0.0443 - acc: 0.985 - ETA: 2s - loss: 0.0430 - acc: 0.985 - ETA: 2s - loss: 0.0425 - acc: 0.985 - ETA: 2s - loss: 0.0484 - acc: 0.984 - ETA: 2s - loss: 0.0478 - acc: 0.984 - ETA: 2s - loss: 0.0472 - acc: 0.985 - ETA: 2s - loss: 0.0465 - acc: 0.985 - ETA: 2s - loss: 0.0470 - acc: 0.985 - ETA: 2s - loss: 0.0469 - acc: 0.985 - ETA: 2s - loss: 0.0469 - acc: 0.985 - ETA: 2s - loss: 0.0464 - acc: 0.986 - ETA: 2s - loss: 0.0464 - acc: 0.985 - ETA: 1s - loss: 0.0460 - acc: 0.985 - ETA: 1s - loss: 0.0467 - acc: 0.985 - ETA: 1s - loss: 0.0468 - acc: 0.985 - ETA: 1s - loss: 0.0489 - acc: 0.985 - ETA: 1s - loss: 0.0496 - acc: 0.984 - ETA: 1s - loss: 0.0510 - acc: 0.984 - ETA: 1s - loss: 0.0546 - acc: 0.983 - ETA: 1s - loss: 0.0571 - acc: 0.983 - ETA: 1s - loss: 0.0594 - acc: 0.982 - ETA: 1s - loss: 0.0590 - acc: 0.982 - ETA: 1s - loss: 0.0591 - acc: 0.982 - ETA: 1s - loss: 0.0596 - acc: 0.982 - ETA: 1s - loss: 0.0593 - acc: 0.983 - ETA: 1s - loss: 0.0598 - acc: 0.982 - ETA: 1s - loss: 0.0599 - acc: 0.982 - ETA: 1s - loss: 0.0591 - acc: 0.982 - ETA: 1s - loss: 0.0599 - acc: 0.982 - ETA: 1s - loss: 0.0602 - acc: 0.982 - ETA: 0s - loss: 0.0637 - acc: 0.981 - ETA: 0s - loss: 0.0635 - acc: 0.981 - ETA: 0s - loss: 0.0637 - acc: 0.981 - ETA: 0s - loss: 0.0655 - acc: 0.981 - ETA: 0s - loss: 0.0650 - acc: 0.981 - ETA: 0s - loss: 0.0643 - acc: 0.981 - ETA: 0s - loss: 0.0643 - acc: 0.981 - ETA: 0s - loss: 0.0639 - acc: 0.981 - ETA: 0s - loss: 0.0655 - acc: 0.981 - ETA: 0s - loss: 0.0653 - acc: 0.980 - ETA: 0s - loss: 0.0659 - acc: 0.980 - ETA: 0s - loss: 0.0680 - acc: 0.980 - ETA: 0s - loss: 0.0678 - acc: 0.980 - ETA: 0s - loss: 0.0678 - acc: 0.980 - ETA: 0s - loss: 0.0676 - acc: 0.980 - ETA: 0s - loss: 0.0675 - acc: 0.980 - ETA: 0s - loss: 0.0679 - acc: 0.9804Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s 634us/step - loss: 0.0677 - acc: 0.9805 - val_loss: 0.8499 - val_acc: 0.8323\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6620/6680 [============================>.] - ETA: 4s - loss: 0.0029 - acc: 1.000 - ETA: 3s - loss: 0.1142 - acc: 0.950 - ETA: 3s - loss: 0.1123 - acc: 0.954 - ETA: 3s - loss: 0.0886 - acc: 0.966 - ETA: 3s - loss: 0.0687 - acc: 0.975 - ETA: 3s - loss: 0.0874 - acc: 0.968 - ETA: 3s - loss: 0.0963 - acc: 0.968 - ETA: 3s - loss: 0.0869 - acc: 0.971 - ETA: 3s - loss: 0.0799 - acc: 0.974 - ETA: 3s - loss: 0.0885 - acc: 0.970 - ETA: 3s - loss: 0.0948 - acc: 0.970 - ETA: 3s - loss: 0.0965 - acc: 0.969 - ETA: 3s - loss: 0.0899 - acc: 0.971 - ETA: 3s - loss: 0.0879 - acc: 0.971 - ETA: 3s - loss: 0.0846 - acc: 0.973 - ETA: 3s - loss: 0.0847 - acc: 0.973 - ETA: 3s - loss: 0.0823 - acc: 0.974 - ETA: 3s - loss: 0.0801 - acc: 0.974 - ETA: 3s - loss: 0.0786 - acc: 0.975 - ETA: 2s - loss: 0.0804 - acc: 0.974 - ETA: 2s - loss: 0.0782 - acc: 0.975 - ETA: 2s - loss: 0.0753 - acc: 0.976 - ETA: 2s - loss: 0.0744 - acc: 0.976 - ETA: 2s - loss: 0.0770 - acc: 0.976 - ETA: 2s - loss: 0.0769 - acc: 0.976 - ETA: 2s - loss: 0.0768 - acc: 0.975 - ETA: 2s - loss: 0.0803 - acc: 0.975 - ETA: 2s - loss: 0.0798 - acc: 0.975 - ETA: 2s - loss: 0.0820 - acc: 0.975 - ETA: 2s - loss: 0.0804 - acc: 0.975 - ETA: 2s - loss: 0.0809 - acc: 0.976 - ETA: 2s - loss: 0.0812 - acc: 0.975 - ETA: 2s - loss: 0.0797 - acc: 0.976 - ETA: 2s - loss: 0.0780 - acc: 0.976 - ETA: 2s - loss: 0.0769 - acc: 0.977 - ETA: 2s - loss: 0.0784 - acc: 0.976 - ETA: 1s - loss: 0.0776 - acc: 0.977 - ETA: 1s - loss: 0.0766 - acc: 0.977 - ETA: 1s - loss: 0.0767 - acc: 0.976 - ETA: 1s - loss: 0.0752 - acc: 0.977 - ETA: 1s - loss: 0.0777 - acc: 0.976 - ETA: 1s - loss: 0.0791 - acc: 0.975 - ETA: 1s - loss: 0.0799 - acc: 0.975 - ETA: 1s - loss: 0.0797 - acc: 0.975 - ETA: 1s - loss: 0.0794 - acc: 0.975 - ETA: 1s - loss: 0.0790 - acc: 0.975 - ETA: 1s - loss: 0.0804 - acc: 0.975 - ETA: 1s - loss: 0.0834 - acc: 0.974 - ETA: 1s - loss: 0.0824 - acc: 0.974 - ETA: 1s - loss: 0.0827 - acc: 0.974 - ETA: 1s - loss: 0.0858 - acc: 0.974 - ETA: 1s - loss: 0.0858 - acc: 0.973 - ETA: 1s - loss: 0.0857 - acc: 0.973 - ETA: 1s - loss: 0.0850 - acc: 0.973 - ETA: 1s - loss: 0.0844 - acc: 0.973 - ETA: 0s - loss: 0.0840 - acc: 0.973 - ETA: 0s - loss: 0.0839 - acc: 0.973 - ETA: 0s - loss: 0.0837 - acc: 0.973 - ETA: 0s - loss: 0.0833 - acc: 0.973 - ETA: 0s - loss: 0.0836 - acc: 0.973 - ETA: 0s - loss: 0.0846 - acc: 0.972 - ETA: 0s - loss: 0.0850 - acc: 0.972 - ETA: 0s - loss: 0.0848 - acc: 0.972 - ETA: 0s - loss: 0.0840 - acc: 0.972 - ETA: 0s - loss: 0.0841 - acc: 0.972 - ETA: 0s - loss: 0.0844 - acc: 0.972 - ETA: 0s - loss: 0.0843 - acc: 0.972 - ETA: 0s - loss: 0.0838 - acc: 0.972 - ETA: 0s - loss: 0.0835 - acc: 0.972 - ETA: 0s - loss: 0.0830 - acc: 0.972 - ETA: 0s - loss: 0.0841 - acc: 0.972 - ETA: 0s - loss: 0.0840 - acc: 0.972 - ETA: 0s - loss: 0.0840 - acc: 0.9724Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s 639us/step - loss: 0.0843 - acc: 0.9722 - val_loss: 0.8225 - val_acc: 0.8299\n",
      "Epoch 14/20\n",
      "6600/6680 [============================>.] - ETA: 4s - loss: 0.0012 - acc: 1.000 - ETA: 3s - loss: 0.0193 - acc: 1.000 - ETA: 3s - loss: 0.0306 - acc: 0.995 - ETA: 3s - loss: 0.0465 - acc: 0.989 - ETA: 3s - loss: 0.0645 - acc: 0.983 - ETA: 3s - loss: 0.0584 - acc: 0.984 - ETA: 3s - loss: 0.0598 - acc: 0.982 - ETA: 3s - loss: 0.0640 - acc: 0.982 - ETA: 3s - loss: 0.0617 - acc: 0.981 - ETA: 3s - loss: 0.0583 - acc: 0.982 - ETA: 3s - loss: 0.0582 - acc: 0.981 - ETA: 3s - loss: 0.0629 - acc: 0.981 - ETA: 3s - loss: 0.0629 - acc: 0.981 - ETA: 3s - loss: 0.0609 - acc: 0.982 - ETA: 3s - loss: 0.0653 - acc: 0.981 - ETA: 3s - loss: 0.0642 - acc: 0.980 - ETA: 3s - loss: 0.0625 - acc: 0.980 - ETA: 3s - loss: 0.0624 - acc: 0.979 - ETA: 3s - loss: 0.0756 - acc: 0.976 - ETA: 3s - loss: 0.0754 - acc: 0.975 - ETA: 2s - loss: 0.0756 - acc: 0.976 - ETA: 2s - loss: 0.0782 - acc: 0.975 - ETA: 2s - loss: 0.0773 - acc: 0.976 - ETA: 2s - loss: 0.0782 - acc: 0.975 - ETA: 2s - loss: 0.0761 - acc: 0.976 - ETA: 2s - loss: 0.0772 - acc: 0.975 - ETA: 2s - loss: 0.0787 - acc: 0.975 - ETA: 2s - loss: 0.0790 - acc: 0.975 - ETA: 2s - loss: 0.0790 - acc: 0.975 - ETA: 2s - loss: 0.0810 - acc: 0.975 - ETA: 2s - loss: 0.0800 - acc: 0.975 - ETA: 2s - loss: 0.0843 - acc: 0.975 - ETA: 2s - loss: 0.0855 - acc: 0.975 - ETA: 2s - loss: 0.0883 - acc: 0.974 - ETA: 2s - loss: 0.0900 - acc: 0.975 - ETA: 2s - loss: 0.0910 - acc: 0.975 - ETA: 2s - loss: 0.0892 - acc: 0.975 - ETA: 2s - loss: 0.0954 - acc: 0.973 - ETA: 2s - loss: 0.0941 - acc: 0.973 - ETA: 1s - loss: 0.0923 - acc: 0.974 - ETA: 1s - loss: 0.0928 - acc: 0.974 - ETA: 1s - loss: 0.0913 - acc: 0.974 - ETA: 1s - loss: 0.0947 - acc: 0.973 - ETA: 1s - loss: 0.0931 - acc: 0.973 - ETA: 1s - loss: 0.0938 - acc: 0.973 - ETA: 1s - loss: 0.0927 - acc: 0.973 - ETA: 1s - loss: 0.0928 - acc: 0.973 - ETA: 1s - loss: 0.0931 - acc: 0.973 - ETA: 1s - loss: 0.0927 - acc: 0.973 - ETA: 1s - loss: 0.0927 - acc: 0.973 - ETA: 1s - loss: 0.0929 - acc: 0.973 - ETA: 1s - loss: 0.0926 - acc: 0.972 - ETA: 1s - loss: 0.0923 - acc: 0.972 - ETA: 1s - loss: 0.0920 - acc: 0.972 - ETA: 1s - loss: 0.0931 - acc: 0.972 - ETA: 1s - loss: 0.0938 - acc: 0.972 - ETA: 0s - loss: 0.0929 - acc: 0.972 - ETA: 0s - loss: 0.0922 - acc: 0.972 - ETA: 0s - loss: 0.0935 - acc: 0.971 - ETA: 0s - loss: 0.0952 - acc: 0.971 - ETA: 0s - loss: 0.0960 - acc: 0.970 - ETA: 0s - loss: 0.0983 - acc: 0.969 - ETA: 0s - loss: 0.0977 - acc: 0.970 - ETA: 0s - loss: 0.0995 - acc: 0.969 - ETA: 0s - loss: 0.0996 - acc: 0.969 - ETA: 0s - loss: 0.1013 - acc: 0.969 - ETA: 0s - loss: 0.1020 - acc: 0.969 - ETA: 0s - loss: 0.1021 - acc: 0.969 - ETA: 0s - loss: 0.1020 - acc: 0.969 - ETA: 0s - loss: 0.1038 - acc: 0.968 - ETA: 0s - loss: 0.1029 - acc: 0.968 - ETA: 0s - loss: 0.1034 - acc: 0.968 - ETA: 0s - loss: 0.1025 - acc: 0.9689Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s 639us/step - loss: 0.1016 - acc: 0.9692 - val_loss: 0.9107 - val_acc: 0.8287\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6600/6680 [============================>.] - ETA: 4s - loss: 0.0490 - acc: 1.000 - ETA: 3s - loss: 0.0157 - acc: 1.000 - ETA: 3s - loss: 0.0213 - acc: 0.995 - ETA: 3s - loss: 0.0254 - acc: 0.990 - ETA: 3s - loss: 0.0318 - acc: 0.989 - ETA: 3s - loss: 0.0588 - acc: 0.987 - ETA: 3s - loss: 0.0573 - acc: 0.987 - ETA: 3s - loss: 0.0503 - acc: 0.989 - ETA: 3s - loss: 0.0563 - acc: 0.985 - ETA: 3s - loss: 0.0521 - acc: 0.987 - ETA: 3s - loss: 0.0517 - acc: 0.986 - ETA: 3s - loss: 0.0495 - acc: 0.987 - ETA: 3s - loss: 0.0465 - acc: 0.988 - ETA: 3s - loss: 0.0480 - acc: 0.986 - ETA: 3s - loss: 0.0450 - acc: 0.987 - ETA: 3s - loss: 0.0446 - acc: 0.987 - ETA: 3s - loss: 0.0423 - acc: 0.988 - ETA: 3s - loss: 0.0408 - acc: 0.989 - ETA: 2s - loss: 0.0395 - acc: 0.989 - ETA: 2s - loss: 0.0384 - acc: 0.990 - ETA: 2s - loss: 0.0372 - acc: 0.990 - ETA: 2s - loss: 0.0372 - acc: 0.989 - ETA: 2s - loss: 0.0365 - acc: 0.990 - ETA: 2s - loss: 0.0353 - acc: 0.990 - ETA: 2s - loss: 0.0342 - acc: 0.991 - ETA: 2s - loss: 0.0399 - acc: 0.990 - ETA: 2s - loss: 0.0393 - acc: 0.990 - ETA: 2s - loss: 0.0399 - acc: 0.990 - ETA: 2s - loss: 0.0390 - acc: 0.990 - ETA: 2s - loss: 0.0387 - acc: 0.990 - ETA: 2s - loss: 0.0389 - acc: 0.990 - ETA: 2s - loss: 0.0405 - acc: 0.989 - ETA: 2s - loss: 0.0403 - acc: 0.989 - ETA: 2s - loss: 0.0396 - acc: 0.989 - ETA: 2s - loss: 0.0402 - acc: 0.989 - ETA: 2s - loss: 0.0401 - acc: 0.989 - ETA: 2s - loss: 0.0394 - acc: 0.989 - ETA: 1s - loss: 0.0386 - acc: 0.989 - ETA: 1s - loss: 0.0382 - acc: 0.989 - ETA: 1s - loss: 0.0377 - acc: 0.989 - ETA: 1s - loss: 0.0378 - acc: 0.989 - ETA: 1s - loss: 0.0382 - acc: 0.989 - ETA: 1s - loss: 0.0375 - acc: 0.989 - ETA: 1s - loss: 0.0377 - acc: 0.989 - ETA: 1s - loss: 0.0376 - acc: 0.989 - ETA: 1s - loss: 0.0377 - acc: 0.989 - ETA: 1s - loss: 0.0380 - acc: 0.989 - ETA: 1s - loss: 0.0378 - acc: 0.989 - ETA: 1s - loss: 0.0370 - acc: 0.989 - ETA: 1s - loss: 0.0367 - acc: 0.989 - ETA: 1s - loss: 0.0360 - acc: 0.990 - ETA: 1s - loss: 0.0364 - acc: 0.990 - ETA: 1s - loss: 0.0362 - acc: 0.990 - ETA: 1s - loss: 0.0367 - acc: 0.989 - ETA: 0s - loss: 0.0368 - acc: 0.989 - ETA: 0s - loss: 0.0368 - acc: 0.989 - ETA: 0s - loss: 0.0365 - acc: 0.989 - ETA: 0s - loss: 0.0360 - acc: 0.989 - ETA: 0s - loss: 0.0365 - acc: 0.989 - ETA: 0s - loss: 0.0369 - acc: 0.989 - ETA: 0s - loss: 0.0370 - acc: 0.989 - ETA: 0s - loss: 0.0365 - acc: 0.989 - ETA: 0s - loss: 0.0366 - acc: 0.989 - ETA: 0s - loss: 0.0361 - acc: 0.989 - ETA: 0s - loss: 0.0361 - acc: 0.989 - ETA: 0s - loss: 0.0359 - acc: 0.989 - ETA: 0s - loss: 0.0362 - acc: 0.988 - ETA: 0s - loss: 0.0363 - acc: 0.988 - ETA: 0s - loss: 0.0369 - acc: 0.989 - ETA: 0s - loss: 0.0367 - acc: 0.989 - ETA: 0s - loss: 0.0366 - acc: 0.989 - ETA: 0s - loss: 0.0365 - acc: 0.9891Epoch 00015: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s 632us/step - loss: 0.0361 - acc: 0.9892 - val_loss: 0.7503 - val_acc: 0.8503\n",
      "Epoch 16/20\n",
      "6620/6680 [============================>.] - ETA: 4s - loss: 0.0146 - acc: 1.000 - ETA: 3s - loss: 0.0331 - acc: 0.991 - ETA: 3s - loss: 0.0293 - acc: 0.990 - ETA: 3s - loss: 0.0324 - acc: 0.986 - ETA: 3s - loss: 0.0270 - acc: 0.989 - ETA: 3s - loss: 0.0252 - acc: 0.989 - ETA: 3s - loss: 0.0224 - acc: 0.990 - ETA: 3s - loss: 0.0290 - acc: 0.990 - ETA: 3s - loss: 0.0268 - acc: 0.991 - ETA: 3s - loss: 0.0256 - acc: 0.991 - ETA: 3s - loss: 0.0240 - acc: 0.992 - ETA: 3s - loss: 0.0222 - acc: 0.992 - ETA: 3s - loss: 0.0256 - acc: 0.992 - ETA: 3s - loss: 0.0240 - acc: 0.992 - ETA: 3s - loss: 0.0249 - acc: 0.992 - ETA: 3s - loss: 0.0241 - acc: 0.993 - ETA: 3s - loss: 0.0228 - acc: 0.993 - ETA: 3s - loss: 0.0247 - acc: 0.992 - ETA: 3s - loss: 0.0244 - acc: 0.992 - ETA: 3s - loss: 0.0238 - acc: 0.992 - ETA: 3s - loss: 0.0227 - acc: 0.993 - ETA: 3s - loss: 0.0217 - acc: 0.993 - ETA: 2s - loss: 0.0209 - acc: 0.993 - ETA: 2s - loss: 0.0208 - acc: 0.993 - ETA: 2s - loss: 0.0200 - acc: 0.993 - ETA: 2s - loss: 0.0225 - acc: 0.993 - ETA: 2s - loss: 0.0220 - acc: 0.993 - ETA: 2s - loss: 0.0216 - acc: 0.993 - ETA: 2s - loss: 0.0210 - acc: 0.994 - ETA: 2s - loss: 0.0205 - acc: 0.994 - ETA: 2s - loss: 0.0201 - acc: 0.994 - ETA: 2s - loss: 0.0196 - acc: 0.994 - ETA: 2s - loss: 0.0191 - acc: 0.994 - ETA: 2s - loss: 0.0188 - acc: 0.994 - ETA: 2s - loss: 0.0186 - acc: 0.995 - ETA: 2s - loss: 0.0181 - acc: 0.995 - ETA: 2s - loss: 0.0179 - acc: 0.995 - ETA: 2s - loss: 0.0176 - acc: 0.995 - ETA: 2s - loss: 0.0176 - acc: 0.995 - ETA: 1s - loss: 0.0178 - acc: 0.995 - ETA: 1s - loss: 0.0181 - acc: 0.995 - ETA: 1s - loss: 0.0180 - acc: 0.994 - ETA: 1s - loss: 0.0185 - acc: 0.994 - ETA: 1s - loss: 0.0182 - acc: 0.994 - ETA: 1s - loss: 0.0179 - acc: 0.994 - ETA: 1s - loss: 0.0176 - acc: 0.995 - ETA: 1s - loss: 0.0194 - acc: 0.994 - ETA: 1s - loss: 0.0193 - acc: 0.995 - ETA: 1s - loss: 0.0189 - acc: 0.995 - ETA: 1s - loss: 0.0188 - acc: 0.995 - ETA: 1s - loss: 0.0186 - acc: 0.995 - ETA: 1s - loss: 0.0184 - acc: 0.995 - ETA: 1s - loss: 0.0187 - acc: 0.995 - ETA: 1s - loss: 0.0188 - acc: 0.995 - ETA: 1s - loss: 0.0187 - acc: 0.995 - ETA: 1s - loss: 0.0185 - acc: 0.995 - ETA: 1s - loss: 0.0183 - acc: 0.995 - ETA: 0s - loss: 0.0181 - acc: 0.995 - ETA: 0s - loss: 0.0181 - acc: 0.995 - ETA: 0s - loss: 0.0180 - acc: 0.995 - ETA: 0s - loss: 0.0184 - acc: 0.995 - ETA: 0s - loss: 0.0183 - acc: 0.995 - ETA: 0s - loss: 0.0181 - acc: 0.995 - ETA: 0s - loss: 0.0180 - acc: 0.995 - ETA: 0s - loss: 0.0195 - acc: 0.995 - ETA: 0s - loss: 0.0195 - acc: 0.995 - ETA: 0s - loss: 0.0201 - acc: 0.995 - ETA: 0s - loss: 0.0200 - acc: 0.995 - ETA: 0s - loss: 0.0199 - acc: 0.995 - ETA: 0s - loss: 0.0200 - acc: 0.995 - ETA: 0s - loss: 0.0200 - acc: 0.995 - ETA: 0s - loss: 0.0198 - acc: 0.995 - ETA: 0s - loss: 0.0198 - acc: 0.995 - ETA: 0s - loss: 0.0196 - acc: 0.995 - ETA: 0s - loss: 0.0194 - acc: 0.9953Epoch 00016: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s 645us/step - loss: 0.0194 - acc: 0.9954 - val_loss: 0.7899 - val_acc: 0.8419\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 4s - loss: 0.0210 - acc: 1.000 - ETA: 3s - loss: 0.0050 - acc: 1.000 - ETA: 3s - loss: 0.0040 - acc: 1.000 - ETA: 4s - loss: 0.0041 - acc: 1.000 - ETA: 3s - loss: 0.0043 - acc: 1.000 - ETA: 3s - loss: 0.0049 - acc: 1.000 - ETA: 3s - loss: 0.0053 - acc: 1.000 - ETA: 3s - loss: 0.0050 - acc: 1.000 - ETA: 3s - loss: 0.0055 - acc: 1.000 - ETA: 3s - loss: 0.0055 - acc: 1.000 - ETA: 3s - loss: 0.0075 - acc: 0.998 - ETA: 3s - loss: 0.0075 - acc: 0.998 - ETA: 3s - loss: 0.0074 - acc: 0.999 - ETA: 3s - loss: 0.0072 - acc: 0.999 - ETA: 3s - loss: 0.0070 - acc: 0.999 - ETA: 3s - loss: 0.0093 - acc: 0.998 - ETA: 3s - loss: 0.0147 - acc: 0.997 - ETA: 3s - loss: 0.0146 - acc: 0.997 - ETA: 3s - loss: 0.0166 - acc: 0.997 - ETA: 3s - loss: 0.0163 - acc: 0.997 - ETA: 3s - loss: 0.0158 - acc: 0.997 - ETA: 3s - loss: 0.0176 - acc: 0.997 - ETA: 3s - loss: 0.0170 - acc: 0.997 - ETA: 3s - loss: 0.0174 - acc: 0.997 - ETA: 3s - loss: 0.0195 - acc: 0.997 - ETA: 2s - loss: 0.0193 - acc: 0.996 - ETA: 2s - loss: 0.0192 - acc: 0.996 - ETA: 2s - loss: 0.0208 - acc: 0.996 - ETA: 2s - loss: 0.0237 - acc: 0.995 - ETA: 2s - loss: 0.0247 - acc: 0.994 - ETA: 2s - loss: 0.0240 - acc: 0.994 - ETA: 2s - loss: 0.0265 - acc: 0.994 - ETA: 2s - loss: 0.0261 - acc: 0.994 - ETA: 2s - loss: 0.0269 - acc: 0.994 - ETA: 2s - loss: 0.0265 - acc: 0.994 - ETA: 2s - loss: 0.0262 - acc: 0.994 - ETA: 2s - loss: 0.0257 - acc: 0.994 - ETA: 2s - loss: 0.0254 - acc: 0.994 - ETA: 2s - loss: 0.0253 - acc: 0.994 - ETA: 2s - loss: 0.0265 - acc: 0.993 - ETA: 2s - loss: 0.0277 - acc: 0.993 - ETA: 2s - loss: 0.0270 - acc: 0.993 - ETA: 2s - loss: 0.0266 - acc: 0.993 - ETA: 1s - loss: 0.0262 - acc: 0.993 - ETA: 1s - loss: 0.0257 - acc: 0.994 - ETA: 1s - loss: 0.0255 - acc: 0.994 - ETA: 1s - loss: 0.0254 - acc: 0.994 - ETA: 1s - loss: 0.0250 - acc: 0.994 - ETA: 1s - loss: 0.0266 - acc: 0.993 - ETA: 1s - loss: 0.0262 - acc: 0.993 - ETA: 1s - loss: 0.0259 - acc: 0.994 - ETA: 1s - loss: 0.0255 - acc: 0.994 - ETA: 1s - loss: 0.0252 - acc: 0.994 - ETA: 1s - loss: 0.0250 - acc: 0.994 - ETA: 1s - loss: 0.0267 - acc: 0.994 - ETA: 1s - loss: 0.0263 - acc: 0.994 - ETA: 1s - loss: 0.0260 - acc: 0.994 - ETA: 1s - loss: 0.0258 - acc: 0.994 - ETA: 1s - loss: 0.0259 - acc: 0.994 - ETA: 1s - loss: 0.0256 - acc: 0.994 - ETA: 1s - loss: 0.0254 - acc: 0.994 - ETA: 1s - loss: 0.0251 - acc: 0.994 - ETA: 0s - loss: 0.0253 - acc: 0.993 - ETA: 0s - loss: 0.0264 - acc: 0.993 - ETA: 0s - loss: 0.0262 - acc: 0.993 - ETA: 0s - loss: 0.0267 - acc: 0.993 - ETA: 0s - loss: 0.0269 - acc: 0.993 - ETA: 0s - loss: 0.0264 - acc: 0.993 - ETA: 0s - loss: 0.0266 - acc: 0.993 - ETA: 0s - loss: 0.0263 - acc: 0.993 - ETA: 0s - loss: 0.0263 - acc: 0.993 - ETA: 0s - loss: 0.0261 - acc: 0.993 - ETA: 0s - loss: 0.0258 - acc: 0.993 - ETA: 0s - loss: 0.0256 - acc: 0.993 - ETA: 0s - loss: 0.0253 - acc: 0.993 - ETA: 0s - loss: 0.0252 - acc: 0.994 - ETA: 0s - loss: 0.0249 - acc: 0.994 - ETA: 0s - loss: 0.0247 - acc: 0.9941Epoch 00017: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s 677us/step - loss: 0.0246 - acc: 0.9942 - val_loss: 0.7503 - val_acc: 0.8467\n",
      "Epoch 18/20\n",
      "6640/6680 [============================>.] - ETA: 4s - loss: 0.0017 - acc: 1.000 - ETA: 3s - loss: 0.0074 - acc: 1.000 - ETA: 3s - loss: 0.0065 - acc: 1.000 - ETA: 3s - loss: 0.0057 - acc: 1.000 - ETA: 3s - loss: 0.0165 - acc: 0.995 - ETA: 3s - loss: 0.0139 - acc: 0.996 - ETA: 3s - loss: 0.0135 - acc: 0.996 - ETA: 3s - loss: 0.0121 - acc: 0.997 - ETA: 3s - loss: 0.0119 - acc: 0.996 - ETA: 3s - loss: 0.0119 - acc: 0.996 - ETA: 3s - loss: 0.0107 - acc: 0.997 - ETA: 3s - loss: 0.0099 - acc: 0.997 - ETA: 3s - loss: 0.0095 - acc: 0.997 - ETA: 3s - loss: 0.0092 - acc: 0.997 - ETA: 2s - loss: 0.0088 - acc: 0.997 - ETA: 2s - loss: 0.0083 - acc: 0.998 - ETA: 2s - loss: 0.0079 - acc: 0.998 - ETA: 2s - loss: 0.0079 - acc: 0.998 - ETA: 2s - loss: 0.0079 - acc: 0.998 - ETA: 2s - loss: 0.0077 - acc: 0.998 - ETA: 2s - loss: 0.0077 - acc: 0.998 - ETA: 2s - loss: 0.0092 - acc: 0.998 - ETA: 2s - loss: 0.0090 - acc: 0.998 - ETA: 2s - loss: 0.0088 - acc: 0.998 - ETA: 2s - loss: 0.0085 - acc: 0.998 - ETA: 2s - loss: 0.0095 - acc: 0.997 - ETA: 2s - loss: 0.0095 - acc: 0.998 - ETA: 2s - loss: 0.0093 - acc: 0.998 - ETA: 2s - loss: 0.0094 - acc: 0.998 - ETA: 2s - loss: 0.0100 - acc: 0.997 - ETA: 2s - loss: 0.0100 - acc: 0.997 - ETA: 2s - loss: 0.0098 - acc: 0.997 - ETA: 2s - loss: 0.0119 - acc: 0.997 - ETA: 2s - loss: 0.0131 - acc: 0.997 - ETA: 2s - loss: 0.0128 - acc: 0.997 - ETA: 2s - loss: 0.0125 - acc: 0.997 - ETA: 1s - loss: 0.0123 - acc: 0.997 - ETA: 1s - loss: 0.0125 - acc: 0.997 - ETA: 1s - loss: 0.0129 - acc: 0.997 - ETA: 1s - loss: 0.0127 - acc: 0.997 - ETA: 1s - loss: 0.0133 - acc: 0.996 - ETA: 1s - loss: 0.0131 - acc: 0.996 - ETA: 1s - loss: 0.0131 - acc: 0.996 - ETA: 1s - loss: 0.0129 - acc: 0.996 - ETA: 1s - loss: 0.0128 - acc: 0.996 - ETA: 1s - loss: 0.0126 - acc: 0.996 - ETA: 1s - loss: 0.0125 - acc: 0.996 - ETA: 1s - loss: 0.0139 - acc: 0.996 - ETA: 1s - loss: 0.0144 - acc: 0.996 - ETA: 1s - loss: 0.0142 - acc: 0.996 - ETA: 1s - loss: 0.0145 - acc: 0.996 - ETA: 1s - loss: 0.0147 - acc: 0.996 - ETA: 1s - loss: 0.0145 - acc: 0.996 - ETA: 1s - loss: 0.0144 - acc: 0.996 - ETA: 1s - loss: 0.0151 - acc: 0.996 - ETA: 1s - loss: 0.0150 - acc: 0.996 - ETA: 1s - loss: 0.0147 - acc: 0.996 - ETA: 1s - loss: 0.0150 - acc: 0.996 - ETA: 1s - loss: 0.0149 - acc: 0.996 - ETA: 1s - loss: 0.0147 - acc: 0.996 - ETA: 1s - loss: 0.0147 - acc: 0.996 - ETA: 0s - loss: 0.0145 - acc: 0.996 - ETA: 0s - loss: 0.0144 - acc: 0.996 - ETA: 0s - loss: 0.0142 - acc: 0.996 - ETA: 0s - loss: 0.0141 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0139 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0139 - acc: 0.996 - ETA: 0s - loss: 0.0138 - acc: 0.996 - ETA: 0s - loss: 0.0145 - acc: 0.996 - ETA: 0s - loss: 0.0143 - acc: 0.996 - ETA: 0s - loss: 0.0142 - acc: 0.996 - ETA: 0s - loss: 0.0141 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0139 - acc: 0.996 - ETA: 0s - loss: 0.0138 - acc: 0.996 - ETA: 0s - loss: 0.0137 - acc: 0.996 - ETA: 0s - loss: 0.0157 - acc: 0.996 - ETA: 0s - loss: 0.0158 - acc: 0.996 - ETA: 0s - loss: 0.0166 - acc: 0.996 - ETA: 0s - loss: 0.0164 - acc: 0.9961Epoch 00018: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s 754us/step - loss: 0.0164 - acc: 0.9961 - val_loss: 0.8346 - val_acc: 0.8431\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6580/6680 [============================>.] - ETA: 4s - loss: 0.0121 - acc: 1.000 - ETA: 4s - loss: 0.0946 - acc: 0.990 - ETA: 4s - loss: 0.0534 - acc: 0.994 - ETA: 4s - loss: 0.0401 - acc: 0.996 - ETA: 4s - loss: 0.0305 - acc: 0.997 - ETA: 3s - loss: 0.0256 - acc: 0.997 - ETA: 3s - loss: 0.0214 - acc: 0.998 - ETA: 3s - loss: 0.0205 - acc: 0.998 - ETA: 3s - loss: 0.0203 - acc: 0.997 - ETA: 3s - loss: 0.0191 - acc: 0.997 - ETA: 3s - loss: 0.0175 - acc: 0.997 - ETA: 3s - loss: 0.0166 - acc: 0.998 - ETA: 3s - loss: 0.0170 - acc: 0.998 - ETA: 3s - loss: 0.0161 - acc: 0.998 - ETA: 3s - loss: 0.0152 - acc: 0.998 - ETA: 3s - loss: 0.0143 - acc: 0.998 - ETA: 3s - loss: 0.0137 - acc: 0.998 - ETA: 3s - loss: 0.0147 - acc: 0.998 - ETA: 3s - loss: 0.0142 - acc: 0.998 - ETA: 3s - loss: 0.0137 - acc: 0.998 - ETA: 2s - loss: 0.0130 - acc: 0.998 - ETA: 2s - loss: 0.0126 - acc: 0.998 - ETA: 2s - loss: 0.0121 - acc: 0.998 - ETA: 2s - loss: 0.0117 - acc: 0.998 - ETA: 2s - loss: 0.0135 - acc: 0.998 - ETA: 2s - loss: 0.0130 - acc: 0.998 - ETA: 2s - loss: 0.0128 - acc: 0.998 - ETA: 2s - loss: 0.0126 - acc: 0.998 - ETA: 2s - loss: 0.0122 - acc: 0.998 - ETA: 2s - loss: 0.0121 - acc: 0.998 - ETA: 2s - loss: 0.0144 - acc: 0.997 - ETA: 2s - loss: 0.0158 - acc: 0.997 - ETA: 2s - loss: 0.0153 - acc: 0.997 - ETA: 2s - loss: 0.0150 - acc: 0.997 - ETA: 1s - loss: 0.0146 - acc: 0.997 - ETA: 1s - loss: 0.0150 - acc: 0.997 - ETA: 1s - loss: 0.0148 - acc: 0.997 - ETA: 1s - loss: 0.0171 - acc: 0.997 - ETA: 1s - loss: 0.0166 - acc: 0.997 - ETA: 1s - loss: 0.0164 - acc: 0.997 - ETA: 1s - loss: 0.0159 - acc: 0.997 - ETA: 1s - loss: 0.0157 - acc: 0.997 - ETA: 1s - loss: 0.0154 - acc: 0.997 - ETA: 1s - loss: 0.0153 - acc: 0.997 - ETA: 1s - loss: 0.0149 - acc: 0.997 - ETA: 1s - loss: 0.0149 - acc: 0.997 - ETA: 1s - loss: 0.0146 - acc: 0.997 - ETA: 1s - loss: 0.0144 - acc: 0.997 - ETA: 1s - loss: 0.0141 - acc: 0.997 - ETA: 1s - loss: 0.0160 - acc: 0.997 - ETA: 0s - loss: 0.0158 - acc: 0.997 - ETA: 0s - loss: 0.0155 - acc: 0.997 - ETA: 0s - loss: 0.0153 - acc: 0.997 - ETA: 0s - loss: 0.0158 - acc: 0.997 - ETA: 0s - loss: 0.0155 - acc: 0.997 - ETA: 0s - loss: 0.0154 - acc: 0.997 - ETA: 0s - loss: 0.0163 - acc: 0.997 - ETA: 0s - loss: 0.0161 - acc: 0.997 - ETA: 0s - loss: 0.0160 - acc: 0.997 - ETA: 0s - loss: 0.0158 - acc: 0.997 - ETA: 0s - loss: 0.0156 - acc: 0.997 - ETA: 0s - loss: 0.0155 - acc: 0.997 - ETA: 0s - loss: 0.0153 - acc: 0.997 - ETA: 0s - loss: 0.0152 - acc: 0.997 - ETA: 0s - loss: 0.0149 - acc: 0.9973Epoch 00019: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s 600us/step - loss: 0.0152 - acc: 0.9972 - val_loss: 0.7840 - val_acc: 0.8527\n",
      "Epoch 20/20\n",
      "6660/6680 [============================>.] - ETA: 5s - loss: 0.0015 - acc: 1.000 - ETA: 4s - loss: 0.0135 - acc: 0.991 - ETA: 4s - loss: 0.0145 - acc: 0.990 - ETA: 4s - loss: 0.0109 - acc: 0.992 - ETA: 4s - loss: 0.0102 - acc: 0.994 - ETA: 4s - loss: 0.0090 - acc: 0.995 - ETA: 3s - loss: 0.0087 - acc: 0.996 - ETA: 3s - loss: 0.0101 - acc: 0.995 - ETA: 3s - loss: 0.0095 - acc: 0.995 - ETA: 3s - loss: 0.0121 - acc: 0.993 - ETA: 3s - loss: 0.0112 - acc: 0.994 - ETA: 3s - loss: 0.0103 - acc: 0.994 - ETA: 3s - loss: 0.0109 - acc: 0.994 - ETA: 3s - loss: 0.0141 - acc: 0.994 - ETA: 3s - loss: 0.0130 - acc: 0.994 - ETA: 3s - loss: 0.0122 - acc: 0.995 - ETA: 3s - loss: 0.0128 - acc: 0.994 - ETA: 3s - loss: 0.0121 - acc: 0.995 - ETA: 3s - loss: 0.0116 - acc: 0.995 - ETA: 2s - loss: 0.0110 - acc: 0.995 - ETA: 2s - loss: 0.0113 - acc: 0.995 - ETA: 2s - loss: 0.0143 - acc: 0.995 - ETA: 2s - loss: 0.0145 - acc: 0.994 - ETA: 2s - loss: 0.0140 - acc: 0.995 - ETA: 2s - loss: 0.0136 - acc: 0.995 - ETA: 2s - loss: 0.0133 - acc: 0.995 - ETA: 2s - loss: 0.0130 - acc: 0.995 - ETA: 2s - loss: 0.0126 - acc: 0.995 - ETA: 2s - loss: 0.0126 - acc: 0.995 - ETA: 2s - loss: 0.0124 - acc: 0.996 - ETA: 2s - loss: 0.0123 - acc: 0.995 - ETA: 2s - loss: 0.0120 - acc: 0.995 - ETA: 2s - loss: 0.0123 - acc: 0.995 - ETA: 2s - loss: 0.0121 - acc: 0.995 - ETA: 2s - loss: 0.0118 - acc: 0.995 - ETA: 2s - loss: 0.0116 - acc: 0.995 - ETA: 2s - loss: 0.0116 - acc: 0.996 - ETA: 2s - loss: 0.0128 - acc: 0.995 - ETA: 2s - loss: 0.0127 - acc: 0.995 - ETA: 2s - loss: 0.0125 - acc: 0.996 - ETA: 2s - loss: 0.0123 - acc: 0.996 - ETA: 1s - loss: 0.0122 - acc: 0.996 - ETA: 1s - loss: 0.0121 - acc: 0.996 - ETA: 1s - loss: 0.0120 - acc: 0.996 - ETA: 1s - loss: 0.0120 - acc: 0.996 - ETA: 1s - loss: 0.0119 - acc: 0.996 - ETA: 1s - loss: 0.0122 - acc: 0.996 - ETA: 1s - loss: 0.0120 - acc: 0.996 - ETA: 1s - loss: 0.0119 - acc: 0.996 - ETA: 1s - loss: 0.0117 - acc: 0.996 - ETA: 1s - loss: 0.0116 - acc: 0.996 - ETA: 1s - loss: 0.0115 - acc: 0.996 - ETA: 1s - loss: 0.0113 - acc: 0.996 - ETA: 1s - loss: 0.0112 - acc: 0.996 - ETA: 1s - loss: 0.0112 - acc: 0.996 - ETA: 1s - loss: 0.0110 - acc: 0.996 - ETA: 1s - loss: 0.0108 - acc: 0.996 - ETA: 1s - loss: 0.0106 - acc: 0.997 - ETA: 1s - loss: 0.0107 - acc: 0.996 - ETA: 0s - loss: 0.0105 - acc: 0.996 - ETA: 0s - loss: 0.0104 - acc: 0.997 - ETA: 0s - loss: 0.0103 - acc: 0.997 - ETA: 0s - loss: 0.0102 - acc: 0.997 - ETA: 0s - loss: 0.0102 - acc: 0.997 - ETA: 0s - loss: 0.0101 - acc: 0.997 - ETA: 0s - loss: 0.0100 - acc: 0.997 - ETA: 0s - loss: 0.0103 - acc: 0.997 - ETA: 0s - loss: 0.0102 - acc: 0.997 - ETA: 0s - loss: 0.0101 - acc: 0.997 - ETA: 0s - loss: 0.0100 - acc: 0.997 - ETA: 0s - loss: 0.0102 - acc: 0.997 - ETA: 0s - loss: 0.0101 - acc: 0.997 - ETA: 0s - loss: 0.0099 - acc: 0.997 - ETA: 0s - loss: 0.0098 - acc: 0.997 - ETA: 0s - loss: 0.0101 - acc: 0.997 - ETA: 0s - loss: 0.0100 - acc: 0.9971Epoch 00020: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s 679us/step - loss: 0.0100 - acc: 0.9972 - val_loss: 0.8028 - val_acc: 0.8587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aaa6b9e588>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.InceptionV3.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "InceptionV3_model.fit(train_InceptionV3, train_targets, \n",
    "          validation_data=(valid_InceptionV3, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the best weights\n",
    "InceptionV3_model.load_weights('saved_models/weights.best.InceptionV3.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 81.5789%\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "\n",
    "# get index of predicted dog breed for each image in test set\n",
    "InceptionV3_predictions = [np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a big increase in test accuracy.\n",
    "\n",
    "While VGG16 model achieved 46.6507% accuracy, InceptionV3 did 81.5789%.\n",
    "\n",
    "Now, going to try other bottleneck features and see how others do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store test accuracy results\n",
    "\n",
    "test_results = {}\n",
    "test_results[\"InceptionV3\"] = 81.5789"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for the whole process\n",
    "\n",
    "def tryout_bottleneck(model_name):\n",
    "    # get the bottleneck features\n",
    "    bottleneck_features = np.load('bottleneck_features/Dog' + model_name + 'Data.npz')\n",
    "    trainset = bottleneck_features['train']\n",
    "    validset = bottleneck_features['valid']\n",
    "    testset = bottleneck_features['test']\n",
    "    \n",
    "    # create a model\n",
    "    try_model = Sequential()\n",
    "    try_model.add(GlobalAveragePooling2D(input_shape=trainset.shape[1:]))\n",
    "    try_model.add(Dense(133, activation='softmax'))\n",
    "    \n",
    "    # compile\n",
    "    try_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # train\n",
    "    checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.' + model_name + '.hdf5', \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    try_model.fit(trainset, train_targets, \n",
    "                  validation_data=(validset, valid_targets),\n",
    "                  epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)\n",
    "    \n",
    "    # load the best weights\n",
    "    try_model.load_weights('saved_models/weights.best.' + model_name + '.hdf5')\n",
    "\n",
    "    # predict\n",
    "    predictions = [np.argmax(try_model.predict(np.expand_dims(feature, axis=0))) for feature in testset]\n",
    "\n",
    "    # report test accuracy\n",
    "    test_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\n",
    "    print('Test accuracy: %.4f%%' % test_accuracy)\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6620/6680 [============================>.] - ETA: 3:48 - loss: 14.9231 - acc: 0.0000e+ - ETA: 24s - loss: 14.3098 - acc: 0.0150     - ETA: 13s - loss: 14.5520 - acc: 0.008 - ETA: 10s - loss: 14.4366 - acc: 0.015 - ETA: 7s - loss: 14.2811 - acc: 0.020 - ETA: 6s - loss: 14.1972 - acc: 0.02 - ETA: 5s - loss: 14.0226 - acc: 0.02 - ETA: 5s - loss: 13.8637 - acc: 0.03 - ETA: 4s - loss: 13.6485 - acc: 0.04 - ETA: 4s - loss: 13.5864 - acc: 0.04 - ETA: 3s - loss: 13.4958 - acc: 0.05 - ETA: 3s - loss: 13.4152 - acc: 0.06 - ETA: 3s - loss: 13.2855 - acc: 0.06 - ETA: 2s - loss: 13.2011 - acc: 0.07 - ETA: 2s - loss: 13.1340 - acc: 0.07 - ETA: 2s - loss: 13.0398 - acc: 0.07 - ETA: 2s - loss: 12.9305 - acc: 0.08 - ETA: 2s - loss: 12.8677 - acc: 0.08 - ETA: 2s - loss: 12.7704 - acc: 0.08 - ETA: 1s - loss: 12.7023 - acc: 0.09 - ETA: 1s - loss: 12.6085 - acc: 0.09 - ETA: 1s - loss: 12.5568 - acc: 0.09 - ETA: 1s - loss: 12.5096 - acc: 0.10 - ETA: 1s - loss: 12.4551 - acc: 0.10 - ETA: 1s - loss: 12.3472 - acc: 0.11 - ETA: 1s - loss: 12.3019 - acc: 0.11 - ETA: 1s - loss: 12.2503 - acc: 0.11 - ETA: 1s - loss: 12.1901 - acc: 0.12 - ETA: 1s - loss: 12.1370 - acc: 0.12 - ETA: 0s - loss: 12.0790 - acc: 0.12 - ETA: 0s - loss: 12.0513 - acc: 0.12 - ETA: 0s - loss: 11.9992 - acc: 0.13 - ETA: 0s - loss: 11.9347 - acc: 0.13 - ETA: 0s - loss: 11.8779 - acc: 0.14 - ETA: 0s - loss: 11.8245 - acc: 0.14 - ETA: 0s - loss: 11.7667 - acc: 0.14 - ETA: 0s - loss: 11.6967 - acc: 0.15 - ETA: 0s - loss: 11.6453 - acc: 0.15 - ETA: 0s - loss: 11.6357 - acc: 0.15 - ETA: 0s - loss: 11.6103 - acc: 0.15 - ETA: 0s - loss: 11.5720 - acc: 0.16 - ETA: 0s - loss: 11.5331 - acc: 0.1631Epoch 00001: val_loss improved from inf to 9.97174, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s 493us/step - loss: 11.5124 - acc: 0.1644 - val_loss: 9.9717 - val_acc: 0.2659\n",
      "Epoch 2/20\n",
      "6520/6680 [============================>.] - ETA: 2s - loss: 8.6665 - acc: 0.350 - ETA: 1s - loss: 9.1152 - acc: 0.350 - ETA: 1s - loss: 8.9989 - acc: 0.352 - ETA: 1s - loss: 9.1353 - acc: 0.346 - ETA: 1s - loss: 9.1027 - acc: 0.339 - ETA: 1s - loss: 9.1110 - acc: 0.338 - ETA: 1s - loss: 9.1747 - acc: 0.337 - ETA: 1s - loss: 9.2748 - acc: 0.336 - ETA: 1s - loss: 9.1772 - acc: 0.345 - ETA: 1s - loss: 9.2968 - acc: 0.340 - ETA: 1s - loss: 9.1485 - acc: 0.346 - ETA: 1s - loss: 9.1397 - acc: 0.346 - ETA: 1s - loss: 9.1304 - acc: 0.348 - ETA: 1s - loss: 9.1369 - acc: 0.346 - ETA: 1s - loss: 9.2194 - acc: 0.340 - ETA: 1s - loss: 9.1495 - acc: 0.345 - ETA: 1s - loss: 9.1488 - acc: 0.344 - ETA: 1s - loss: 9.1548 - acc: 0.346 - ETA: 1s - loss: 9.1352 - acc: 0.347 - ETA: 1s - loss: 9.0845 - acc: 0.350 - ETA: 1s - loss: 9.0573 - acc: 0.350 - ETA: 0s - loss: 9.0590 - acc: 0.351 - ETA: 0s - loss: 9.0460 - acc: 0.350 - ETA: 0s - loss: 9.0587 - acc: 0.349 - ETA: 0s - loss: 9.0748 - acc: 0.349 - ETA: 0s - loss: 9.0714 - acc: 0.351 - ETA: 0s - loss: 9.0798 - acc: 0.350 - ETA: 0s - loss: 9.0308 - acc: 0.353 - ETA: 0s - loss: 9.0424 - acc: 0.354 - ETA: 0s - loss: 9.0182 - acc: 0.354 - ETA: 0s - loss: 9.0203 - acc: 0.354 - ETA: 0s - loss: 9.0377 - acc: 0.353 - ETA: 0s - loss: 9.0293 - acc: 0.354 - ETA: 0s - loss: 9.0412 - acc: 0.353 - ETA: 0s - loss: 9.0323 - acc: 0.355 - ETA: 0s - loss: 9.0395 - acc: 0.354 - ETA: 0s - loss: 9.0032 - acc: 0.356 - ETA: 0s - loss: 8.9722 - acc: 0.358 - ETA: 0s - loss: 8.9769 - acc: 0.358 - ETA: 0s - loss: 8.9801 - acc: 0.3587Epoch 00002: val_loss improved from 9.97174 to 9.05774, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 2s 339us/step - loss: 8.9566 - acc: 0.3605 - val_loss: 9.0577 - val_acc: 0.3473\n",
      "Epoch 3/20\n",
      "6620/6680 [============================>.] - ETA: 3s - loss: 8.3977 - acc: 0.350 - ETA: 1s - loss: 7.6259 - acc: 0.470 - ETA: 1s - loss: 8.3161 - acc: 0.433 - ETA: 1s - loss: 8.2563 - acc: 0.438 - ETA: 1s - loss: 8.0900 - acc: 0.454 - ETA: 1s - loss: 8.0843 - acc: 0.450 - ETA: 1s - loss: 7.8917 - acc: 0.464 - ETA: 1s - loss: 8.0404 - acc: 0.453 - ETA: 1s - loss: 7.9615 - acc: 0.457 - ETA: 1s - loss: 7.9808 - acc: 0.457 - ETA: 1s - loss: 8.0748 - acc: 0.452 - ETA: 1s - loss: 8.0793 - acc: 0.451 - ETA: 1s - loss: 8.0541 - acc: 0.452 - ETA: 1s - loss: 8.1707 - acc: 0.446 - ETA: 1s - loss: 8.1829 - acc: 0.443 - ETA: 1s - loss: 8.2387 - acc: 0.441 - ETA: 1s - loss: 8.2595 - acc: 0.439 - ETA: 1s - loss: 8.2840 - acc: 0.437 - ETA: 1s - loss: 8.2818 - acc: 0.438 - ETA: 1s - loss: 8.2993 - acc: 0.437 - ETA: 1s - loss: 8.3244 - acc: 0.436 - ETA: 0s - loss: 8.3196 - acc: 0.436 - ETA: 0s - loss: 8.3409 - acc: 0.434 - ETA: 0s - loss: 8.3770 - acc: 0.432 - ETA: 0s - loss: 8.3702 - acc: 0.432 - ETA: 0s - loss: 8.4095 - acc: 0.431 - ETA: 0s - loss: 8.3576 - acc: 0.434 - ETA: 0s - loss: 8.3561 - acc: 0.434 - ETA: 0s - loss: 8.3507 - acc: 0.434 - ETA: 0s - loss: 8.3539 - acc: 0.434 - ETA: 0s - loss: 8.3472 - acc: 0.434 - ETA: 0s - loss: 8.3273 - acc: 0.436 - ETA: 0s - loss: 8.3368 - acc: 0.435 - ETA: 0s - loss: 8.3203 - acc: 0.436 - ETA: 0s - loss: 8.3555 - acc: 0.434 - ETA: 0s - loss: 8.3761 - acc: 0.432 - ETA: 0s - loss: 8.3723 - acc: 0.432 - ETA: 0s - loss: 8.3997 - acc: 0.431 - ETA: 0s - loss: 8.3997 - acc: 0.430 - ETA: 0s - loss: 8.4146 - acc: 0.4296Epoch 00003: val_loss improved from 9.05774 to 8.89700, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 2s 337us/step - loss: 8.4282 - acc: 0.4286 - val_loss: 8.8970 - val_acc: 0.3653\n",
      "Epoch 4/20\n",
      "6560/6680 [============================>.] - ETA: 3s - loss: 12.2520 - acc: 0.20 - ETA: 2s - loss: 8.6971 - acc: 0.4350 - ETA: 2s - loss: 9.1452 - acc: 0.400 - ETA: 1s - loss: 9.0012 - acc: 0.411 - ETA: 1s - loss: 9.0291 - acc: 0.414 - ETA: 1s - loss: 8.9446 - acc: 0.421 - ETA: 1s - loss: 8.8306 - acc: 0.429 - ETA: 1s - loss: 8.6011 - acc: 0.444 - ETA: 1s - loss: 8.5397 - acc: 0.447 - ETA: 1s - loss: 8.5576 - acc: 0.446 - ETA: 1s - loss: 8.6299 - acc: 0.442 - ETA: 1s - loss: 8.5944 - acc: 0.445 - ETA: 1s - loss: 8.4804 - acc: 0.451 - ETA: 1s - loss: 8.4597 - acc: 0.452 - ETA: 1s - loss: 8.3842 - acc: 0.456 - ETA: 1s - loss: 8.3162 - acc: 0.460 - ETA: 1s - loss: 8.3090 - acc: 0.460 - ETA: 1s - loss: 8.2773 - acc: 0.461 - ETA: 1s - loss: 8.2007 - acc: 0.466 - ETA: 1s - loss: 8.2048 - acc: 0.465 - ETA: 1s - loss: 8.1814 - acc: 0.467 - ETA: 0s - loss: 8.1471 - acc: 0.468 - ETA: 0s - loss: 8.1930 - acc: 0.466 - ETA: 0s - loss: 8.2173 - acc: 0.464 - ETA: 0s - loss: 8.2431 - acc: 0.463 - ETA: 0s - loss: 8.2245 - acc: 0.463 - ETA: 0s - loss: 8.2388 - acc: 0.462 - ETA: 0s - loss: 8.2689 - acc: 0.460 - ETA: 0s - loss: 8.2569 - acc: 0.460 - ETA: 0s - loss: 8.2627 - acc: 0.460 - ETA: 0s - loss: 8.2309 - acc: 0.462 - ETA: 0s - loss: 8.2018 - acc: 0.464 - ETA: 0s - loss: 8.2112 - acc: 0.464 - ETA: 0s - loss: 8.1936 - acc: 0.465 - ETA: 0s - loss: 8.2081 - acc: 0.464 - ETA: 0s - loss: 8.2275 - acc: 0.463 - ETA: 0s - loss: 8.2206 - acc: 0.462 - ETA: 0s - loss: 8.2474 - acc: 0.460 - ETA: 0s - loss: 8.2649 - acc: 0.459 - ETA: 0s - loss: 8.2687 - acc: 0.4599Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s 337us/step - loss: 8.2793 - acc: 0.4593 - val_loss: 8.9318 - val_acc: 0.3617\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6560/6680 [============================>.] - ETA: 3s - loss: 8.9764 - acc: 0.400 - ETA: 2s - loss: 8.6801 - acc: 0.427 - ETA: 2s - loss: 8.4524 - acc: 0.441 - ETA: 1s - loss: 8.2732 - acc: 0.457 - ETA: 1s - loss: 8.2396 - acc: 0.461 - ETA: 1s - loss: 8.0978 - acc: 0.475 - ETA: 1s - loss: 8.2195 - acc: 0.469 - ETA: 1s - loss: 8.2443 - acc: 0.467 - ETA: 1s - loss: 8.2658 - acc: 0.466 - ETA: 1s - loss: 8.1926 - acc: 0.472 - ETA: 1s - loss: 8.0939 - acc: 0.480 - ETA: 1s - loss: 8.0258 - acc: 0.485 - ETA: 1s - loss: 8.1162 - acc: 0.480 - ETA: 1s - loss: 8.1493 - acc: 0.480 - ETA: 1s - loss: 8.1187 - acc: 0.481 - ETA: 1s - loss: 8.1477 - acc: 0.480 - ETA: 1s - loss: 8.1328 - acc: 0.482 - ETA: 1s - loss: 8.0694 - acc: 0.486 - ETA: 1s - loss: 8.0694 - acc: 0.487 - ETA: 1s - loss: 8.0914 - acc: 0.485 - ETA: 1s - loss: 8.1659 - acc: 0.481 - ETA: 0s - loss: 8.1575 - acc: 0.482 - ETA: 0s - loss: 8.1517 - acc: 0.481 - ETA: 0s - loss: 8.1502 - acc: 0.482 - ETA: 0s - loss: 8.1393 - acc: 0.483 - ETA: 0s - loss: 8.1424 - acc: 0.482 - ETA: 0s - loss: 8.1690 - acc: 0.481 - ETA: 0s - loss: 8.1900 - acc: 0.479 - ETA: 0s - loss: 8.1785 - acc: 0.480 - ETA: 0s - loss: 8.1809 - acc: 0.480 - ETA: 0s - loss: 8.2221 - acc: 0.478 - ETA: 0s - loss: 8.2458 - acc: 0.476 - ETA: 0s - loss: 8.2153 - acc: 0.478 - ETA: 0s - loss: 8.1977 - acc: 0.478 - ETA: 0s - loss: 8.2014 - acc: 0.478 - ETA: 0s - loss: 8.2037 - acc: 0.478 - ETA: 0s - loss: 8.2381 - acc: 0.476 - ETA: 0s - loss: 8.2535 - acc: 0.474 - ETA: 0s - loss: 8.2351 - acc: 0.475 - ETA: 0s - loss: 8.2225 - acc: 0.4765Epoch 00005: val_loss improved from 8.89700 to 8.80471, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 2s 338us/step - loss: 8.2194 - acc: 0.4762 - val_loss: 8.8047 - val_acc: 0.3880\n",
      "Epoch 6/20\n",
      "6580/6680 [============================>.] - ETA: 2s - loss: 8.0773 - acc: 0.500 - ETA: 2s - loss: 9.1709 - acc: 0.416 - ETA: 1s - loss: 8.3216 - acc: 0.475 - ETA: 1s - loss: 8.3440 - acc: 0.475 - ETA: 1s - loss: 8.3433 - acc: 0.477 - ETA: 1s - loss: 8.2797 - acc: 0.480 - ETA: 1s - loss: 8.1677 - acc: 0.486 - ETA: 1s - loss: 8.1197 - acc: 0.490 - ETA: 1s - loss: 8.2574 - acc: 0.481 - ETA: 1s - loss: 8.2608 - acc: 0.481 - ETA: 1s - loss: 8.2514 - acc: 0.482 - ETA: 1s - loss: 8.2325 - acc: 0.482 - ETA: 1s - loss: 8.2652 - acc: 0.479 - ETA: 1s - loss: 8.2586 - acc: 0.480 - ETA: 1s - loss: 8.2744 - acc: 0.479 - ETA: 1s - loss: 8.2417 - acc: 0.481 - ETA: 1s - loss: 8.2409 - acc: 0.480 - ETA: 1s - loss: 8.2116 - acc: 0.482 - ETA: 1s - loss: 8.2419 - acc: 0.480 - ETA: 1s - loss: 8.2388 - acc: 0.480 - ETA: 1s - loss: 8.2360 - acc: 0.480 - ETA: 0s - loss: 8.2049 - acc: 0.482 - ETA: 0s - loss: 8.1999 - acc: 0.482 - ETA: 0s - loss: 8.1459 - acc: 0.486 - ETA: 0s - loss: 8.1352 - acc: 0.487 - ETA: 0s - loss: 8.1338 - acc: 0.487 - ETA: 0s - loss: 8.1614 - acc: 0.485 - ETA: 0s - loss: 8.1697 - acc: 0.484 - ETA: 0s - loss: 8.1598 - acc: 0.485 - ETA: 0s - loss: 8.1569 - acc: 0.485 - ETA: 0s - loss: 8.1829 - acc: 0.483 - ETA: 0s - loss: 8.1762 - acc: 0.484 - ETA: 0s - loss: 8.1977 - acc: 0.482 - ETA: 0s - loss: 8.2207 - acc: 0.481 - ETA: 0s - loss: 8.2153 - acc: 0.481 - ETA: 0s - loss: 8.1973 - acc: 0.483 - ETA: 0s - loss: 8.1707 - acc: 0.484 - ETA: 0s - loss: 8.1626 - acc: 0.485 - ETA: 0s - loss: 8.1803 - acc: 0.484 - ETA: 0s - loss: 8.1881 - acc: 0.4840Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s 335us/step - loss: 8.1985 - acc: 0.4835 - val_loss: 8.8242 - val_acc: 0.3844\n",
      "Epoch 7/20\n",
      "6600/6680 [============================>.] - ETA: 2s - loss: 9.7139 - acc: 0.400 - ETA: 2s - loss: 7.7790 - acc: 0.512 - ETA: 2s - loss: 8.3735 - acc: 0.478 - ETA: 2s - loss: 8.5580 - acc: 0.466 - ETA: 2s - loss: 8.3429 - acc: 0.478 - ETA: 2s - loss: 8.4368 - acc: 0.472 - ETA: 1s - loss: 8.3499 - acc: 0.477 - ETA: 1s - loss: 8.3250 - acc: 0.478 - ETA: 1s - loss: 8.1928 - acc: 0.486 - ETA: 1s - loss: 8.2156 - acc: 0.485 - ETA: 1s - loss: 8.1587 - acc: 0.489 - ETA: 1s - loss: 8.1607 - acc: 0.489 - ETA: 1s - loss: 8.1887 - acc: 0.487 - ETA: 1s - loss: 8.2499 - acc: 0.483 - ETA: 1s - loss: 8.2598 - acc: 0.483 - ETA: 1s - loss: 8.2805 - acc: 0.482 - ETA: 1s - loss: 8.3221 - acc: 0.479 - ETA: 1s - loss: 8.2890 - acc: 0.482 - ETA: 1s - loss: 8.2375 - acc: 0.485 - ETA: 1s - loss: 8.2502 - acc: 0.484 - ETA: 1s - loss: 8.2266 - acc: 0.486 - ETA: 1s - loss: 8.1994 - acc: 0.488 - ETA: 1s - loss: 8.1612 - acc: 0.490 - ETA: 1s - loss: 8.1745 - acc: 0.489 - ETA: 0s - loss: 8.2048 - acc: 0.487 - ETA: 0s - loss: 8.2114 - acc: 0.487 - ETA: 0s - loss: 8.2449 - acc: 0.485 - ETA: 0s - loss: 8.2432 - acc: 0.485 - ETA: 0s - loss: 8.2407 - acc: 0.485 - ETA: 0s - loss: 8.2379 - acc: 0.486 - ETA: 0s - loss: 8.2321 - acc: 0.486 - ETA: 0s - loss: 8.1974 - acc: 0.488 - ETA: 0s - loss: 8.1868 - acc: 0.489 - ETA: 0s - loss: 8.2025 - acc: 0.488 - ETA: 0s - loss: 8.2114 - acc: 0.487 - ETA: 0s - loss: 8.2057 - acc: 0.487 - ETA: 0s - loss: 8.1888 - acc: 0.488 - ETA: 0s - loss: 8.1893 - acc: 0.488 - ETA: 0s - loss: 8.2111 - acc: 0.487 - ETA: 0s - loss: 8.2093 - acc: 0.486 - ETA: 0s - loss: 8.1931 - acc: 0.487 - ETA: 0s - loss: 8.1908 - acc: 0.487 - ETA: 0s - loss: 8.1828 - acc: 0.4882Epoch 00007: val_loss improved from 8.80471 to 8.79097, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 2s 364us/step - loss: 8.1838 - acc: 0.4882 - val_loss: 8.7910 - val_acc: 0.3916\n",
      "Epoch 8/20\n",
      "6640/6680 [============================>.] - ETA: 2s - loss: 8.0609 - acc: 0.500 - ETA: 2s - loss: 7.4373 - acc: 0.538 - ETA: 2s - loss: 7.5164 - acc: 0.531 - ETA: 2s - loss: 7.4978 - acc: 0.533 - ETA: 2s - loss: 7.5681 - acc: 0.528 - ETA: 2s - loss: 7.6009 - acc: 0.524 - ETA: 1s - loss: 7.9584 - acc: 0.501 - ETA: 1s - loss: 7.9735 - acc: 0.500 - ETA: 1s - loss: 7.9343 - acc: 0.504 - ETA: 1s - loss: 8.0508 - acc: 0.497 - ETA: 1s - loss: 8.1271 - acc: 0.492 - ETA: 1s - loss: 8.0634 - acc: 0.496 - ETA: 1s - loss: 8.0991 - acc: 0.494 - ETA: 1s - loss: 8.1384 - acc: 0.492 - ETA: 1s - loss: 8.1487 - acc: 0.491 - ETA: 1s - loss: 8.1862 - acc: 0.489 - ETA: 1s - loss: 8.1050 - acc: 0.494 - ETA: 1s - loss: 8.0971 - acc: 0.494 - ETA: 1s - loss: 8.1026 - acc: 0.493 - ETA: 1s - loss: 8.1300 - acc: 0.491 - ETA: 1s - loss: 8.1802 - acc: 0.488 - ETA: 1s - loss: 8.1696 - acc: 0.489 - ETA: 1s - loss: 8.1694 - acc: 0.489 - ETA: 1s - loss: 8.1693 - acc: 0.489 - ETA: 1s - loss: 8.1562 - acc: 0.490 - ETA: 1s - loss: 8.1570 - acc: 0.490 - ETA: 0s - loss: 8.1372 - acc: 0.491 - ETA: 0s - loss: 8.1095 - acc: 0.493 - ETA: 0s - loss: 8.1002 - acc: 0.494 - ETA: 0s - loss: 8.0882 - acc: 0.495 - ETA: 0s - loss: 8.1019 - acc: 0.494 - ETA: 0s - loss: 8.0971 - acc: 0.494 - ETA: 0s - loss: 8.1180 - acc: 0.492 - ETA: 0s - loss: 8.1325 - acc: 0.491 - ETA: 0s - loss: 8.1523 - acc: 0.490 - ETA: 0s - loss: 8.1803 - acc: 0.489 - ETA: 0s - loss: 8.1975 - acc: 0.487 - ETA: 0s - loss: 8.2088 - acc: 0.487 - ETA: 0s - loss: 8.2051 - acc: 0.487 - ETA: 0s - loss: 8.1905 - acc: 0.488 - ETA: 0s - loss: 8.2005 - acc: 0.487 - ETA: 0s - loss: 8.1920 - acc: 0.488 - ETA: 0s - loss: 8.1910 - acc: 0.488 - ETA: 0s - loss: 8.1881 - acc: 0.4883Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s 364us/step - loss: 8.1825 - acc: 0.4886 - val_loss: 8.7947 - val_acc: 0.3892\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6520/6680 [============================>.] - ETA: 2s - loss: 10.4773 - acc: 0.35 - ETA: 2s - loss: 7.8078 - acc: 0.5111 - ETA: 2s - loss: 8.3117 - acc: 0.479 - ETA: 2s - loss: 8.5217 - acc: 0.468 - ETA: 2s - loss: 8.6650 - acc: 0.459 - ETA: 1s - loss: 8.6454 - acc: 0.461 - ETA: 1s - loss: 8.5779 - acc: 0.465 - ETA: 1s - loss: 8.5421 - acc: 0.467 - ETA: 1s - loss: 8.5068 - acc: 0.469 - ETA: 1s - loss: 8.5594 - acc: 0.466 - ETA: 1s - loss: 8.4679 - acc: 0.472 - ETA: 1s - loss: 8.4960 - acc: 0.470 - ETA: 1s - loss: 8.4952 - acc: 0.470 - ETA: 1s - loss: 8.4766 - acc: 0.471 - ETA: 1s - loss: 8.4098 - acc: 0.475 - ETA: 1s - loss: 8.4352 - acc: 0.474 - ETA: 1s - loss: 8.4419 - acc: 0.473 - ETA: 1s - loss: 8.4914 - acc: 0.470 - ETA: 1s - loss: 8.5047 - acc: 0.470 - ETA: 1s - loss: 8.4593 - acc: 0.473 - ETA: 1s - loss: 8.4410 - acc: 0.474 - ETA: 1s - loss: 8.3838 - acc: 0.477 - ETA: 1s - loss: 8.3567 - acc: 0.479 - ETA: 1s - loss: 8.3631 - acc: 0.479 - ETA: 0s - loss: 8.3248 - acc: 0.481 - ETA: 0s - loss: 8.3329 - acc: 0.480 - ETA: 0s - loss: 8.2996 - acc: 0.482 - ETA: 0s - loss: 8.3060 - acc: 0.482 - ETA: 0s - loss: 8.2676 - acc: 0.485 - ETA: 0s - loss: 8.2612 - acc: 0.485 - ETA: 0s - loss: 8.2577 - acc: 0.485 - ETA: 0s - loss: 8.2746 - acc: 0.484 - ETA: 0s - loss: 8.2775 - acc: 0.484 - ETA: 0s - loss: 8.2583 - acc: 0.486 - ETA: 0s - loss: 8.2341 - acc: 0.487 - ETA: 0s - loss: 8.2468 - acc: 0.486 - ETA: 0s - loss: 8.2277 - acc: 0.488 - ETA: 0s - loss: 8.2036 - acc: 0.489 - ETA: 0s - loss: 8.2026 - acc: 0.489 - ETA: 0s - loss: 8.1724 - acc: 0.491 - ETA: 0s - loss: 8.1622 - acc: 0.492 - ETA: 0s - loss: 8.1598 - acc: 0.492 - ETA: 0s - loss: 8.1578 - acc: 0.4925Epoch 00009: val_loss improved from 8.79097 to 8.78582, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 2s 366us/step - loss: 8.1772 - acc: 0.4913 - val_loss: 8.7858 - val_acc: 0.3880\n",
      "Epoch 10/20\n",
      "6640/6680 [============================>.] - ETA: 3s - loss: 10.4770 - acc: 0.35 - ETA: 2s - loss: 8.4229 - acc: 0.4778 - ETA: 2s - loss: 7.8387 - acc: 0.513 - ETA: 1s - loss: 8.0620 - acc: 0.500 - ETA: 1s - loss: 8.3226 - acc: 0.483 - ETA: 1s - loss: 8.2489 - acc: 0.488 - ETA: 1s - loss: 8.4089 - acc: 0.478 - ETA: 1s - loss: 8.4435 - acc: 0.476 - ETA: 1s - loss: 8.4015 - acc: 0.478 - ETA: 1s - loss: 8.4092 - acc: 0.477 - ETA: 1s - loss: 8.3755 - acc: 0.479 - ETA: 1s - loss: 8.3300 - acc: 0.482 - ETA: 1s - loss: 8.3736 - acc: 0.479 - ETA: 1s - loss: 8.3523 - acc: 0.480 - ETA: 1s - loss: 8.4199 - acc: 0.476 - ETA: 1s - loss: 8.4293 - acc: 0.476 - ETA: 1s - loss: 8.4531 - acc: 0.474 - ETA: 1s - loss: 8.4423 - acc: 0.475 - ETA: 1s - loss: 8.4707 - acc: 0.473 - ETA: 1s - loss: 8.4245 - acc: 0.476 - ETA: 1s - loss: 8.4103 - acc: 0.476 - ETA: 1s - loss: 8.3971 - acc: 0.477 - ETA: 0s - loss: 8.3764 - acc: 0.479 - ETA: 0s - loss: 8.3885 - acc: 0.478 - ETA: 0s - loss: 8.3799 - acc: 0.478 - ETA: 0s - loss: 8.3188 - acc: 0.482 - ETA: 0s - loss: 8.3316 - acc: 0.481 - ETA: 0s - loss: 8.3018 - acc: 0.483 - ETA: 0s - loss: 8.3214 - acc: 0.481 - ETA: 0s - loss: 8.3162 - acc: 0.482 - ETA: 0s - loss: 8.3089 - acc: 0.482 - ETA: 0s - loss: 8.3093 - acc: 0.482 - ETA: 0s - loss: 8.3071 - acc: 0.482 - ETA: 0s - loss: 8.3184 - acc: 0.481 - ETA: 0s - loss: 8.2651 - acc: 0.483 - ETA: 0s - loss: 8.2411 - acc: 0.485 - ETA: 0s - loss: 8.2167 - acc: 0.486 - ETA: 0s - loss: 8.2034 - acc: 0.486 - ETA: 0s - loss: 8.1978 - acc: 0.487 - ETA: 0s - loss: 8.2029 - acc: 0.486 - ETA: 0s - loss: 8.1978 - acc: 0.4869Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s 335us/step - loss: 8.1945 - acc: 0.4871 - val_loss: 8.8657 - val_acc: 0.3760\n",
      "Epoch 11/20\n",
      "6620/6680 [============================>.] - ETA: 2s - loss: 11.2833 - acc: 0.30 - ETA: 1s - loss: 8.4712 - acc: 0.4700 - ETA: 1s - loss: 8.9363 - acc: 0.436 - ETA: 1s - loss: 8.7292 - acc: 0.451 - ETA: 1s - loss: 8.5654 - acc: 0.462 - ETA: 1s - loss: 8.6682 - acc: 0.457 - ETA: 1s - loss: 8.6372 - acc: 0.457 - ETA: 1s - loss: 8.6835 - acc: 0.454 - ETA: 1s - loss: 8.6618 - acc: 0.456 - ETA: 1s - loss: 8.5475 - acc: 0.461 - ETA: 1s - loss: 8.5378 - acc: 0.461 - ETA: 1s - loss: 8.5188 - acc: 0.463 - ETA: 1s - loss: 8.5278 - acc: 0.462 - ETA: 1s - loss: 8.4874 - acc: 0.465 - ETA: 1s - loss: 8.4541 - acc: 0.467 - ETA: 1s - loss: 8.3871 - acc: 0.472 - ETA: 1s - loss: 8.2962 - acc: 0.477 - ETA: 1s - loss: 8.2835 - acc: 0.478 - ETA: 1s - loss: 8.2686 - acc: 0.479 - ETA: 1s - loss: 8.2650 - acc: 0.479 - ETA: 1s - loss: 8.2468 - acc: 0.480 - ETA: 0s - loss: 8.3199 - acc: 0.475 - ETA: 0s - loss: 8.3237 - acc: 0.474 - ETA: 0s - loss: 8.2943 - acc: 0.476 - ETA: 0s - loss: 8.3216 - acc: 0.473 - ETA: 0s - loss: 8.2952 - acc: 0.474 - ETA: 0s - loss: 8.2860 - acc: 0.474 - ETA: 0s - loss: 8.2590 - acc: 0.475 - ETA: 0s - loss: 8.2544 - acc: 0.474 - ETA: 0s - loss: 8.2509 - acc: 0.473 - ETA: 0s - loss: 8.2984 - acc: 0.470 - ETA: 0s - loss: 8.3226 - acc: 0.467 - ETA: 0s - loss: 8.3281 - acc: 0.466 - ETA: 0s - loss: 8.3228 - acc: 0.466 - ETA: 0s - loss: 8.3379 - acc: 0.464 - ETA: 0s - loss: 8.3440 - acc: 0.464 - ETA: 0s - loss: 8.3281 - acc: 0.464 - ETA: 0s - loss: 8.3193 - acc: 0.465 - ETA: 0s - loss: 8.3036 - acc: 0.466 - ETA: 0s - loss: 8.2850 - acc: 0.4666Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s 331us/step - loss: 8.2751 - acc: 0.4669 - val_loss: 9.1258 - val_acc: 0.3665\n",
      "Epoch 12/20\n",
      "6560/6680 [============================>.] - ETA: 3s - loss: 8.0743 - acc: 0.500 - ETA: 1s - loss: 8.9761 - acc: 0.425 - ETA: 1s - loss: 8.6312 - acc: 0.444 - ETA: 1s - loss: 8.7107 - acc: 0.440 - ETA: 1s - loss: 8.6324 - acc: 0.447 - ETA: 1s - loss: 8.6640 - acc: 0.444 - ETA: 1s - loss: 8.7956 - acc: 0.433 - ETA: 1s - loss: 8.6286 - acc: 0.443 - ETA: 1s - loss: 8.5384 - acc: 0.446 - ETA: 1s - loss: 8.5870 - acc: 0.444 - ETA: 1s - loss: 8.6941 - acc: 0.438 - ETA: 1s - loss: 8.6855 - acc: 0.438 - ETA: 1s - loss: 8.6329 - acc: 0.441 - ETA: 1s - loss: 8.6412 - acc: 0.440 - ETA: 1s - loss: 8.6034 - acc: 0.441 - ETA: 1s - loss: 8.5285 - acc: 0.446 - ETA: 1s - loss: 8.5299 - acc: 0.447 - ETA: 1s - loss: 8.4958 - acc: 0.448 - ETA: 1s - loss: 8.5413 - acc: 0.444 - ETA: 1s - loss: 8.5182 - acc: 0.446 - ETA: 1s - loss: 8.4906 - acc: 0.448 - ETA: 0s - loss: 8.4771 - acc: 0.449 - ETA: 0s - loss: 8.4925 - acc: 0.449 - ETA: 0s - loss: 8.4933 - acc: 0.449 - ETA: 0s - loss: 8.4780 - acc: 0.450 - ETA: 0s - loss: 8.5249 - acc: 0.448 - ETA: 0s - loss: 8.5192 - acc: 0.448 - ETA: 0s - loss: 8.5092 - acc: 0.449 - ETA: 0s - loss: 8.4977 - acc: 0.449 - ETA: 0s - loss: 8.4563 - acc: 0.452 - ETA: 0s - loss: 8.4394 - acc: 0.453 - ETA: 0s - loss: 8.4309 - acc: 0.454 - ETA: 0s - loss: 8.4279 - acc: 0.455 - ETA: 0s - loss: 8.4174 - acc: 0.455 - ETA: 0s - loss: 8.3731 - acc: 0.458 - ETA: 0s - loss: 8.3767 - acc: 0.458 - ETA: 0s - loss: 8.3839 - acc: 0.457 - ETA: 0s - loss: 8.3781 - acc: 0.457 - ETA: 0s - loss: 8.3639 - acc: 0.458 - ETA: 0s - loss: 8.3657 - acc: 0.458 - ETA: 0s - loss: 8.3516 - acc: 0.458 - ETA: 0s - loss: 8.3472 - acc: 0.459 - ETA: 0s - loss: 8.3444 - acc: 0.4590Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s 360us/step - loss: 8.3107 - acc: 0.4611 - val_loss: 9.0537 - val_acc: 0.3737\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6620/6680 [============================>.] - ETA: 3s - loss: 8.8703 - acc: 0.450 - ETA: 2s - loss: 9.1502 - acc: 0.415 - ETA: 1s - loss: 8.8295 - acc: 0.436 - ETA: 1s - loss: 8.5700 - acc: 0.455 - ETA: 1s - loss: 8.4189 - acc: 0.462 - ETA: 1s - loss: 8.5576 - acc: 0.454 - ETA: 1s - loss: 8.5065 - acc: 0.456 - ETA: 1s - loss: 8.5305 - acc: 0.455 - ETA: 1s - loss: 8.6000 - acc: 0.452 - ETA: 1s - loss: 8.5841 - acc: 0.453 - ETA: 1s - loss: 8.6120 - acc: 0.452 - ETA: 1s - loss: 8.6005 - acc: 0.453 - ETA: 1s - loss: 8.5976 - acc: 0.454 - ETA: 1s - loss: 8.5172 - acc: 0.460 - ETA: 1s - loss: 8.5034 - acc: 0.461 - ETA: 1s - loss: 8.5117 - acc: 0.460 - ETA: 1s - loss: 8.4721 - acc: 0.463 - ETA: 1s - loss: 8.4240 - acc: 0.466 - ETA: 1s - loss: 8.3910 - acc: 0.468 - ETA: 1s - loss: 8.4169 - acc: 0.467 - ETA: 1s - loss: 8.3386 - acc: 0.471 - ETA: 1s - loss: 8.3114 - acc: 0.473 - ETA: 1s - loss: 8.2995 - acc: 0.473 - ETA: 1s - loss: 8.2542 - acc: 0.476 - ETA: 1s - loss: 8.2682 - acc: 0.475 - ETA: 1s - loss: 8.3135 - acc: 0.473 - ETA: 1s - loss: 8.2960 - acc: 0.474 - ETA: 1s - loss: 8.2870 - acc: 0.474 - ETA: 1s - loss: 8.2853 - acc: 0.474 - ETA: 1s - loss: 8.2706 - acc: 0.474 - ETA: 1s - loss: 8.2607 - acc: 0.475 - ETA: 1s - loss: 8.3127 - acc: 0.471 - ETA: 1s - loss: 8.3261 - acc: 0.471 - ETA: 1s - loss: 8.3062 - acc: 0.473 - ETA: 1s - loss: 8.3243 - acc: 0.472 - ETA: 1s - loss: 8.3290 - acc: 0.472 - ETA: 1s - loss: 8.3578 - acc: 0.470 - ETA: 1s - loss: 8.3376 - acc: 0.471 - ETA: 1s - loss: 8.3457 - acc: 0.471 - ETA: 0s - loss: 8.3892 - acc: 0.468 - ETA: 0s - loss: 8.3843 - acc: 0.469 - ETA: 0s - loss: 8.4070 - acc: 0.468 - ETA: 0s - loss: 8.3988 - acc: 0.468 - ETA: 0s - loss: 8.3567 - acc: 0.471 - ETA: 0s - loss: 8.3061 - acc: 0.474 - ETA: 0s - loss: 8.2698 - acc: 0.476 - ETA: 0s - loss: 8.2740 - acc: 0.476 - ETA: 0s - loss: 8.2808 - acc: 0.476 - ETA: 0s - loss: 8.2968 - acc: 0.475 - ETA: 0s - loss: 8.2682 - acc: 0.476 - ETA: 0s - loss: 8.2453 - acc: 0.478 - ETA: 0s - loss: 8.2169 - acc: 0.479 - ETA: 0s - loss: 8.2048 - acc: 0.480 - ETA: 0s - loss: 8.1998 - acc: 0.480 - ETA: 0s - loss: 8.1924 - acc: 0.481 - ETA: 0s - loss: 8.2094 - acc: 0.4805Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 471us/step - loss: 8.2110 - acc: 0.4802 - val_loss: 8.9366 - val_acc: 0.3940\n",
      "Epoch 14/20\n",
      "6520/6680 [============================>.] - ETA: 2s - loss: 10.5032 - acc: 0.35 - ETA: 2s - loss: 8.9801 - acc: 0.4333 - ETA: 2s - loss: 8.5999 - acc: 0.458 - ETA: 2s - loss: 8.5903 - acc: 0.462 - ETA: 2s - loss: 8.4026 - acc: 0.471 - ETA: 2s - loss: 8.5071 - acc: 0.466 - ETA: 1s - loss: 8.4310 - acc: 0.472 - ETA: 1s - loss: 8.3532 - acc: 0.477 - ETA: 1s - loss: 8.3035 - acc: 0.481 - ETA: 1s - loss: 8.3370 - acc: 0.479 - ETA: 1s - loss: 8.3834 - acc: 0.476 - ETA: 1s - loss: 8.2986 - acc: 0.481 - ETA: 1s - loss: 8.2981 - acc: 0.481 - ETA: 1s - loss: 8.3380 - acc: 0.479 - ETA: 1s - loss: 8.2068 - acc: 0.487 - ETA: 1s - loss: 8.1862 - acc: 0.488 - ETA: 1s - loss: 8.1417 - acc: 0.490 - ETA: 1s - loss: 8.0687 - acc: 0.495 - ETA: 1s - loss: 8.0683 - acc: 0.495 - ETA: 1s - loss: 8.0280 - acc: 0.497 - ETA: 1s - loss: 8.0516 - acc: 0.496 - ETA: 1s - loss: 8.0182 - acc: 0.498 - ETA: 1s - loss: 8.0501 - acc: 0.496 - ETA: 1s - loss: 8.0766 - acc: 0.494 - ETA: 1s - loss: 8.0854 - acc: 0.494 - ETA: 1s - loss: 8.1077 - acc: 0.492 - ETA: 1s - loss: 8.0754 - acc: 0.495 - ETA: 1s - loss: 8.0750 - acc: 0.495 - ETA: 1s - loss: 8.0997 - acc: 0.493 - ETA: 1s - loss: 8.1027 - acc: 0.493 - ETA: 1s - loss: 8.0781 - acc: 0.495 - ETA: 1s - loss: 8.0314 - acc: 0.498 - ETA: 0s - loss: 8.0510 - acc: 0.497 - ETA: 0s - loss: 8.0476 - acc: 0.497 - ETA: 0s - loss: 8.0833 - acc: 0.495 - ETA: 0s - loss: 8.1202 - acc: 0.493 - ETA: 0s - loss: 8.1424 - acc: 0.491 - ETA: 0s - loss: 8.1128 - acc: 0.493 - ETA: 0s - loss: 8.1240 - acc: 0.492 - ETA: 0s - loss: 8.1231 - acc: 0.492 - ETA: 0s - loss: 8.1537 - acc: 0.490 - ETA: 0s - loss: 8.1857 - acc: 0.488 - ETA: 0s - loss: 8.1914 - acc: 0.487 - ETA: 0s - loss: 8.2255 - acc: 0.485 - ETA: 0s - loss: 8.2051 - acc: 0.486 - ETA: 0s - loss: 8.2053 - acc: 0.486 - ETA: 0s - loss: 8.1700 - acc: 0.4890Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 407us/step - loss: 8.1819 - acc: 0.4883 - val_loss: 8.8698 - val_acc: 0.3952\n",
      "Epoch 15/20\n",
      "6600/6680 [============================>.] - ETA: 2s - loss: 9.6714 - acc: 0.400 - ETA: 2s - loss: 8.7757 - acc: 0.455 - ETA: 2s - loss: 8.5357 - acc: 0.467 - ETA: 2s - loss: 8.1904 - acc: 0.490 - ETA: 1s - loss: 7.9213 - acc: 0.505 - ETA: 1s - loss: 8.0495 - acc: 0.495 - ETA: 1s - loss: 7.9714 - acc: 0.501 - ETA: 1s - loss: 8.0806 - acc: 0.494 - ETA: 1s - loss: 8.1156 - acc: 0.493 - ETA: 1s - loss: 8.1640 - acc: 0.490 - ETA: 1s - loss: 8.0588 - acc: 0.496 - ETA: 1s - loss: 8.0149 - acc: 0.499 - ETA: 1s - loss: 8.0619 - acc: 0.496 - ETA: 1s - loss: 8.0770 - acc: 0.495 - ETA: 1s - loss: 8.0476 - acc: 0.497 - ETA: 1s - loss: 8.0526 - acc: 0.497 - ETA: 1s - loss: 8.0973 - acc: 0.494 - ETA: 1s - loss: 8.1458 - acc: 0.490 - ETA: 1s - loss: 8.1582 - acc: 0.489 - ETA: 1s - loss: 8.1997 - acc: 0.486 - ETA: 1s - loss: 8.1993 - acc: 0.486 - ETA: 1s - loss: 8.2255 - acc: 0.484 - ETA: 0s - loss: 8.2003 - acc: 0.485 - ETA: 0s - loss: 8.2007 - acc: 0.485 - ETA: 0s - loss: 8.1604 - acc: 0.487 - ETA: 0s - loss: 8.1352 - acc: 0.488 - ETA: 0s - loss: 8.1914 - acc: 0.485 - ETA: 0s - loss: 8.1949 - acc: 0.485 - ETA: 0s - loss: 8.1817 - acc: 0.485 - ETA: 0s - loss: 8.1964 - acc: 0.484 - ETA: 0s - loss: 8.2128 - acc: 0.483 - ETA: 0s - loss: 8.1927 - acc: 0.484 - ETA: 0s - loss: 8.1579 - acc: 0.486 - ETA: 0s - loss: 8.1866 - acc: 0.484 - ETA: 0s - loss: 8.2069 - acc: 0.483 - ETA: 0s - loss: 8.2175 - acc: 0.482 - ETA: 0s - loss: 8.2444 - acc: 0.481 - ETA: 0s - loss: 8.2529 - acc: 0.480 - ETA: 0s - loss: 8.2076 - acc: 0.483 - ETA: 0s - loss: 8.2110 - acc: 0.483 - ETA: 0s - loss: 8.1785 - acc: 0.485 - ETA: 0s - loss: 8.1751 - acc: 0.485 - ETA: 0s - loss: 8.1977 - acc: 0.4841Epoch 00015: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s 361us/step - loss: 8.2015 - acc: 0.4838 - val_loss: 8.9062 - val_acc: 0.3832\n",
      "Epoch 16/20\n",
      "6580/6680 [============================>.] - ETA: 2s - loss: 7.2569 - acc: 0.550 - ETA: 2s - loss: 8.0601 - acc: 0.488 - ETA: 2s - loss: 8.2238 - acc: 0.478 - ETA: 2s - loss: 8.2522 - acc: 0.475 - ETA: 2s - loss: 8.1799 - acc: 0.482 - ETA: 1s - loss: 8.2376 - acc: 0.481 - ETA: 1s - loss: 8.2081 - acc: 0.484 - ETA: 1s - loss: 8.3038 - acc: 0.477 - ETA: 1s - loss: 8.2345 - acc: 0.482 - ETA: 1s - loss: 8.1929 - acc: 0.485 - ETA: 1s - loss: 8.0975 - acc: 0.492 - ETA: 1s - loss: 8.1308 - acc: 0.490 - ETA: 1s - loss: 8.0418 - acc: 0.496 - ETA: 1s - loss: 8.0997 - acc: 0.492 - ETA: 1s - loss: 7.9961 - acc: 0.499 - ETA: 1s - loss: 7.9940 - acc: 0.500 - ETA: 1s - loss: 8.0548 - acc: 0.496 - ETA: 1s - loss: 8.0786 - acc: 0.495 - ETA: 1s - loss: 8.0886 - acc: 0.494 - ETA: 1s - loss: 8.1240 - acc: 0.492 - ETA: 1s - loss: 8.1208 - acc: 0.493 - ETA: 1s - loss: 8.1232 - acc: 0.493 - ETA: 1s - loss: 8.1213 - acc: 0.492 - ETA: 0s - loss: 8.0512 - acc: 0.497 - ETA: 0s - loss: 8.0599 - acc: 0.496 - ETA: 0s - loss: 8.0818 - acc: 0.495 - ETA: 0s - loss: 8.0978 - acc: 0.494 - ETA: 0s - loss: 8.1192 - acc: 0.492 - ETA: 0s - loss: 8.1500 - acc: 0.490 - ETA: 0s - loss: 8.1682 - acc: 0.489 - ETA: 0s - loss: 8.1709 - acc: 0.489 - ETA: 0s - loss: 8.1642 - acc: 0.490 - ETA: 0s - loss: 8.1746 - acc: 0.489 - ETA: 0s - loss: 8.1564 - acc: 0.490 - ETA: 0s - loss: 8.1476 - acc: 0.491 - ETA: 0s - loss: 8.1656 - acc: 0.489 - ETA: 0s - loss: 8.1434 - acc: 0.491 - ETA: 0s - loss: 8.1611 - acc: 0.490 - ETA: 0s - loss: 8.1902 - acc: 0.488 - ETA: 0s - loss: 8.1847 - acc: 0.488 - ETA: 0s - loss: 8.1837 - acc: 0.488 - ETA: 0s - loss: 8.1881 - acc: 0.4888Epoch 00016: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s 349us/step - loss: 8.1814 - acc: 0.4892 - val_loss: 8.8301 - val_acc: 0.3964\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6600/6680 [============================>.] - ETA: 3s - loss: 11.2830 - acc: 0.30 - ETA: 2s - loss: 8.4316 - acc: 0.4722 - ETA: 2s - loss: 7.9763 - acc: 0.500 - ETA: 2s - loss: 8.0996 - acc: 0.494 - ETA: 1s - loss: 8.4074 - acc: 0.475 - ETA: 1s - loss: 8.3713 - acc: 0.478 - ETA: 1s - loss: 8.3879 - acc: 0.477 - ETA: 1s - loss: 8.2973 - acc: 0.483 - ETA: 1s - loss: 8.2210 - acc: 0.488 - ETA: 1s - loss: 8.3220 - acc: 0.482 - ETA: 1s - loss: 8.3162 - acc: 0.482 - ETA: 1s - loss: 8.2783 - acc: 0.485 - ETA: 1s - loss: 8.3016 - acc: 0.483 - ETA: 1s - loss: 8.2587 - acc: 0.486 - ETA: 1s - loss: 8.2239 - acc: 0.488 - ETA: 1s - loss: 8.2078 - acc: 0.489 - ETA: 1s - loss: 8.2297 - acc: 0.488 - ETA: 1s - loss: 8.1798 - acc: 0.491 - ETA: 1s - loss: 8.1796 - acc: 0.491 - ETA: 1s - loss: 8.2263 - acc: 0.488 - ETA: 1s - loss: 8.2281 - acc: 0.488 - ETA: 1s - loss: 8.2584 - acc: 0.486 - ETA: 1s - loss: 8.2504 - acc: 0.487 - ETA: 1s - loss: 8.2569 - acc: 0.486 - ETA: 0s - loss: 8.2189 - acc: 0.489 - ETA: 0s - loss: 8.1880 - acc: 0.491 - ETA: 0s - loss: 8.1877 - acc: 0.491 - ETA: 0s - loss: 8.1641 - acc: 0.492 - ETA: 0s - loss: 8.1463 - acc: 0.493 - ETA: 0s - loss: 8.1366 - acc: 0.493 - ETA: 0s - loss: 8.1606 - acc: 0.492 - ETA: 0s - loss: 8.1248 - acc: 0.494 - ETA: 0s - loss: 8.1322 - acc: 0.494 - ETA: 0s - loss: 8.1361 - acc: 0.494 - ETA: 0s - loss: 8.1430 - acc: 0.493 - ETA: 0s - loss: 8.1492 - acc: 0.493 - ETA: 0s - loss: 8.1775 - acc: 0.491 - ETA: 0s - loss: 8.1848 - acc: 0.491 - ETA: 0s - loss: 8.1894 - acc: 0.490 - ETA: 0s - loss: 8.1862 - acc: 0.490 - ETA: 0s - loss: 8.1683 - acc: 0.492 - ETA: 0s - loss: 8.1703 - acc: 0.4920Epoch 00017: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s 350us/step - loss: 8.1689 - acc: 0.4921 - val_loss: 8.8586 - val_acc: 0.3964\n",
      "Epoch 18/20\n",
      "6560/6680 [============================>.] - ETA: 2s - loss: 8.8651 - acc: 0.450 - ETA: 2s - loss: 8.2894 - acc: 0.485 - ETA: 2s - loss: 8.0031 - acc: 0.503 - ETA: 2s - loss: 8.0970 - acc: 0.497 - ETA: 2s - loss: 7.9768 - acc: 0.505 - ETA: 2s - loss: 7.8139 - acc: 0.515 - ETA: 2s - loss: 7.6480 - acc: 0.525 - ETA: 1s - loss: 7.7405 - acc: 0.519 - ETA: 1s - loss: 7.8351 - acc: 0.513 - ETA: 1s - loss: 7.8495 - acc: 0.513 - ETA: 1s - loss: 7.7875 - acc: 0.516 - ETA: 1s - loss: 7.8798 - acc: 0.511 - ETA: 1s - loss: 7.9285 - acc: 0.508 - ETA: 1s - loss: 7.9228 - acc: 0.508 - ETA: 1s - loss: 7.9033 - acc: 0.509 - ETA: 1s - loss: 7.9341 - acc: 0.507 - ETA: 1s - loss: 7.8498 - acc: 0.512 - ETA: 1s - loss: 7.8751 - acc: 0.511 - ETA: 1s - loss: 7.8858 - acc: 0.510 - ETA: 1s - loss: 7.8528 - acc: 0.512 - ETA: 1s - loss: 7.9009 - acc: 0.509 - ETA: 1s - loss: 7.9285 - acc: 0.507 - ETA: 1s - loss: 7.9959 - acc: 0.503 - ETA: 1s - loss: 8.0076 - acc: 0.502 - ETA: 1s - loss: 8.0316 - acc: 0.501 - ETA: 0s - loss: 8.0454 - acc: 0.500 - ETA: 0s - loss: 8.0395 - acc: 0.500 - ETA: 0s - loss: 8.0364 - acc: 0.500 - ETA: 0s - loss: 8.0601 - acc: 0.499 - ETA: 0s - loss: 8.0929 - acc: 0.497 - ETA: 0s - loss: 8.0953 - acc: 0.497 - ETA: 0s - loss: 8.0534 - acc: 0.499 - ETA: 0s - loss: 8.0953 - acc: 0.497 - ETA: 0s - loss: 8.0878 - acc: 0.497 - ETA: 0s - loss: 8.0981 - acc: 0.496 - ETA: 0s - loss: 8.1268 - acc: 0.495 - ETA: 0s - loss: 8.1391 - acc: 0.494 - ETA: 0s - loss: 8.1697 - acc: 0.492 - ETA: 0s - loss: 8.1609 - acc: 0.492 - ETA: 0s - loss: 8.1610 - acc: 0.492 - ETA: 0s - loss: 8.1661 - acc: 0.492 - ETA: 0s - loss: 8.1601 - acc: 0.492 - ETA: 0s - loss: 8.1826 - acc: 0.4913Epoch 00018: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s 359us/step - loss: 8.1732 - acc: 0.4919 - val_loss: 8.7978 - val_acc: 0.4060\n",
      "Epoch 19/20\n",
      "6540/6680 [============================>.] - ETA: 3s - loss: 11.2827 - acc: 0.30 - ETA: 2s - loss: 8.0593 - acc: 0.5000 - ETA: 2s - loss: 8.0593 - acc: 0.500 - ETA: 2s - loss: 8.1562 - acc: 0.494 - ETA: 2s - loss: 8.1374 - acc: 0.495 - ETA: 2s - loss: 7.9139 - acc: 0.507 - ETA: 2s - loss: 8.0618 - acc: 0.498 - ETA: 1s - loss: 8.2106 - acc: 0.489 - ETA: 1s - loss: 8.2330 - acc: 0.488 - ETA: 1s - loss: 8.2642 - acc: 0.486 - ETA: 1s - loss: 8.2638 - acc: 0.486 - ETA: 1s - loss: 8.0740 - acc: 0.498 - ETA: 1s - loss: 8.0994 - acc: 0.496 - ETA: 1s - loss: 8.1060 - acc: 0.496 - ETA: 1s - loss: 8.0952 - acc: 0.496 - ETA: 1s - loss: 8.0872 - acc: 0.497 - ETA: 1s - loss: 8.0410 - acc: 0.499 - ETA: 1s - loss: 8.0701 - acc: 0.497 - ETA: 1s - loss: 8.1267 - acc: 0.493 - ETA: 1s - loss: 8.1115 - acc: 0.493 - ETA: 1s - loss: 8.0984 - acc: 0.494 - ETA: 1s - loss: 8.1503 - acc: 0.491 - ETA: 1s - loss: 8.1417 - acc: 0.492 - ETA: 1s - loss: 8.1812 - acc: 0.489 - ETA: 0s - loss: 8.1723 - acc: 0.490 - ETA: 0s - loss: 8.2117 - acc: 0.487 - ETA: 0s - loss: 8.2337 - acc: 0.486 - ETA: 0s - loss: 8.2912 - acc: 0.482 - ETA: 0s - loss: 8.2731 - acc: 0.483 - ETA: 0s - loss: 8.2706 - acc: 0.483 - ETA: 0s - loss: 8.2697 - acc: 0.483 - ETA: 0s - loss: 8.2316 - acc: 0.485 - ETA: 0s - loss: 8.2612 - acc: 0.483 - ETA: 0s - loss: 8.2334 - acc: 0.485 - ETA: 0s - loss: 8.2105 - acc: 0.487 - ETA: 0s - loss: 8.1739 - acc: 0.489 - ETA: 0s - loss: 8.1708 - acc: 0.489 - ETA: 0s - loss: 8.1863 - acc: 0.488 - ETA: 0s - loss: 8.1797 - acc: 0.488 - ETA: 0s - loss: 8.1695 - acc: 0.488 - ETA: 0s - loss: 8.1944 - acc: 0.4870Epoch 00019: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s 342us/step - loss: 8.1977 - acc: 0.4867 - val_loss: 9.0424 - val_acc: 0.3796\n",
      "Epoch 20/20\n",
      "6640/6680 [============================>.] - ETA: 2s - loss: 4.8371 - acc: 0.700 - ETA: 2s - loss: 7.6315 - acc: 0.516 - ETA: 1s - loss: 7.6628 - acc: 0.513 - ETA: 1s - loss: 7.6034 - acc: 0.513 - ETA: 1s - loss: 7.7900 - acc: 0.500 - ETA: 1s - loss: 7.8540 - acc: 0.496 - ETA: 1s - loss: 7.7848 - acc: 0.502 - ETA: 1s - loss: 7.8539 - acc: 0.496 - ETA: 1s - loss: 7.8401 - acc: 0.497 - ETA: 1s - loss: 7.8972 - acc: 0.495 - ETA: 1s - loss: 7.9147 - acc: 0.493 - ETA: 1s - loss: 8.0012 - acc: 0.489 - ETA: 1s - loss: 8.0508 - acc: 0.486 - ETA: 1s - loss: 8.0786 - acc: 0.484 - ETA: 1s - loss: 8.0785 - acc: 0.485 - ETA: 1s - loss: 8.0828 - acc: 0.484 - ETA: 1s - loss: 8.0632 - acc: 0.484 - ETA: 1s - loss: 8.0943 - acc: 0.482 - ETA: 1s - loss: 8.0942 - acc: 0.481 - ETA: 1s - loss: 8.1053 - acc: 0.481 - ETA: 1s - loss: 8.0925 - acc: 0.482 - ETA: 0s - loss: 8.0539 - acc: 0.484 - ETA: 0s - loss: 8.1056 - acc: 0.481 - ETA: 0s - loss: 8.0954 - acc: 0.482 - ETA: 0s - loss: 8.1139 - acc: 0.481 - ETA: 0s - loss: 8.1285 - acc: 0.480 - ETA: 0s - loss: 8.1713 - acc: 0.478 - ETA: 0s - loss: 8.1642 - acc: 0.479 - ETA: 0s - loss: 8.1528 - acc: 0.479 - ETA: 0s - loss: 8.1957 - acc: 0.477 - ETA: 0s - loss: 8.2114 - acc: 0.476 - ETA: 0s - loss: 8.2377 - acc: 0.474 - ETA: 0s - loss: 8.2481 - acc: 0.474 - ETA: 0s - loss: 8.2654 - acc: 0.473 - ETA: 0s - loss: 8.2486 - acc: 0.474 - ETA: 0s - loss: 8.2727 - acc: 0.473 - ETA: 0s - loss: 8.2548 - acc: 0.474 - ETA: 0s - loss: 8.2340 - acc: 0.475 - ETA: 0s - loss: 8.2294 - acc: 0.476 - ETA: 0s - loss: 8.2290 - acc: 0.4762Epoch 00020: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s 331us/step - loss: 8.2401 - acc: 0.4756 - val_loss: 9.0161 - val_acc: 0.3832\n",
      "Test accuracy: 41.0287%\n"
     ]
    }
   ],
   "source": [
    "# try out VGG19\n",
    "\n",
    "test_results['VGG19'] = tryout_bottleneck('VGG19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6640/6680 [============================>.] - ETA: 3:08 - loss: 5.3298 - acc: 0.0000e+0 - ETA: 28s - loss: 5.9407 - acc: 0.0071    - ETA: 17s - loss: 5.9389 - acc: 0.02 - ETA: 12s - loss: 5.8116 - acc: 0.04 - ETA: 9s - loss: 5.5243 - acc: 0.0667 - ETA: 8s - loss: 5.1833 - acc: 0.091 - ETA: 7s - loss: 4.8898 - acc: 0.116 - ETA: 6s - loss: 4.5848 - acc: 0.159 - ETA: 5s - loss: 4.3553 - acc: 0.184 - ETA: 5s - loss: 4.1345 - acc: 0.214 - ETA: 4s - loss: 3.9482 - acc: 0.234 - ETA: 4s - loss: 3.7721 - acc: 0.260 - ETA: 4s - loss: 3.6239 - acc: 0.285 - ETA: 4s - loss: 3.4947 - acc: 0.303 - ETA: 3s - loss: 3.3765 - acc: 0.325 - ETA: 3s - loss: 3.2507 - acc: 0.339 - ETA: 3s - loss: 3.1598 - acc: 0.350 - ETA: 3s - loss: 3.0767 - acc: 0.360 - ETA: 3s - loss: 2.9844 - acc: 0.376 - ETA: 2s - loss: 2.9033 - acc: 0.390 - ETA: 2s - loss: 2.8418 - acc: 0.400 - ETA: 2s - loss: 2.7694 - acc: 0.412 - ETA: 2s - loss: 2.7115 - acc: 0.422 - ETA: 2s - loss: 2.6411 - acc: 0.433 - ETA: 2s - loss: 2.5944 - acc: 0.440 - ETA: 2s - loss: 2.5409 - acc: 0.449 - ETA: 2s - loss: 2.4785 - acc: 0.460 - ETA: 2s - loss: 2.4275 - acc: 0.469 - ETA: 2s - loss: 2.3807 - acc: 0.477 - ETA: 1s - loss: 2.3406 - acc: 0.484 - ETA: 1s - loss: 2.2889 - acc: 0.492 - ETA: 1s - loss: 2.2517 - acc: 0.500 - ETA: 1s - loss: 2.2175 - acc: 0.506 - ETA: 1s - loss: 2.1825 - acc: 0.512 - ETA: 1s - loss: 2.1441 - acc: 0.519 - ETA: 1s - loss: 2.1129 - acc: 0.524 - ETA: 1s - loss: 2.0819 - acc: 0.529 - ETA: 1s - loss: 2.0459 - acc: 0.537 - ETA: 1s - loss: 2.0201 - acc: 0.542 - ETA: 1s - loss: 1.9975 - acc: 0.546 - ETA: 1s - loss: 1.9770 - acc: 0.550 - ETA: 0s - loss: 1.9502 - acc: 0.555 - ETA: 0s - loss: 1.9255 - acc: 0.560 - ETA: 0s - loss: 1.9034 - acc: 0.564 - ETA: 0s - loss: 1.8846 - acc: 0.567 - ETA: 0s - loss: 1.8601 - acc: 0.571 - ETA: 0s - loss: 1.8377 - acc: 0.575 - ETA: 0s - loss: 1.8173 - acc: 0.579 - ETA: 0s - loss: 1.8028 - acc: 0.581 - ETA: 0s - loss: 1.7926 - acc: 0.582 - ETA: 0s - loss: 1.7770 - acc: 0.584 - ETA: 0s - loss: 1.7631 - acc: 0.587 - ETA: 0s - loss: 1.7417 - acc: 0.593 - ETA: 0s - loss: 1.7265 - acc: 0.595 - ETA: 0s - loss: 1.7118 - acc: 0.597 - ETA: 0s - loss: 1.6963 - acc: 0.6002Epoch 00001: val_loss improved from inf to 0.82201, saving model to saved_models/weights.best.Resnet50.hdf5\n",
      "6680/6680 [==============================] - 4s 590us/step - loss: 1.6922 - acc: 0.6007 - val_loss: 0.8220 - val_acc: 0.7653\n",
      "Epoch 2/20\n",
      "6620/6680 [============================>.] - ETA: 3s - loss: 0.3150 - acc: 0.900 - ETA: 2s - loss: 0.3054 - acc: 0.935 - ETA: 2s - loss: 0.3564 - acc: 0.911 - ETA: 2s - loss: 0.3704 - acc: 0.913 - ETA: 2s - loss: 0.3580 - acc: 0.912 - ETA: 2s - loss: 0.3594 - acc: 0.915 - ETA: 2s - loss: 0.3596 - acc: 0.916 - ETA: 2s - loss: 0.3678 - acc: 0.911 - ETA: 2s - loss: 0.3752 - acc: 0.908 - ETA: 2s - loss: 0.3812 - acc: 0.903 - ETA: 2s - loss: 0.3820 - acc: 0.904 - ETA: 2s - loss: 0.3734 - acc: 0.907 - ETA: 2s - loss: 0.3739 - acc: 0.906 - ETA: 2s - loss: 0.3713 - acc: 0.905 - ETA: 2s - loss: 0.3702 - acc: 0.904 - ETA: 2s - loss: 0.3668 - acc: 0.907 - ETA: 2s - loss: 0.3650 - acc: 0.905 - ETA: 2s - loss: 0.3642 - acc: 0.906 - ETA: 2s - loss: 0.3602 - acc: 0.908 - ETA: 1s - loss: 0.3598 - acc: 0.908 - ETA: 1s - loss: 0.3580 - acc: 0.909 - ETA: 1s - loss: 0.3587 - acc: 0.909 - ETA: 1s - loss: 0.3606 - acc: 0.908 - ETA: 1s - loss: 0.3558 - acc: 0.910 - ETA: 1s - loss: 0.3575 - acc: 0.910 - ETA: 1s - loss: 0.3560 - acc: 0.911 - ETA: 1s - loss: 0.3571 - acc: 0.910 - ETA: 1s - loss: 0.3553 - acc: 0.910 - ETA: 1s - loss: 0.3530 - acc: 0.910 - ETA: 1s - loss: 0.3517 - acc: 0.910 - ETA: 1s - loss: 0.3498 - acc: 0.911 - ETA: 1s - loss: 0.3506 - acc: 0.911 - ETA: 1s - loss: 0.3496 - acc: 0.910 - ETA: 1s - loss: 0.3495 - acc: 0.909 - ETA: 1s - loss: 0.3495 - acc: 0.909 - ETA: 1s - loss: 0.3482 - acc: 0.909 - ETA: 1s - loss: 0.3476 - acc: 0.909 - ETA: 1s - loss: 0.3455 - acc: 0.910 - ETA: 0s - loss: 0.3437 - acc: 0.910 - ETA: 0s - loss: 0.3424 - acc: 0.910 - ETA: 0s - loss: 0.3429 - acc: 0.910 - ETA: 0s - loss: 0.3428 - acc: 0.910 - ETA: 0s - loss: 0.3420 - acc: 0.910 - ETA: 0s - loss: 0.3422 - acc: 0.909 - ETA: 0s - loss: 0.3416 - acc: 0.909 - ETA: 0s - loss: 0.3452 - acc: 0.907 - ETA: 0s - loss: 0.3449 - acc: 0.907 - ETA: 0s - loss: 0.3471 - acc: 0.906 - ETA: 0s - loss: 0.3459 - acc: 0.907 - ETA: 0s - loss: 0.3455 - acc: 0.907 - ETA: 0s - loss: 0.3448 - acc: 0.907 - ETA: 0s - loss: 0.3431 - acc: 0.907 - ETA: 0s - loss: 0.3428 - acc: 0.907 - ETA: 0s - loss: 0.3419 - acc: 0.907 - ETA: 0s - loss: 0.3403 - acc: 0.908 - ETA: 0s - loss: 0.3402 - acc: 0.9083Epoch 00002: val_loss improved from 0.82201 to 0.70973, saving model to saved_models/weights.best.Resnet50.hdf5\n",
      "6680/6680 [==============================] - 3s 464us/step - loss: 0.3409 - acc: 0.9078 - val_loss: 0.7097 - val_acc: 0.7868\n",
      "Epoch 3/20\n",
      "6600/6680 [============================>.] - ETA: 3s - loss: 0.2386 - acc: 0.900 - ETA: 2s - loss: 0.1575 - acc: 0.950 - ETA: 2s - loss: 0.1572 - acc: 0.965 - ETA: 2s - loss: 0.1530 - acc: 0.965 - ETA: 2s - loss: 0.1518 - acc: 0.967 - ETA: 2s - loss: 0.1503 - acc: 0.968 - ETA: 2s - loss: 0.1586 - acc: 0.967 - ETA: 2s - loss: 0.1550 - acc: 0.970 - ETA: 2s - loss: 0.1572 - acc: 0.969 - ETA: 2s - loss: 0.1566 - acc: 0.971 - ETA: 2s - loss: 0.1543 - acc: 0.971 - ETA: 2s - loss: 0.1499 - acc: 0.973 - ETA: 2s - loss: 0.1503 - acc: 0.974 - ETA: 2s - loss: 0.1469 - acc: 0.975 - ETA: 2s - loss: 0.1478 - acc: 0.974 - ETA: 2s - loss: 0.1494 - acc: 0.973 - ETA: 2s - loss: 0.1490 - acc: 0.974 - ETA: 2s - loss: 0.1488 - acc: 0.974 - ETA: 1s - loss: 0.1518 - acc: 0.973 - ETA: 1s - loss: 0.1533 - acc: 0.972 - ETA: 1s - loss: 0.1507 - acc: 0.973 - ETA: 1s - loss: 0.1495 - acc: 0.973 - ETA: 1s - loss: 0.1483 - acc: 0.973 - ETA: 1s - loss: 0.1490 - acc: 0.972 - ETA: 1s - loss: 0.1507 - acc: 0.972 - ETA: 1s - loss: 0.1522 - acc: 0.971 - ETA: 1s - loss: 0.1527 - acc: 0.971 - ETA: 1s - loss: 0.1529 - acc: 0.971 - ETA: 1s - loss: 0.1537 - acc: 0.970 - ETA: 1s - loss: 0.1545 - acc: 0.970 - ETA: 1s - loss: 0.1540 - acc: 0.970 - ETA: 1s - loss: 0.1535 - acc: 0.970 - ETA: 1s - loss: 0.1538 - acc: 0.971 - ETA: 1s - loss: 0.1558 - acc: 0.970 - ETA: 1s - loss: 0.1557 - acc: 0.971 - ETA: 1s - loss: 0.1546 - acc: 0.972 - ETA: 0s - loss: 0.1547 - acc: 0.972 - ETA: 0s - loss: 0.1553 - acc: 0.971 - ETA: 0s - loss: 0.1562 - acc: 0.971 - ETA: 0s - loss: 0.1570 - acc: 0.971 - ETA: 0s - loss: 0.1566 - acc: 0.970 - ETA: 0s - loss: 0.1575 - acc: 0.971 - ETA: 0s - loss: 0.1573 - acc: 0.970 - ETA: 0s - loss: 0.1579 - acc: 0.970 - ETA: 0s - loss: 0.1619 - acc: 0.969 - ETA: 0s - loss: 0.1615 - acc: 0.969 - ETA: 0s - loss: 0.1619 - acc: 0.968 - ETA: 0s - loss: 0.1630 - acc: 0.967 - ETA: 0s - loss: 0.1634 - acc: 0.968 - ETA: 0s - loss: 0.1636 - acc: 0.967 - ETA: 0s - loss: 0.1637 - acc: 0.967 - ETA: 0s - loss: 0.1647 - acc: 0.967 - ETA: 0s - loss: 0.1651 - acc: 0.966 - ETA: 0s - loss: 0.1648 - acc: 0.966 - ETA: 0s - loss: 0.1652 - acc: 0.9664Epoch 00003: val_loss improved from 0.70973 to 0.59922, saving model to saved_models/weights.best.Resnet50.hdf5\n",
      "6680/6680 [==============================] - 3s 460us/step - loss: 0.1650 - acc: 0.9662 - val_loss: 0.5992 - val_acc: 0.8168\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 3s - loss: 0.0810 - acc: 1.000 - ETA: 2s - loss: 0.0684 - acc: 1.000 - ETA: 2s - loss: 0.0738 - acc: 0.992 - ETA: 2s - loss: 0.0932 - acc: 0.984 - ETA: 2s - loss: 0.0914 - acc: 0.986 - ETA: 2s - loss: 0.0892 - acc: 0.987 - ETA: 2s - loss: 0.0882 - acc: 0.989 - ETA: 2s - loss: 0.0912 - acc: 0.988 - ETA: 2s - loss: 0.0888 - acc: 0.988 - ETA: 2s - loss: 0.0873 - acc: 0.989 - ETA: 2s - loss: 0.0864 - acc: 0.989 - ETA: 2s - loss: 0.0842 - acc: 0.990 - ETA: 2s - loss: 0.0877 - acc: 0.989 - ETA: 2s - loss: 0.0878 - acc: 0.989 - ETA: 2s - loss: 0.0874 - acc: 0.989 - ETA: 2s - loss: 0.0887 - acc: 0.989 - ETA: 2s - loss: 0.0874 - acc: 0.989 - ETA: 2s - loss: 0.0871 - acc: 0.989 - ETA: 2s - loss: 0.0883 - acc: 0.989 - ETA: 1s - loss: 0.0884 - acc: 0.989 - ETA: 1s - loss: 0.0900 - acc: 0.989 - ETA: 1s - loss: 0.0903 - acc: 0.989 - ETA: 1s - loss: 0.0897 - acc: 0.989 - ETA: 1s - loss: 0.0892 - acc: 0.990 - ETA: 1s - loss: 0.0883 - acc: 0.990 - ETA: 1s - loss: 0.0873 - acc: 0.990 - ETA: 1s - loss: 0.0871 - acc: 0.990 - ETA: 1s - loss: 0.0871 - acc: 0.989 - ETA: 1s - loss: 0.0875 - acc: 0.989 - ETA: 1s - loss: 0.0872 - acc: 0.989 - ETA: 1s - loss: 0.0867 - acc: 0.990 - ETA: 1s - loss: 0.0857 - acc: 0.990 - ETA: 1s - loss: 0.0858 - acc: 0.990 - ETA: 1s - loss: 0.0856 - acc: 0.990 - ETA: 1s - loss: 0.0856 - acc: 0.990 - ETA: 1s - loss: 0.0853 - acc: 0.990 - ETA: 1s - loss: 0.0847 - acc: 0.990 - ETA: 0s - loss: 0.0874 - acc: 0.989 - ETA: 0s - loss: 0.0871 - acc: 0.989 - ETA: 0s - loss: 0.0871 - acc: 0.989 - ETA: 0s - loss: 0.0867 - acc: 0.989 - ETA: 0s - loss: 0.0866 - acc: 0.989 - ETA: 0s - loss: 0.0870 - acc: 0.989 - ETA: 0s - loss: 0.0873 - acc: 0.989 - ETA: 0s - loss: 0.0869 - acc: 0.989 - ETA: 0s - loss: 0.0870 - acc: 0.989 - ETA: 0s - loss: 0.0871 - acc: 0.990 - ETA: 0s - loss: 0.0867 - acc: 0.990 - ETA: 0s - loss: 0.0871 - acc: 0.990 - ETA: 0s - loss: 0.0869 - acc: 0.989 - ETA: 0s - loss: 0.0870 - acc: 0.989 - ETA: 0s - loss: 0.0876 - acc: 0.989 - ETA: 0s - loss: 0.0878 - acc: 0.990 - ETA: 0s - loss: 0.0875 - acc: 0.990 - ETA: 0s - loss: 0.0886 - acc: 0.989 - ETA: 0s - loss: 0.0888 - acc: 0.9896Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 461us/step - loss: 0.0890 - acc: 0.9895 - val_loss: 0.6099 - val_acc: 0.8168\n",
      "Epoch 5/20\n",
      "6600/6680 [============================>.] - ETA: 3s - loss: 0.0300 - acc: 1.000 - ETA: 2s - loss: 0.0567 - acc: 1.000 - ETA: 2s - loss: 0.0516 - acc: 1.000 - ETA: 2s - loss: 0.0504 - acc: 1.000 - ETA: 2s - loss: 0.0549 - acc: 0.998 - ETA: 2s - loss: 0.0527 - acc: 0.998 - ETA: 2s - loss: 0.0517 - acc: 0.998 - ETA: 2s - loss: 0.0520 - acc: 0.997 - ETA: 2s - loss: 0.0533 - acc: 0.997 - ETA: 2s - loss: 0.0531 - acc: 0.997 - ETA: 2s - loss: 0.0540 - acc: 0.996 - ETA: 2s - loss: 0.0541 - acc: 0.997 - ETA: 2s - loss: 0.0526 - acc: 0.997 - ETA: 2s - loss: 0.0515 - acc: 0.997 - ETA: 2s - loss: 0.0505 - acc: 0.997 - ETA: 2s - loss: 0.0505 - acc: 0.997 - ETA: 2s - loss: 0.0495 - acc: 0.998 - ETA: 1s - loss: 0.0493 - acc: 0.998 - ETA: 1s - loss: 0.0492 - acc: 0.998 - ETA: 1s - loss: 0.0488 - acc: 0.998 - ETA: 1s - loss: 0.0488 - acc: 0.998 - ETA: 1s - loss: 0.0483 - acc: 0.998 - ETA: 1s - loss: 0.0483 - acc: 0.998 - ETA: 1s - loss: 0.0482 - acc: 0.998 - ETA: 1s - loss: 0.0484 - acc: 0.998 - ETA: 1s - loss: 0.0478 - acc: 0.998 - ETA: 1s - loss: 0.0477 - acc: 0.998 - ETA: 1s - loss: 0.0477 - acc: 0.998 - ETA: 1s - loss: 0.0478 - acc: 0.998 - ETA: 1s - loss: 0.0483 - acc: 0.998 - ETA: 1s - loss: 0.0488 - acc: 0.998 - ETA: 1s - loss: 0.0486 - acc: 0.998 - ETA: 1s - loss: 0.0497 - acc: 0.998 - ETA: 1s - loss: 0.0500 - acc: 0.998 - ETA: 1s - loss: 0.0500 - acc: 0.997 - ETA: 1s - loss: 0.0499 - acc: 0.997 - ETA: 1s - loss: 0.0501 - acc: 0.998 - ETA: 0s - loss: 0.0501 - acc: 0.997 - ETA: 0s - loss: 0.0499 - acc: 0.997 - ETA: 0s - loss: 0.0494 - acc: 0.997 - ETA: 0s - loss: 0.0495 - acc: 0.997 - ETA: 0s - loss: 0.0507 - acc: 0.997 - ETA: 0s - loss: 0.0507 - acc: 0.997 - ETA: 0s - loss: 0.0508 - acc: 0.997 - ETA: 0s - loss: 0.0508 - acc: 0.997 - ETA: 0s - loss: 0.0507 - acc: 0.997 - ETA: 0s - loss: 0.0505 - acc: 0.997 - ETA: 0s - loss: 0.0503 - acc: 0.997 - ETA: 0s - loss: 0.0508 - acc: 0.997 - ETA: 0s - loss: 0.0507 - acc: 0.997 - ETA: 0s - loss: 0.0512 - acc: 0.997 - ETA: 0s - loss: 0.0517 - acc: 0.996 - ETA: 0s - loss: 0.0526 - acc: 0.996 - ETA: 0s - loss: 0.0527 - acc: 0.996 - ETA: 0s - loss: 0.0531 - acc: 0.9961Epoch 00005: val_loss improved from 0.59922 to 0.56498, saving model to saved_models/weights.best.Resnet50.hdf5\n",
      "6680/6680 [==============================] - 3s 457us/step - loss: 0.0530 - acc: 0.9961 - val_loss: 0.5650 - val_acc: 0.8323\n",
      "Epoch 6/20\n",
      "6660/6680 [============================>.] - ETA: 3s - loss: 0.0193 - acc: 1.000 - ETA: 2s - loss: 0.0312 - acc: 0.992 - ETA: 2s - loss: 0.0397 - acc: 0.992 - ETA: 2s - loss: 0.0398 - acc: 0.995 - ETA: 2s - loss: 0.0400 - acc: 0.994 - ETA: 2s - loss: 0.0455 - acc: 0.993 - ETA: 2s - loss: 0.0427 - acc: 0.994 - ETA: 2s - loss: 0.0417 - acc: 0.995 - ETA: 2s - loss: 0.0407 - acc: 0.996 - ETA: 2s - loss: 0.0399 - acc: 0.996 - ETA: 2s - loss: 0.0390 - acc: 0.995 - ETA: 2s - loss: 0.0383 - acc: 0.996 - ETA: 2s - loss: 0.0377 - acc: 0.996 - ETA: 2s - loss: 0.0387 - acc: 0.995 - ETA: 2s - loss: 0.0388 - acc: 0.995 - ETA: 2s - loss: 0.0382 - acc: 0.996 - ETA: 2s - loss: 0.0374 - acc: 0.996 - ETA: 1s - loss: 0.0375 - acc: 0.996 - ETA: 1s - loss: 0.0368 - acc: 0.996 - ETA: 1s - loss: 0.0370 - acc: 0.996 - ETA: 1s - loss: 0.0367 - acc: 0.996 - ETA: 1s - loss: 0.0376 - acc: 0.996 - ETA: 1s - loss: 0.0377 - acc: 0.996 - ETA: 1s - loss: 0.0376 - acc: 0.996 - ETA: 1s - loss: 0.0372 - acc: 0.996 - ETA: 1s - loss: 0.0370 - acc: 0.997 - ETA: 1s - loss: 0.0366 - acc: 0.997 - ETA: 1s - loss: 0.0369 - acc: 0.997 - ETA: 1s - loss: 0.0365 - acc: 0.997 - ETA: 1s - loss: 0.0362 - acc: 0.997 - ETA: 1s - loss: 0.0361 - acc: 0.997 - ETA: 1s - loss: 0.0362 - acc: 0.997 - ETA: 1s - loss: 0.0361 - acc: 0.997 - ETA: 1s - loss: 0.0361 - acc: 0.997 - ETA: 1s - loss: 0.0359 - acc: 0.997 - ETA: 1s - loss: 0.0360 - acc: 0.997 - ETA: 0s - loss: 0.0359 - acc: 0.997 - ETA: 0s - loss: 0.0359 - acc: 0.997 - ETA: 0s - loss: 0.0361 - acc: 0.997 - ETA: 0s - loss: 0.0364 - acc: 0.997 - ETA: 0s - loss: 0.0372 - acc: 0.997 - ETA: 0s - loss: 0.0384 - acc: 0.996 - ETA: 0s - loss: 0.0391 - acc: 0.996 - ETA: 0s - loss: 0.0397 - acc: 0.996 - ETA: 0s - loss: 0.0400 - acc: 0.996 - ETA: 0s - loss: 0.0400 - acc: 0.996 - ETA: 0s - loss: 0.0397 - acc: 0.996 - ETA: 0s - loss: 0.0398 - acc: 0.996 - ETA: 0s - loss: 0.0397 - acc: 0.996 - ETA: 0s - loss: 0.0395 - acc: 0.996 - ETA: 0s - loss: 0.0401 - acc: 0.996 - ETA: 0s - loss: 0.0399 - acc: 0.996 - ETA: 0s - loss: 0.0401 - acc: 0.996 - ETA: 0s - loss: 0.0400 - acc: 0.996 - ETA: 0s - loss: 0.0400 - acc: 0.9962Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 450us/step - loss: 0.0400 - acc: 0.9963 - val_loss: 0.6003 - val_acc: 0.8431\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 3s - loss: 0.0190 - acc: 1.000 - ETA: 2s - loss: 0.0313 - acc: 1.000 - ETA: 2s - loss: 0.0283 - acc: 1.000 - ETA: 2s - loss: 0.0248 - acc: 1.000 - ETA: 2s - loss: 0.0244 - acc: 1.000 - ETA: 2s - loss: 0.0339 - acc: 0.998 - ETA: 2s - loss: 0.0324 - acc: 0.998 - ETA: 2s - loss: 0.0353 - acc: 0.996 - ETA: 2s - loss: 0.0338 - acc: 0.997 - ETA: 2s - loss: 0.0330 - acc: 0.997 - ETA: 2s - loss: 0.0340 - acc: 0.996 - ETA: 2s - loss: 0.0336 - acc: 0.997 - ETA: 2s - loss: 0.0328 - acc: 0.997 - ETA: 2s - loss: 0.0336 - acc: 0.996 - ETA: 2s - loss: 0.0329 - acc: 0.997 - ETA: 2s - loss: 0.0321 - acc: 0.997 - ETA: 2s - loss: 0.0314 - acc: 0.997 - ETA: 2s - loss: 0.0314 - acc: 0.997 - ETA: 1s - loss: 0.0314 - acc: 0.996 - ETA: 1s - loss: 0.0308 - acc: 0.997 - ETA: 1s - loss: 0.0307 - acc: 0.997 - ETA: 1s - loss: 0.0313 - acc: 0.996 - ETA: 1s - loss: 0.0307 - acc: 0.997 - ETA: 1s - loss: 0.0307 - acc: 0.996 - ETA: 1s - loss: 0.0304 - acc: 0.996 - ETA: 1s - loss: 0.0300 - acc: 0.997 - ETA: 1s - loss: 0.0298 - acc: 0.997 - ETA: 1s - loss: 0.0297 - acc: 0.997 - ETA: 1s - loss: 0.0293 - acc: 0.997 - ETA: 1s - loss: 0.0292 - acc: 0.997 - ETA: 1s - loss: 0.0291 - acc: 0.997 - ETA: 1s - loss: 0.0290 - acc: 0.997 - ETA: 1s - loss: 0.0288 - acc: 0.997 - ETA: 1s - loss: 0.0287 - acc: 0.997 - ETA: 1s - loss: 0.0284 - acc: 0.997 - ETA: 1s - loss: 0.0283 - acc: 0.997 - ETA: 0s - loss: 0.0295 - acc: 0.997 - ETA: 0s - loss: 0.0294 - acc: 0.997 - ETA: 0s - loss: 0.0297 - acc: 0.997 - ETA: 0s - loss: 0.0295 - acc: 0.997 - ETA: 0s - loss: 0.0297 - acc: 0.997 - ETA: 0s - loss: 0.0296 - acc: 0.997 - ETA: 0s - loss: 0.0294 - acc: 0.997 - ETA: 0s - loss: 0.0303 - acc: 0.997 - ETA: 0s - loss: 0.0302 - acc: 0.997 - ETA: 0s - loss: 0.0302 - acc: 0.997 - ETA: 0s - loss: 0.0301 - acc: 0.997 - ETA: 0s - loss: 0.0308 - acc: 0.997 - ETA: 0s - loss: 0.0309 - acc: 0.997 - ETA: 0s - loss: 0.0308 - acc: 0.997 - ETA: 0s - loss: 0.0311 - acc: 0.997 - ETA: 0s - loss: 0.0309 - acc: 0.997 - ETA: 0s - loss: 0.0310 - acc: 0.997 - ETA: 0s - loss: 0.0311 - acc: 0.997 - ETA: 0s - loss: 0.0309 - acc: 0.997 - ETA: 0s - loss: 0.0308 - acc: 0.9971Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 455us/step - loss: 0.0308 - acc: 0.9972 - val_loss: 0.5672 - val_acc: 0.8335\n",
      "Epoch 8/20\n",
      "6620/6680 [============================>.] - ETA: 3s - loss: 0.0194 - acc: 1.000 - ETA: 2s - loss: 0.0178 - acc: 1.000 - ETA: 2s - loss: 0.0172 - acc: 1.000 - ETA: 2s - loss: 0.0193 - acc: 1.000 - ETA: 2s - loss: 0.0187 - acc: 1.000 - ETA: 2s - loss: 0.0185 - acc: 1.000 - ETA: 2s - loss: 0.0183 - acc: 1.000 - ETA: 2s - loss: 0.0180 - acc: 1.000 - ETA: 2s - loss: 0.0174 - acc: 1.000 - ETA: 2s - loss: 0.0171 - acc: 1.000 - ETA: 2s - loss: 0.0173 - acc: 1.000 - ETA: 2s - loss: 0.0175 - acc: 1.000 - ETA: 2s - loss: 0.0171 - acc: 1.000 - ETA: 2s - loss: 0.0217 - acc: 0.999 - ETA: 2s - loss: 0.0224 - acc: 0.998 - ETA: 2s - loss: 0.0230 - acc: 0.998 - ETA: 2s - loss: 0.0242 - acc: 0.998 - ETA: 1s - loss: 0.0242 - acc: 0.998 - ETA: 1s - loss: 0.0242 - acc: 0.998 - ETA: 1s - loss: 0.0240 - acc: 0.998 - ETA: 1s - loss: 0.0236 - acc: 0.998 - ETA: 1s - loss: 0.0235 - acc: 0.998 - ETA: 1s - loss: 0.0235 - acc: 0.998 - ETA: 1s - loss: 0.0233 - acc: 0.997 - ETA: 1s - loss: 0.0231 - acc: 0.998 - ETA: 1s - loss: 0.0229 - acc: 0.998 - ETA: 1s - loss: 0.0242 - acc: 0.997 - ETA: 1s - loss: 0.0239 - acc: 0.997 - ETA: 1s - loss: 0.0247 - acc: 0.997 - ETA: 1s - loss: 0.0243 - acc: 0.997 - ETA: 1s - loss: 0.0241 - acc: 0.997 - ETA: 1s - loss: 0.0243 - acc: 0.997 - ETA: 1s - loss: 0.0241 - acc: 0.997 - ETA: 1s - loss: 0.0238 - acc: 0.997 - ETA: 1s - loss: 0.0236 - acc: 0.997 - ETA: 1s - loss: 0.0235 - acc: 0.997 - ETA: 0s - loss: 0.0233 - acc: 0.997 - ETA: 0s - loss: 0.0232 - acc: 0.997 - ETA: 0s - loss: 0.0235 - acc: 0.997 - ETA: 0s - loss: 0.0235 - acc: 0.997 - ETA: 0s - loss: 0.0233 - acc: 0.997 - ETA: 0s - loss: 0.0234 - acc: 0.997 - ETA: 0s - loss: 0.0231 - acc: 0.997 - ETA: 0s - loss: 0.0243 - acc: 0.997 - ETA: 0s - loss: 0.0241 - acc: 0.997 - ETA: 0s - loss: 0.0241 - acc: 0.997 - ETA: 0s - loss: 0.0240 - acc: 0.997 - ETA: 0s - loss: 0.0239 - acc: 0.997 - ETA: 0s - loss: 0.0238 - acc: 0.997 - ETA: 0s - loss: 0.0236 - acc: 0.997 - ETA: 0s - loss: 0.0246 - acc: 0.997 - ETA: 0s - loss: 0.0247 - acc: 0.997 - ETA: 0s - loss: 0.0252 - acc: 0.997 - ETA: 0s - loss: 0.0251 - acc: 0.997 - ETA: 0s - loss: 0.0252 - acc: 0.9973Epoch 00008: val_loss improved from 0.56498 to 0.54497, saving model to saved_models/weights.best.Resnet50.hdf5\n",
      "6680/6680 [==============================] - 3s 450us/step - loss: 0.0251 - acc: 0.9973 - val_loss: 0.5450 - val_acc: 0.8359\n",
      "Epoch 9/20\n",
      "6560/6680 [============================>.] - ETA: 3s - loss: 0.0154 - acc: 1.000 - ETA: 2s - loss: 0.0145 - acc: 1.000 - ETA: 2s - loss: 0.0164 - acc: 1.000 - ETA: 2s - loss: 0.0153 - acc: 1.000 - ETA: 2s - loss: 0.0158 - acc: 0.998 - ETA: 2s - loss: 0.0157 - acc: 0.998 - ETA: 2s - loss: 0.0181 - acc: 0.997 - ETA: 2s - loss: 0.0171 - acc: 0.997 - ETA: 2s - loss: 0.0205 - acc: 0.997 - ETA: 2s - loss: 0.0199 - acc: 0.997 - ETA: 2s - loss: 0.0200 - acc: 0.996 - ETA: 2s - loss: 0.0204 - acc: 0.996 - ETA: 2s - loss: 0.0198 - acc: 0.996 - ETA: 2s - loss: 0.0192 - acc: 0.996 - ETA: 2s - loss: 0.0189 - acc: 0.997 - ETA: 2s - loss: 0.0184 - acc: 0.997 - ETA: 2s - loss: 0.0181 - acc: 0.997 - ETA: 1s - loss: 0.0177 - acc: 0.997 - ETA: 1s - loss: 0.0175 - acc: 0.997 - ETA: 1s - loss: 0.0183 - acc: 0.997 - ETA: 1s - loss: 0.0180 - acc: 0.997 - ETA: 1s - loss: 0.0178 - acc: 0.997 - ETA: 1s - loss: 0.0177 - acc: 0.997 - ETA: 1s - loss: 0.0173 - acc: 0.997 - ETA: 1s - loss: 0.0172 - acc: 0.998 - ETA: 1s - loss: 0.0169 - acc: 0.998 - ETA: 1s - loss: 0.0167 - acc: 0.998 - ETA: 1s - loss: 0.0166 - acc: 0.998 - ETA: 1s - loss: 0.0166 - acc: 0.998 - ETA: 1s - loss: 0.0164 - acc: 0.998 - ETA: 1s - loss: 0.0164 - acc: 0.998 - ETA: 1s - loss: 0.0176 - acc: 0.998 - ETA: 1s - loss: 0.0175 - acc: 0.998 - ETA: 1s - loss: 0.0181 - acc: 0.997 - ETA: 1s - loss: 0.0181 - acc: 0.997 - ETA: 0s - loss: 0.0180 - acc: 0.997 - ETA: 0s - loss: 0.0179 - acc: 0.998 - ETA: 0s - loss: 0.0179 - acc: 0.998 - ETA: 0s - loss: 0.0179 - acc: 0.998 - ETA: 0s - loss: 0.0178 - acc: 0.998 - ETA: 0s - loss: 0.0178 - acc: 0.998 - ETA: 0s - loss: 0.0184 - acc: 0.998 - ETA: 0s - loss: 0.0185 - acc: 0.997 - ETA: 0s - loss: 0.0187 - acc: 0.997 - ETA: 0s - loss: 0.0189 - acc: 0.997 - ETA: 0s - loss: 0.0202 - acc: 0.997 - ETA: 0s - loss: 0.0209 - acc: 0.997 - ETA: 0s - loss: 0.0221 - acc: 0.996 - ETA: 0s - loss: 0.0229 - acc: 0.996 - ETA: 0s - loss: 0.0229 - acc: 0.996 - ETA: 0s - loss: 0.0237 - acc: 0.996 - ETA: 0s - loss: 0.0235 - acc: 0.996 - ETA: 0s - loss: 0.0233 - acc: 0.996 - ETA: 0s - loss: 0.0232 - acc: 0.9963Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 446us/step - loss: 0.0231 - acc: 0.9964 - val_loss: 0.5680 - val_acc: 0.8335\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 3s - loss: 0.0127 - acc: 1.000 - ETA: 2s - loss: 0.0183 - acc: 1.000 - ETA: 2s - loss: 0.0145 - acc: 1.000 - ETA: 2s - loss: 0.0208 - acc: 0.997 - ETA: 2s - loss: 0.0203 - acc: 0.996 - ETA: 2s - loss: 0.0188 - acc: 0.997 - ETA: 2s - loss: 0.0211 - acc: 0.996 - ETA: 2s - loss: 0.0195 - acc: 0.996 - ETA: 2s - loss: 0.0183 - acc: 0.997 - ETA: 2s - loss: 0.0180 - acc: 0.996 - ETA: 2s - loss: 0.0179 - acc: 0.996 - ETA: 2s - loss: 0.0173 - acc: 0.997 - ETA: 2s - loss: 0.0168 - acc: 0.997 - ETA: 2s - loss: 0.0164 - acc: 0.997 - ETA: 2s - loss: 0.0159 - acc: 0.997 - ETA: 2s - loss: 0.0157 - acc: 0.997 - ETA: 1s - loss: 0.0153 - acc: 0.998 - ETA: 1s - loss: 0.0152 - acc: 0.998 - ETA: 1s - loss: 0.0150 - acc: 0.998 - ETA: 1s - loss: 0.0147 - acc: 0.998 - ETA: 1s - loss: 0.0145 - acc: 0.998 - ETA: 1s - loss: 0.0143 - acc: 0.998 - ETA: 1s - loss: 0.0142 - acc: 0.998 - ETA: 1s - loss: 0.0140 - acc: 0.998 - ETA: 1s - loss: 0.0140 - acc: 0.998 - ETA: 1s - loss: 0.0138 - acc: 0.998 - ETA: 1s - loss: 0.0151 - acc: 0.998 - ETA: 1s - loss: 0.0149 - acc: 0.998 - ETA: 1s - loss: 0.0155 - acc: 0.998 - ETA: 1s - loss: 0.0155 - acc: 0.998 - ETA: 1s - loss: 0.0154 - acc: 0.998 - ETA: 1s - loss: 0.0152 - acc: 0.998 - ETA: 1s - loss: 0.0151 - acc: 0.998 - ETA: 1s - loss: 0.0150 - acc: 0.998 - ETA: 0s - loss: 0.0150 - acc: 0.998 - ETA: 0s - loss: 0.0156 - acc: 0.998 - ETA: 0s - loss: 0.0159 - acc: 0.998 - ETA: 0s - loss: 0.0157 - acc: 0.998 - ETA: 0s - loss: 0.0163 - acc: 0.997 - ETA: 0s - loss: 0.0162 - acc: 0.998 - ETA: 0s - loss: 0.0176 - acc: 0.997 - ETA: 0s - loss: 0.0179 - acc: 0.997 - ETA: 0s - loss: 0.0177 - acc: 0.997 - ETA: 0s - loss: 0.0178 - acc: 0.997 - ETA: 0s - loss: 0.0177 - acc: 0.997 - ETA: 0s - loss: 0.0183 - acc: 0.997 - ETA: 0s - loss: 0.0185 - acc: 0.997 - ETA: 0s - loss: 0.0185 - acc: 0.997 - ETA: 0s - loss: 0.0196 - acc: 0.997 - ETA: 0s - loss: 0.0200 - acc: 0.997 - ETA: 0s - loss: 0.0198 - acc: 0.997 - ETA: 0s - loss: 0.0197 - acc: 0.997 - ETA: 0s - loss: 0.0197 - acc: 0.997 - ETA: 0s - loss: 0.0196 - acc: 0.9973Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 444us/step - loss: 0.0195 - acc: 0.9973 - val_loss: 0.5692 - val_acc: 0.8419\n",
      "Epoch 11/20\n",
      "6580/6680 [============================>.] - ETA: 3s - loss: 0.0096 - acc: 1.000 - ETA: 3s - loss: 0.0088 - acc: 1.000 - ETA: 2s - loss: 0.0100 - acc: 1.000 - ETA: 2s - loss: 0.0098 - acc: 1.000 - ETA: 2s - loss: 0.0094 - acc: 1.000 - ETA: 2s - loss: 0.0097 - acc: 1.000 - ETA: 2s - loss: 0.0094 - acc: 1.000 - ETA: 2s - loss: 0.0091 - acc: 1.000 - ETA: 2s - loss: 0.0093 - acc: 1.000 - ETA: 2s - loss: 0.0090 - acc: 1.000 - ETA: 2s - loss: 0.0090 - acc: 1.000 - ETA: 2s - loss: 0.0120 - acc: 0.999 - ETA: 2s - loss: 0.0116 - acc: 0.999 - ETA: 2s - loss: 0.0121 - acc: 0.998 - ETA: 2s - loss: 0.0125 - acc: 0.998 - ETA: 2s - loss: 0.0122 - acc: 0.998 - ETA: 2s - loss: 0.0119 - acc: 0.998 - ETA: 2s - loss: 0.0118 - acc: 0.998 - ETA: 2s - loss: 0.0117 - acc: 0.998 - ETA: 1s - loss: 0.0117 - acc: 0.998 - ETA: 1s - loss: 0.0116 - acc: 0.998 - ETA: 1s - loss: 0.0114 - acc: 0.998 - ETA: 1s - loss: 0.0113 - acc: 0.998 - ETA: 1s - loss: 0.0112 - acc: 0.998 - ETA: 1s - loss: 0.0110 - acc: 0.999 - ETA: 1s - loss: 0.0109 - acc: 0.999 - ETA: 1s - loss: 0.0108 - acc: 0.999 - ETA: 1s - loss: 0.0106 - acc: 0.999 - ETA: 1s - loss: 0.0106 - acc: 0.999 - ETA: 1s - loss: 0.0105 - acc: 0.999 - ETA: 1s - loss: 0.0105 - acc: 0.999 - ETA: 1s - loss: 0.0105 - acc: 0.999 - ETA: 1s - loss: 0.0106 - acc: 0.999 - ETA: 1s - loss: 0.0105 - acc: 0.999 - ETA: 1s - loss: 0.0104 - acc: 0.999 - ETA: 1s - loss: 0.0104 - acc: 0.999 - ETA: 1s - loss: 0.0104 - acc: 0.999 - ETA: 0s - loss: 0.0104 - acc: 0.999 - ETA: 0s - loss: 0.0103 - acc: 0.999 - ETA: 0s - loss: 0.0102 - acc: 0.999 - ETA: 0s - loss: 0.0102 - acc: 0.999 - ETA: 0s - loss: 0.0102 - acc: 0.999 - ETA: 0s - loss: 0.0107 - acc: 0.999 - ETA: 0s - loss: 0.0126 - acc: 0.998 - ETA: 0s - loss: 0.0125 - acc: 0.998 - ETA: 0s - loss: 0.0128 - acc: 0.998 - ETA: 0s - loss: 0.0128 - acc: 0.998 - ETA: 0s - loss: 0.0128 - acc: 0.998 - ETA: 0s - loss: 0.0135 - acc: 0.998 - ETA: 0s - loss: 0.0136 - acc: 0.998 - ETA: 0s - loss: 0.0143 - acc: 0.998 - ETA: 0s - loss: 0.0145 - acc: 0.998 - ETA: 0s - loss: 0.0150 - acc: 0.998 - ETA: 0s - loss: 0.0150 - acc: 0.998 - ETA: 0s - loss: 0.0149 - acc: 0.9983Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 451us/step - loss: 0.0152 - acc: 0.9982 - val_loss: 0.5919 - val_acc: 0.8455\n",
      "Epoch 12/20\n",
      "6620/6680 [============================>.] - ETA: 4s - loss: 0.0051 - acc: 1.000 - ETA: 2s - loss: 0.0085 - acc: 1.000 - ETA: 2s - loss: 0.0124 - acc: 1.000 - ETA: 2s - loss: 0.0264 - acc: 0.995 - ETA: 2s - loss: 0.0225 - acc: 0.996 - ETA: 2s - loss: 0.0197 - acc: 0.997 - ETA: 2s - loss: 0.0181 - acc: 0.997 - ETA: 2s - loss: 0.0179 - acc: 0.996 - ETA: 2s - loss: 0.0169 - acc: 0.997 - ETA: 2s - loss: 0.0161 - acc: 0.997 - ETA: 2s - loss: 0.0168 - acc: 0.996 - ETA: 2s - loss: 0.0160 - acc: 0.997 - ETA: 2s - loss: 0.0172 - acc: 0.996 - ETA: 2s - loss: 0.0167 - acc: 0.997 - ETA: 2s - loss: 0.0166 - acc: 0.997 - ETA: 2s - loss: 0.0178 - acc: 0.996 - ETA: 1s - loss: 0.0173 - acc: 0.997 - ETA: 1s - loss: 0.0173 - acc: 0.997 - ETA: 1s - loss: 0.0171 - acc: 0.997 - ETA: 1s - loss: 0.0169 - acc: 0.997 - ETA: 1s - loss: 0.0165 - acc: 0.997 - ETA: 1s - loss: 0.0162 - acc: 0.997 - ETA: 1s - loss: 0.0169 - acc: 0.997 - ETA: 1s - loss: 0.0167 - acc: 0.997 - ETA: 1s - loss: 0.0167 - acc: 0.997 - ETA: 1s - loss: 0.0172 - acc: 0.997 - ETA: 1s - loss: 0.0169 - acc: 0.997 - ETA: 1s - loss: 0.0173 - acc: 0.997 - ETA: 1s - loss: 0.0170 - acc: 0.997 - ETA: 1s - loss: 0.0167 - acc: 0.997 - ETA: 1s - loss: 0.0164 - acc: 0.997 - ETA: 1s - loss: 0.0161 - acc: 0.997 - ETA: 1s - loss: 0.0158 - acc: 0.997 - ETA: 1s - loss: 0.0158 - acc: 0.997 - ETA: 1s - loss: 0.0155 - acc: 0.997 - ETA: 0s - loss: 0.0153 - acc: 0.997 - ETA: 0s - loss: 0.0150 - acc: 0.997 - ETA: 0s - loss: 0.0149 - acc: 0.997 - ETA: 0s - loss: 0.0146 - acc: 0.997 - ETA: 0s - loss: 0.0152 - acc: 0.997 - ETA: 0s - loss: 0.0151 - acc: 0.997 - ETA: 0s - loss: 0.0149 - acc: 0.997 - ETA: 0s - loss: 0.0148 - acc: 0.997 - ETA: 0s - loss: 0.0147 - acc: 0.998 - ETA: 0s - loss: 0.0146 - acc: 0.998 - ETA: 0s - loss: 0.0144 - acc: 0.998 - ETA: 0s - loss: 0.0143 - acc: 0.998 - ETA: 0s - loss: 0.0142 - acc: 0.998 - ETA: 0s - loss: 0.0141 - acc: 0.998 - ETA: 0s - loss: 0.0140 - acc: 0.998 - ETA: 0s - loss: 0.0139 - acc: 0.998 - ETA: 0s - loss: 0.0138 - acc: 0.998 - ETA: 0s - loss: 0.0137 - acc: 0.998 - ETA: 0s - loss: 0.0136 - acc: 0.9983Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 444us/step - loss: 0.0142 - acc: 0.9982 - val_loss: 0.5785 - val_acc: 0.8383\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6620/6680 [============================>.] - ETA: 3s - loss: 0.0084 - acc: 1.000 - ETA: 3s - loss: 0.0067 - acc: 1.000 - ETA: 2s - loss: 0.0079 - acc: 1.000 - ETA: 2s - loss: 0.0206 - acc: 0.994 - ETA: 2s - loss: 0.0184 - acc: 0.996 - ETA: 2s - loss: 0.0161 - acc: 0.996 - ETA: 2s - loss: 0.0145 - acc: 0.997 - ETA: 2s - loss: 0.0142 - acc: 0.997 - ETA: 2s - loss: 0.0132 - acc: 0.998 - ETA: 2s - loss: 0.0123 - acc: 0.998 - ETA: 2s - loss: 0.0116 - acc: 0.998 - ETA: 2s - loss: 0.0132 - acc: 0.997 - ETA: 2s - loss: 0.0129 - acc: 0.998 - ETA: 2s - loss: 0.0125 - acc: 0.998 - ETA: 2s - loss: 0.0123 - acc: 0.998 - ETA: 2s - loss: 0.0119 - acc: 0.998 - ETA: 2s - loss: 0.0115 - acc: 0.998 - ETA: 1s - loss: 0.0131 - acc: 0.997 - ETA: 1s - loss: 0.0130 - acc: 0.997 - ETA: 1s - loss: 0.0129 - acc: 0.997 - ETA: 1s - loss: 0.0127 - acc: 0.998 - ETA: 1s - loss: 0.0124 - acc: 0.998 - ETA: 1s - loss: 0.0122 - acc: 0.998 - ETA: 1s - loss: 0.0119 - acc: 0.998 - ETA: 1s - loss: 0.0117 - acc: 0.998 - ETA: 1s - loss: 0.0114 - acc: 0.998 - ETA: 1s - loss: 0.0112 - acc: 0.998 - ETA: 1s - loss: 0.0122 - acc: 0.998 - ETA: 1s - loss: 0.0120 - acc: 0.998 - ETA: 1s - loss: 0.0122 - acc: 0.998 - ETA: 1s - loss: 0.0120 - acc: 0.998 - ETA: 1s - loss: 0.0118 - acc: 0.998 - ETA: 1s - loss: 0.0130 - acc: 0.998 - ETA: 1s - loss: 0.0129 - acc: 0.998 - ETA: 1s - loss: 0.0127 - acc: 0.998 - ETA: 1s - loss: 0.0125 - acc: 0.998 - ETA: 0s - loss: 0.0123 - acc: 0.998 - ETA: 0s - loss: 0.0121 - acc: 0.998 - ETA: 0s - loss: 0.0120 - acc: 0.998 - ETA: 0s - loss: 0.0119 - acc: 0.998 - ETA: 0s - loss: 0.0118 - acc: 0.998 - ETA: 0s - loss: 0.0117 - acc: 0.998 - ETA: 0s - loss: 0.0116 - acc: 0.998 - ETA: 0s - loss: 0.0115 - acc: 0.998 - ETA: 0s - loss: 0.0119 - acc: 0.998 - ETA: 0s - loss: 0.0118 - acc: 0.998 - ETA: 0s - loss: 0.0116 - acc: 0.998 - ETA: 0s - loss: 0.0116 - acc: 0.998 - ETA: 0s - loss: 0.0115 - acc: 0.998 - ETA: 0s - loss: 0.0115 - acc: 0.998 - ETA: 0s - loss: 0.0119 - acc: 0.998 - ETA: 0s - loss: 0.0118 - acc: 0.998 - ETA: 0s - loss: 0.0132 - acc: 0.998 - ETA: 0s - loss: 0.0132 - acc: 0.9982Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 444us/step - loss: 0.0131 - acc: 0.9982 - val_loss: 0.5868 - val_acc: 0.8383\n",
      "Epoch 14/20\n",
      "6640/6680 [============================>.] - ETA: 3s - loss: 0.0452 - acc: 0.950 - ETA: 2s - loss: 0.0099 - acc: 0.992 - ETA: 2s - loss: 0.0072 - acc: 0.996 - ETA: 2s - loss: 0.0063 - acc: 0.997 - ETA: 2s - loss: 0.0061 - acc: 0.998 - ETA: 2s - loss: 0.0058 - acc: 0.998 - ETA: 2s - loss: 0.0057 - acc: 0.998 - ETA: 2s - loss: 0.0055 - acc: 0.998 - ETA: 2s - loss: 0.0053 - acc: 0.999 - ETA: 2s - loss: 0.0075 - acc: 0.998 - ETA: 2s - loss: 0.0073 - acc: 0.998 - ETA: 2s - loss: 0.0070 - acc: 0.998 - ETA: 2s - loss: 0.0069 - acc: 0.998 - ETA: 2s - loss: 0.0078 - acc: 0.998 - ETA: 2s - loss: 0.0075 - acc: 0.998 - ETA: 2s - loss: 0.0078 - acc: 0.998 - ETA: 2s - loss: 0.0076 - acc: 0.998 - ETA: 2s - loss: 0.0075 - acc: 0.998 - ETA: 1s - loss: 0.0073 - acc: 0.998 - ETA: 1s - loss: 0.0071 - acc: 0.998 - ETA: 1s - loss: 0.0070 - acc: 0.998 - ETA: 1s - loss: 0.0070 - acc: 0.998 - ETA: 1s - loss: 0.0069 - acc: 0.998 - ETA: 1s - loss: 0.0068 - acc: 0.998 - ETA: 1s - loss: 0.0067 - acc: 0.999 - ETA: 1s - loss: 0.0080 - acc: 0.998 - ETA: 1s - loss: 0.0079 - acc: 0.998 - ETA: 1s - loss: 0.0078 - acc: 0.998 - ETA: 1s - loss: 0.0077 - acc: 0.998 - ETA: 1s - loss: 0.0076 - acc: 0.998 - ETA: 1s - loss: 0.0075 - acc: 0.998 - ETA: 1s - loss: 0.0091 - acc: 0.998 - ETA: 1s - loss: 0.0089 - acc: 0.998 - ETA: 1s - loss: 0.0088 - acc: 0.998 - ETA: 1s - loss: 0.0110 - acc: 0.998 - ETA: 1s - loss: 0.0108 - acc: 0.998 - ETA: 0s - loss: 0.0109 - acc: 0.998 - ETA: 0s - loss: 0.0111 - acc: 0.997 - ETA: 0s - loss: 0.0115 - acc: 0.997 - ETA: 0s - loss: 0.0115 - acc: 0.997 - ETA: 0s - loss: 0.0114 - acc: 0.997 - ETA: 0s - loss: 0.0123 - acc: 0.997 - ETA: 0s - loss: 0.0123 - acc: 0.997 - ETA: 0s - loss: 0.0127 - acc: 0.997 - ETA: 0s - loss: 0.0126 - acc: 0.997 - ETA: 0s - loss: 0.0124 - acc: 0.997 - ETA: 0s - loss: 0.0122 - acc: 0.997 - ETA: 0s - loss: 0.0136 - acc: 0.997 - ETA: 0s - loss: 0.0136 - acc: 0.997 - ETA: 0s - loss: 0.0134 - acc: 0.997 - ETA: 0s - loss: 0.0138 - acc: 0.997 - ETA: 0s - loss: 0.0146 - acc: 0.997 - ETA: 0s - loss: 0.0146 - acc: 0.997 - ETA: 0s - loss: 0.0145 - acc: 0.997 - ETA: 0s - loss: 0.0144 - acc: 0.9974Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 446us/step - loss: 0.0143 - acc: 0.9975 - val_loss: 0.6524 - val_acc: 0.8299\n",
      "Epoch 15/20\n",
      "6640/6680 [============================>.] - ETA: 3s - loss: 0.0078 - acc: 1.000 - ETA: 2s - loss: 0.0793 - acc: 0.985 - ETA: 2s - loss: 0.0418 - acc: 0.992 - ETA: 2s - loss: 0.0310 - acc: 0.995 - ETA: 2s - loss: 0.0258 - acc: 0.996 - ETA: 2s - loss: 0.0267 - acc: 0.995 - ETA: 2s - loss: 0.0231 - acc: 0.996 - ETA: 2s - loss: 0.0206 - acc: 0.996 - ETA: 2s - loss: 0.0200 - acc: 0.996 - ETA: 2s - loss: 0.0185 - acc: 0.996 - ETA: 2s - loss: 0.0172 - acc: 0.996 - ETA: 2s - loss: 0.0161 - acc: 0.997 - ETA: 2s - loss: 0.0151 - acc: 0.997 - ETA: 2s - loss: 0.0143 - acc: 0.997 - ETA: 2s - loss: 0.0135 - acc: 0.997 - ETA: 2s - loss: 0.0131 - acc: 0.997 - ETA: 2s - loss: 0.0126 - acc: 0.998 - ETA: 1s - loss: 0.0120 - acc: 0.998 - ETA: 1s - loss: 0.0116 - acc: 0.998 - ETA: 1s - loss: 0.0112 - acc: 0.998 - ETA: 1s - loss: 0.0118 - acc: 0.998 - ETA: 1s - loss: 0.0115 - acc: 0.998 - ETA: 1s - loss: 0.0112 - acc: 0.998 - ETA: 1s - loss: 0.0127 - acc: 0.997 - ETA: 1s - loss: 0.0138 - acc: 0.997 - ETA: 1s - loss: 0.0136 - acc: 0.997 - ETA: 1s - loss: 0.0144 - acc: 0.997 - ETA: 1s - loss: 0.0140 - acc: 0.997 - ETA: 1s - loss: 0.0140 - acc: 0.997 - ETA: 1s - loss: 0.0139 - acc: 0.997 - ETA: 1s - loss: 0.0138 - acc: 0.997 - ETA: 1s - loss: 0.0136 - acc: 0.997 - ETA: 1s - loss: 0.0135 - acc: 0.998 - ETA: 1s - loss: 0.0133 - acc: 0.998 - ETA: 1s - loss: 0.0146 - acc: 0.997 - ETA: 0s - loss: 0.0147 - acc: 0.997 - ETA: 0s - loss: 0.0146 - acc: 0.997 - ETA: 0s - loss: 0.0150 - acc: 0.997 - ETA: 0s - loss: 0.0149 - acc: 0.997 - ETA: 0s - loss: 0.0149 - acc: 0.997 - ETA: 0s - loss: 0.0147 - acc: 0.997 - ETA: 0s - loss: 0.0154 - acc: 0.997 - ETA: 0s - loss: 0.0152 - acc: 0.997 - ETA: 0s - loss: 0.0150 - acc: 0.997 - ETA: 0s - loss: 0.0149 - acc: 0.997 - ETA: 0s - loss: 0.0149 - acc: 0.997 - ETA: 0s - loss: 0.0147 - acc: 0.997 - ETA: 0s - loss: 0.0146 - acc: 0.997 - ETA: 0s - loss: 0.0151 - acc: 0.997 - ETA: 0s - loss: 0.0157 - acc: 0.997 - ETA: 0s - loss: 0.0156 - acc: 0.997 - ETA: 0s - loss: 0.0160 - acc: 0.997 - ETA: 0s - loss: 0.0162 - acc: 0.997 - ETA: 0s - loss: 0.0160 - acc: 0.9971Epoch 00015: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 442us/step - loss: 0.0160 - acc: 0.9972 - val_loss: 0.7143 - val_acc: 0.8204\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 3s - loss: 0.0131 - acc: 1.000 - ETA: 3s - loss: 0.0392 - acc: 0.985 - ETA: 2s - loss: 0.0258 - acc: 0.992 - ETA: 2s - loss: 0.0195 - acc: 0.995 - ETA: 2s - loss: 0.0224 - acc: 0.994 - ETA: 2s - loss: 0.0217 - acc: 0.993 - ETA: 2s - loss: 0.0193 - acc: 0.994 - ETA: 2s - loss: 0.0182 - acc: 0.995 - ETA: 2s - loss: 0.0168 - acc: 0.996 - ETA: 2s - loss: 0.0155 - acc: 0.996 - ETA: 2s - loss: 0.0160 - acc: 0.996 - ETA: 2s - loss: 0.0151 - acc: 0.996 - ETA: 2s - loss: 0.0143 - acc: 0.996 - ETA: 2s - loss: 0.0137 - acc: 0.997 - ETA: 2s - loss: 0.0143 - acc: 0.996 - ETA: 2s - loss: 0.0137 - acc: 0.996 - ETA: 2s - loss: 0.0134 - acc: 0.997 - ETA: 1s - loss: 0.0132 - acc: 0.997 - ETA: 1s - loss: 0.0168 - acc: 0.997 - ETA: 1s - loss: 0.0206 - acc: 0.995 - ETA: 1s - loss: 0.0197 - acc: 0.996 - ETA: 1s - loss: 0.0224 - acc: 0.995 - ETA: 1s - loss: 0.0217 - acc: 0.995 - ETA: 1s - loss: 0.0223 - acc: 0.995 - ETA: 1s - loss: 0.0223 - acc: 0.995 - ETA: 1s - loss: 0.0217 - acc: 0.995 - ETA: 1s - loss: 0.0235 - acc: 0.995 - ETA: 1s - loss: 0.0230 - acc: 0.995 - ETA: 1s - loss: 0.0234 - acc: 0.995 - ETA: 1s - loss: 0.0229 - acc: 0.995 - ETA: 1s - loss: 0.0230 - acc: 0.995 - ETA: 1s - loss: 0.0225 - acc: 0.995 - ETA: 1s - loss: 0.0221 - acc: 0.995 - ETA: 1s - loss: 0.0216 - acc: 0.995 - ETA: 1s - loss: 0.0215 - acc: 0.996 - ETA: 0s - loss: 0.0214 - acc: 0.995 - ETA: 0s - loss: 0.0215 - acc: 0.995 - ETA: 0s - loss: 0.0235 - acc: 0.995 - ETA: 0s - loss: 0.0230 - acc: 0.995 - ETA: 0s - loss: 0.0232 - acc: 0.995 - ETA: 0s - loss: 0.0228 - acc: 0.995 - ETA: 0s - loss: 0.0253 - acc: 0.994 - ETA: 0s - loss: 0.0264 - acc: 0.994 - ETA: 0s - loss: 0.0289 - acc: 0.993 - ETA: 0s - loss: 0.0311 - acc: 0.993 - ETA: 0s - loss: 0.0309 - acc: 0.993 - ETA: 0s - loss: 0.0325 - acc: 0.992 - ETA: 0s - loss: 0.0335 - acc: 0.992 - ETA: 0s - loss: 0.0357 - acc: 0.991 - ETA: 0s - loss: 0.0375 - acc: 0.991 - ETA: 0s - loss: 0.0387 - acc: 0.991 - ETA: 0s - loss: 0.0384 - acc: 0.991 - ETA: 0s - loss: 0.0392 - acc: 0.991 - ETA: 0s - loss: 0.0391 - acc: 0.9910Epoch 00016: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 443us/step - loss: 0.0392 - acc: 0.9909 - val_loss: 1.0672 - val_acc: 0.7701\n",
      "Epoch 17/20\n",
      "6580/6680 [============================>.] - ETA: 4s - loss: 0.2265 - acc: 0.950 - ETA: 3s - loss: 0.0640 - acc: 0.992 - ETA: 2s - loss: 0.0728 - acc: 0.984 - ETA: 2s - loss: 0.1067 - acc: 0.978 - ETA: 2s - loss: 0.1113 - acc: 0.976 - ETA: 2s - loss: 0.1072 - acc: 0.975 - ETA: 2s - loss: 0.1003 - acc: 0.977 - ETA: 2s - loss: 0.0923 - acc: 0.976 - ETA: 2s - loss: 0.0874 - acc: 0.978 - ETA: 2s - loss: 0.0871 - acc: 0.978 - ETA: 2s - loss: 0.0904 - acc: 0.976 - ETA: 2s - loss: 0.0903 - acc: 0.976 - ETA: 2s - loss: 0.1025 - acc: 0.973 - ETA: 2s - loss: 0.1068 - acc: 0.971 - ETA: 2s - loss: 0.1132 - acc: 0.970 - ETA: 2s - loss: 0.1117 - acc: 0.969 - ETA: 2s - loss: 0.1109 - acc: 0.968 - ETA: 1s - loss: 0.1088 - acc: 0.968 - ETA: 1s - loss: 0.1193 - acc: 0.965 - ETA: 1s - loss: 0.1241 - acc: 0.963 - ETA: 1s - loss: 0.1253 - acc: 0.963 - ETA: 1s - loss: 0.1361 - acc: 0.961 - ETA: 1s - loss: 0.1379 - acc: 0.961 - ETA: 1s - loss: 0.1379 - acc: 0.960 - ETA: 1s - loss: 0.1347 - acc: 0.961 - ETA: 1s - loss: 0.1368 - acc: 0.962 - ETA: 1s - loss: 0.1350 - acc: 0.962 - ETA: 1s - loss: 0.1327 - acc: 0.962 - ETA: 1s - loss: 0.1351 - acc: 0.962 - ETA: 1s - loss: 0.1404 - acc: 0.960 - ETA: 1s - loss: 0.1381 - acc: 0.960 - ETA: 1s - loss: 0.1444 - acc: 0.959 - ETA: 1s - loss: 0.1450 - acc: 0.958 - ETA: 1s - loss: 0.1489 - acc: 0.958 - ETA: 1s - loss: 0.1519 - acc: 0.957 - ETA: 0s - loss: 0.1540 - acc: 0.956 - ETA: 0s - loss: 0.1569 - acc: 0.956 - ETA: 0s - loss: 0.1563 - acc: 0.955 - ETA: 0s - loss: 0.1572 - acc: 0.956 - ETA: 0s - loss: 0.1565 - acc: 0.955 - ETA: 0s - loss: 0.1549 - acc: 0.956 - ETA: 0s - loss: 0.1549 - acc: 0.955 - ETA: 0s - loss: 0.1541 - acc: 0.955 - ETA: 0s - loss: 0.1532 - acc: 0.955 - ETA: 0s - loss: 0.1546 - acc: 0.954 - ETA: 0s - loss: 0.1553 - acc: 0.953 - ETA: 0s - loss: 0.1573 - acc: 0.953 - ETA: 0s - loss: 0.1568 - acc: 0.953 - ETA: 0s - loss: 0.1564 - acc: 0.953 - ETA: 0s - loss: 0.1565 - acc: 0.952 - ETA: 0s - loss: 0.1550 - acc: 0.953 - ETA: 0s - loss: 0.1585 - acc: 0.952 - ETA: 0s - loss: 0.1596 - acc: 0.951 - ETA: 0s - loss: 0.1574 - acc: 0.9523Epoch 00017: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 444us/step - loss: 0.1576 - acc: 0.9521 - val_loss: 1.1075 - val_acc: 0.7677\n",
      "Epoch 18/20\n",
      "6580/6680 [============================>.] - ETA: 3s - loss: 0.1962 - acc: 0.900 - ETA: 3s - loss: 0.0829 - acc: 0.964 - ETA: 2s - loss: 0.1195 - acc: 0.957 - ETA: 2s - loss: 0.0932 - acc: 0.968 - ETA: 2s - loss: 0.1024 - acc: 0.964 - ETA: 2s - loss: 0.0925 - acc: 0.967 - ETA: 2s - loss: 0.0828 - acc: 0.972 - ETA: 2s - loss: 0.0881 - acc: 0.971 - ETA: 2s - loss: 0.0918 - acc: 0.971 - ETA: 2s - loss: 0.0871 - acc: 0.972 - ETA: 2s - loss: 0.0812 - acc: 0.974 - ETA: 2s - loss: 0.0749 - acc: 0.976 - ETA: 2s - loss: 0.0708 - acc: 0.978 - ETA: 2s - loss: 0.0702 - acc: 0.978 - ETA: 2s - loss: 0.0668 - acc: 0.978 - ETA: 2s - loss: 0.0628 - acc: 0.980 - ETA: 2s - loss: 0.0602 - acc: 0.981 - ETA: 1s - loss: 0.0577 - acc: 0.981 - ETA: 1s - loss: 0.0582 - acc: 0.982 - ETA: 1s - loss: 0.0557 - acc: 0.982 - ETA: 1s - loss: 0.0564 - acc: 0.982 - ETA: 1s - loss: 0.0546 - acc: 0.983 - ETA: 1s - loss: 0.0537 - acc: 0.984 - ETA: 1s - loss: 0.0536 - acc: 0.984 - ETA: 1s - loss: 0.0523 - acc: 0.984 - ETA: 1s - loss: 0.0518 - acc: 0.985 - ETA: 1s - loss: 0.0506 - acc: 0.985 - ETA: 1s - loss: 0.0492 - acc: 0.985 - ETA: 1s - loss: 0.0481 - acc: 0.986 - ETA: 1s - loss: 0.0469 - acc: 0.986 - ETA: 1s - loss: 0.0471 - acc: 0.986 - ETA: 1s - loss: 0.0465 - acc: 0.987 - ETA: 1s - loss: 0.0454 - acc: 0.987 - ETA: 1s - loss: 0.0446 - acc: 0.987 - ETA: 1s - loss: 0.0452 - acc: 0.987 - ETA: 1s - loss: 0.0443 - acc: 0.987 - ETA: 0s - loss: 0.0435 - acc: 0.987 - ETA: 0s - loss: 0.0429 - acc: 0.987 - ETA: 0s - loss: 0.0429 - acc: 0.987 - ETA: 0s - loss: 0.0422 - acc: 0.988 - ETA: 0s - loss: 0.0425 - acc: 0.988 - ETA: 0s - loss: 0.0419 - acc: 0.988 - ETA: 0s - loss: 0.0413 - acc: 0.988 - ETA: 0s - loss: 0.0412 - acc: 0.988 - ETA: 0s - loss: 0.0405 - acc: 0.988 - ETA: 0s - loss: 0.0399 - acc: 0.988 - ETA: 0s - loss: 0.0395 - acc: 0.988 - ETA: 0s - loss: 0.0390 - acc: 0.989 - ETA: 0s - loss: 0.0387 - acc: 0.989 - ETA: 0s - loss: 0.0384 - acc: 0.989 - ETA: 0s - loss: 0.0380 - acc: 0.989 - ETA: 0s - loss: 0.0376 - acc: 0.989 - ETA: 0s - loss: 0.0384 - acc: 0.989 - ETA: 0s - loss: 0.0384 - acc: 0.9895Epoch 00018: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 444us/step - loss: 0.0387 - acc: 0.9892 - val_loss: 0.8425 - val_acc: 0.8012\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6580/6680 [============================>.] - ETA: 4s - loss: 0.0094 - acc: 1.000 - ETA: 3s - loss: 0.0093 - acc: 0.992 - ETA: 2s - loss: 0.0425 - acc: 0.992 - ETA: 2s - loss: 0.0411 - acc: 0.987 - ETA: 2s - loss: 0.0324 - acc: 0.990 - ETA: 2s - loss: 0.0273 - acc: 0.992 - ETA: 2s - loss: 0.0281 - acc: 0.992 - ETA: 2s - loss: 0.0266 - acc: 0.992 - ETA: 2s - loss: 0.0240 - acc: 0.993 - ETA: 2s - loss: 0.0218 - acc: 0.993 - ETA: 2s - loss: 0.0205 - acc: 0.994 - ETA: 2s - loss: 0.0192 - acc: 0.994 - ETA: 2s - loss: 0.0179 - acc: 0.995 - ETA: 2s - loss: 0.0167 - acc: 0.995 - ETA: 2s - loss: 0.0159 - acc: 0.996 - ETA: 2s - loss: 0.0149 - acc: 0.996 - ETA: 2s - loss: 0.0143 - acc: 0.996 - ETA: 1s - loss: 0.0164 - acc: 0.996 - ETA: 1s - loss: 0.0157 - acc: 0.996 - ETA: 1s - loss: 0.0153 - acc: 0.996 - ETA: 1s - loss: 0.0152 - acc: 0.996 - ETA: 1s - loss: 0.0168 - acc: 0.996 - ETA: 1s - loss: 0.0168 - acc: 0.996 - ETA: 1s - loss: 0.0164 - acc: 0.996 - ETA: 1s - loss: 0.0161 - acc: 0.996 - ETA: 1s - loss: 0.0157 - acc: 0.996 - ETA: 1s - loss: 0.0153 - acc: 0.996 - ETA: 1s - loss: 0.0149 - acc: 0.996 - ETA: 1s - loss: 0.0146 - acc: 0.996 - ETA: 1s - loss: 0.0142 - acc: 0.996 - ETA: 1s - loss: 0.0144 - acc: 0.996 - ETA: 1s - loss: 0.0159 - acc: 0.996 - ETA: 1s - loss: 0.0155 - acc: 0.996 - ETA: 1s - loss: 0.0151 - acc: 0.996 - ETA: 1s - loss: 0.0162 - acc: 0.995 - ETA: 1s - loss: 0.0161 - acc: 0.995 - ETA: 1s - loss: 0.0161 - acc: 0.995 - ETA: 0s - loss: 0.0173 - acc: 0.995 - ETA: 0s - loss: 0.0169 - acc: 0.995 - ETA: 0s - loss: 0.0185 - acc: 0.995 - ETA: 0s - loss: 0.0184 - acc: 0.995 - ETA: 0s - loss: 0.0192 - acc: 0.995 - ETA: 0s - loss: 0.0207 - acc: 0.995 - ETA: 0s - loss: 0.0215 - acc: 0.994 - ETA: 0s - loss: 0.0212 - acc: 0.995 - ETA: 0s - loss: 0.0210 - acc: 0.995 - ETA: 0s - loss: 0.0206 - acc: 0.995 - ETA: 0s - loss: 0.0203 - acc: 0.995 - ETA: 0s - loss: 0.0202 - acc: 0.995 - ETA: 0s - loss: 0.0200 - acc: 0.995 - ETA: 0s - loss: 0.0197 - acc: 0.995 - ETA: 0s - loss: 0.0194 - acc: 0.995 - ETA: 0s - loss: 0.0192 - acc: 0.995 - ETA: 0s - loss: 0.0189 - acc: 0.995 - ETA: 0s - loss: 0.0186 - acc: 0.995 - ETA: 0s - loss: 0.0183 - acc: 0.995 - ETA: 0s - loss: 0.0180 - acc: 0.995 - ETA: 0s - loss: 0.0178 - acc: 0.9959Epoch 00019: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 478us/step - loss: 0.0176 - acc: 0.9960 - val_loss: 0.7529 - val_acc: 0.8251\n",
      "Epoch 20/20\n",
      "6660/6680 [============================>.] - ETA: 4s - loss: 0.0017 - acc: 1.000 - ETA: 3s - loss: 0.0028 - acc: 1.000 - ETA: 3s - loss: 0.0023 - acc: 1.000 - ETA: 3s - loss: 0.0022 - acc: 1.000 - ETA: 3s - loss: 0.0023 - acc: 1.000 - ETA: 3s - loss: 0.0023 - acc: 1.000 - ETA: 3s - loss: 0.0022 - acc: 1.000 - ETA: 3s - loss: 0.0021 - acc: 1.000 - ETA: 2s - loss: 0.0021 - acc: 1.000 - ETA: 2s - loss: 0.0021 - acc: 1.000 - ETA: 2s - loss: 0.0020 - acc: 1.000 - ETA: 2s - loss: 0.0020 - acc: 1.000 - ETA: 2s - loss: 0.0020 - acc: 1.000 - ETA: 2s - loss: 0.0020 - acc: 1.000 - ETA: 2s - loss: 0.0019 - acc: 1.000 - ETA: 2s - loss: 0.0020 - acc: 1.000 - ETA: 2s - loss: 0.0020 - acc: 1.000 - ETA: 2s - loss: 0.0021 - acc: 1.000 - ETA: 2s - loss: 0.0021 - acc: 1.000 - ETA: 2s - loss: 0.0020 - acc: 1.000 - ETA: 2s - loss: 0.0020 - acc: 1.000 - ETA: 2s - loss: 0.0020 - acc: 1.000 - ETA: 1s - loss: 0.0020 - acc: 1.000 - ETA: 1s - loss: 0.0019 - acc: 1.000 - ETA: 1s - loss: 0.0019 - acc: 1.000 - ETA: 1s - loss: 0.0019 - acc: 1.000 - ETA: 1s - loss: 0.0052 - acc: 0.999 - ETA: 1s - loss: 0.0051 - acc: 0.999 - ETA: 1s - loss: 0.0050 - acc: 0.999 - ETA: 1s - loss: 0.0061 - acc: 0.999 - ETA: 1s - loss: 0.0060 - acc: 0.999 - ETA: 1s - loss: 0.0063 - acc: 0.999 - ETA: 1s - loss: 0.0072 - acc: 0.998 - ETA: 1s - loss: 0.0069 - acc: 0.999 - ETA: 1s - loss: 0.0078 - acc: 0.998 - ETA: 1s - loss: 0.0076 - acc: 0.998 - ETA: 1s - loss: 0.0077 - acc: 0.998 - ETA: 1s - loss: 0.0082 - acc: 0.998 - ETA: 0s - loss: 0.0080 - acc: 0.998 - ETA: 0s - loss: 0.0090 - acc: 0.998 - ETA: 0s - loss: 0.0088 - acc: 0.998 - ETA: 0s - loss: 0.0091 - acc: 0.998 - ETA: 0s - loss: 0.0093 - acc: 0.997 - ETA: 0s - loss: 0.0091 - acc: 0.997 - ETA: 0s - loss: 0.0089 - acc: 0.997 - ETA: 0s - loss: 0.0090 - acc: 0.997 - ETA: 0s - loss: 0.0088 - acc: 0.997 - ETA: 0s - loss: 0.0087 - acc: 0.997 - ETA: 0s - loss: 0.0086 - acc: 0.997 - ETA: 0s - loss: 0.0084 - acc: 0.998 - ETA: 0s - loss: 0.0090 - acc: 0.997 - ETA: 0s - loss: 0.0089 - acc: 0.997 - ETA: 0s - loss: 0.0099 - acc: 0.997 - ETA: 0s - loss: 0.0105 - acc: 0.997 - ETA: 0s - loss: 0.0103 - acc: 0.997 - ETA: 0s - loss: 0.0106 - acc: 0.9976Epoch 00020: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 461us/step - loss: 0.0106 - acc: 0.9976 - val_loss: 0.7709 - val_acc: 0.8216\n",
      "Test accuracy: 82.5359%\n"
     ]
    }
   ],
   "source": [
    "# try out Resnet50\n",
    "\n",
    "test_results['Resnet50'] = tryout_bottleneck('Resnet50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try purely\n",
    "# underfits\n",
    "# first extract features via max-pooling\n",
    "# these features need to adopt to the problem's data\n",
    "# flatten the data to add a dense layer, back propagation will update the weights\n",
    "# finally softmax layer to determine to which category should the image be allocated by giving softmax scores\n",
    "# overfits - dropout layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Define your architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model\n",
    "InceptionV3_model = Sequential()\n",
    "\n",
    "# add a max-pooling layer for feature selection\n",
    "InceptionV3_model.add(MaxPooling2D(input_shape=train_InceptionV3.shape[1:]))\n",
    "\n",
    "# add a dropout layer to avoid overfitting\n",
    "InceptionV3_model.add(Dropout(0.4))\n",
    "\n",
    "# flatten to get fully connected layer\n",
    "InceptionV3_model.add(Flatten())\n",
    "\n",
    "# dense layer with 500 nodes\n",
    "InceptionV3_model.add(Dense(500, activation='relu'))\n",
    "\n",
    "# dropout\n",
    "InceptionV3_model.add(Dropout(0.4))\n",
    "\n",
    "# finally softmax layer for output (133 classes)\n",
    "InceptionV3_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "InceptionV3_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "InceptionV3_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.  \n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.InceptionV3.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "InceptionV3_model.fit(train_InceptionV3, train_targets, \n",
    "          validation_data=(valid_InceptionV3, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Load the model weights with the best validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "InceptionV3_model.load_weights('saved_models/weights.best.InceptionV3.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Calculate classification accuracy on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "InceptionV3_predictions = [np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Predict Dog Breed with the Model\n",
    "\n",
    "Write a function that takes an image path as input and returns the dog breed (`Affenpinscher`, `Afghan_hound`, etc) that is predicted by your model.  \n",
    "\n",
    "Similar to the analogous function in Step 5, your function should have three steps:\n",
    "1. Extract the bottleneck features corresponding to the chosen CNN model.\n",
    "2. Supply the bottleneck features as input to the model to return the predicted vector.  Note that the argmax of this prediction vector gives the index of the predicted dog breed.\n",
    "3. Use the `dog_names` array defined in Step 0 of this notebook to return the corresponding breed.\n",
    "\n",
    "The functions to extract the bottleneck features can be found in `extract_bottleneck_features.py`, and they have been imported in an earlier code cell.  To obtain the bottleneck features corresponding to your chosen CNN architecture, you need to use the function\n",
    "\n",
    "    extract_{network}\n",
    "    \n",
    "where `{network}`, in the above filename, should be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Write a function that takes a path to an image as input\n",
    "### and returns the dog breed that is predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def InceptionV3_predict_breed(img_path):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_InceptionV3(path_to_tensor(img_path))\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = InceptionV3_model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step6'></a>\n",
    "## Step 6: Write your Algorithm\n",
    "\n",
    "Write an algorithm that accepts a file path to an image and first determines whether the image contains a human, dog, or neither.  Then,\n",
    "- if a __dog__ is detected in the image, return the predicted breed.\n",
    "- if a __human__ is detected in the image, return the resembling dog breed.\n",
    "- if __neither__ is detected in the image, provide output that indicates an error.\n",
    "\n",
    "You are welcome to write your own functions for detecting humans and dogs in images, but feel free to use the `face_detector` and `dog_detector` functions developed above.  You are __required__ to use your CNN from Step 5 to predict dog breed.  \n",
    "\n",
    "Some sample output for our algorithm is provided below, but feel free to design your own user experience!\n",
    "\n",
    "![Sample Human Output](images/sample_human_output.png)\n",
    "\n",
    "\n",
    "### (IMPLEMENTATION) Write your Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Write your algorithm.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dog_app(imgpath):\n",
    "    \n",
    "    # convert img to option_model's form\n",
    "    data = cv2.resize(cv2.imread(imgpath), (64,64)).reshape(-1,64,64,3)\n",
    "    \n",
    "    # print image\n",
    "    \n",
    "    img = cv2.imread(imgpath)\n",
    "    cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # display the image\n",
    "    plt.imshow(cv_rgb)    \n",
    "    \n",
    "    # if a dog's detected\n",
    "    if dog_detector(imgpath):\n",
    "        print(\"Detected a dog!\")\n",
    "        return InceptionV3_predict_breed(imgpath)\n",
    "    # else if a human's detected\n",
    "    elif option_model.predict(data):\n",
    "        print(\"Detected a human!\")\n",
    "        print(\"You look like a ...\")\n",
    "        return InceptionV3_predict_breed(imgpath)\n",
    "    else:\n",
    "        print(\"Error: neither a human nor a dog detected.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step7'></a>\n",
    "## Step 7: Test Your Algorithm\n",
    "\n",
    "In this section, you will take your new algorithm for a spin!  What kind of dog does the algorithm think that __you__ look like?  If you have a dog, does it predict your dog's breed accurately?  If you have a cat, does it mistakenly think that your cat is a dog?\n",
    "\n",
    "### (IMPLEMENTATION) Test Your Algorithm on Sample Images!\n",
    "\n",
    "Test your algorithm at least six images on your computer.  Feel free to use any images you like.  Use at least two human and two dog images.  \n",
    "\n",
    "__Question 6:__ Is the output better than you expected :) ?  Or worse :( ?  Provide at least three possible points of improvement for your algorithm.\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO: Execute your algorithm from Step 6 on\n",
    "## at least 6 images on your computer.\n",
    "## Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgpath = 'images/test1.jpg'\n",
    "\n",
    "dog_app(imgpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgpath = 'images/test2.jpg'\n",
    "\n",
    "dog_app(imgpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgpath = 'images/test3.jpg'\n",
    "\n",
    "dog_app(imgpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgpath = 'images/test4.jpg'\n",
    "\n",
    "dog_app(imgpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgpath = 'images/test5.png'\n",
    "\n",
    "dog_app(imgpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgpath = 'images/test6.jpg'\n",
    "\n",
    "dog_app(imgpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgpath = 'images/test7.jpg'\n",
    "\n",
    "dog_app(imgpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgpath = 'images/test8.jpg'\n",
    "\n",
    "dog_app(imgpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### possible points of improvement\n",
    "\n",
    "the model did worse than i expected. =( I'm disappointed that Steve Jobs was not detected as a human.\n",
    "\n",
    "\n",
    "1. Steve Jobs is appraently a human but the human detector could not detect correctly.\n",
    "\n",
    "   => maybe because he had his fingers on his chin. training on more images as such might help.\n",
    "   \n",
    "2. I don't know of the dog breeds very much. 'Giant_schnauzer' does not mean much to me.\n",
    "\n",
    "   => showing a sample image of the dog breed predicted may give better user experience.\n",
    "   \n",
    "3. As I suspected.. the model does not recognize side-face images as humans.\n",
    "\n",
    "   => training on more images as such might help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "dog-project",
   "language": "python",
   "name": "dog-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
